{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "# For AMP\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "print(\"Imports complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Global Hyperparameters & Configuration\n",
        "\n",
        "# --- System ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = \"/kaggle/input/officehome/OfficeHome\"\n",
        "MODEL_SAVE_DIR_BASE = \"/kaggle/working/models_resnet50/\"\n",
        "os.makedirs(MODEL_SAVE_DIR_BASE, exist_ok=True)\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# --- Dataset & DataLoader ---\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "NUM_CLASSES = 65\n",
        "\n",
        "# --- ResNet-50 Backbone (Changed from ViT) ---\n",
        "RESNET_MODEL_NAME = 'resnet50'\n",
        "RESNET_EMBED_DIM = 2048  # ResNet-50 outputs 2048-dim features (vs 768 for ViT)\n",
        "\n",
        "# --- LoRA ---\n",
        "LORA_RANK = 16\n",
        "LORA_ALPHA = 32.0\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# --- DAD Module ---\n",
        "DAD_K_STEPS = 200\n",
        "DAD_P_THETA_TIMESTEP_EMBED_DIM = 128\n",
        "DAD_P_THETA_HIDDEN_DIM_MULT = 4\n",
        "DAD_P_THETA_MLP_HIDDEN_DIM = 1024\n",
        "DAD_BETA_START = 1e-4\n",
        "DAD_BETA_END = 2e-2\n",
        "\n",
        "# --- Training (General) ---\n",
        "SOURCE_DOMAIN_NAME = 'Art'\n",
        "TARGET_DOMAIN_NAMES_ORDERED = ['Clipart', 'Product', 'RealWorld']\n",
        "ALL_TRAINABLE_DOMAIN_NAMES = [SOURCE_DOMAIN_NAME] + TARGET_DOMAIN_NAMES_ORDERED\n",
        "\n",
        "# --- Source Domain Training (Art) ---\n",
        "ART_EPOCHS = 10\n",
        "ART_LR_LORA_HEAD_BN = 5e-4\n",
        "\n",
        "# --- Continual Adaptation (Per Target Domain) ---\n",
        "ADAPT_MLS_R_ITER = 10\n",
        "ADAPT_LR_LORA_HEAD_BN = 1e-4\n",
        "ADAPT_LR_P_THETA = 1e-4\n",
        "ADAPT_LTR_EPOCHS = 5\n",
        "EARLY_STOPPING_PATIENCE_ADAPT = 3\n",
        "\n",
        "# --- EMA Teacher & Pseudo-Labeling ---\n",
        "EMA_DECAY = 0.999\n",
        "PSEUDO_LABEL_THRESHOLD_START = 0.7\n",
        "PSEUDO_LABEL_THRESHOLD_END = 0.9\n",
        "\n",
        "# --- FixMatch ---\n",
        "FIXMATCH_CONF_THRESHOLD = 0.95\n",
        "FIXMATCH_LAMBDA = 1.0\n",
        "\n",
        "# --- SHOT ---\n",
        "SHOT_LAMBDA_COND_ENT = 0.05\n",
        "SHOT_LAMBDA_ENT_MAX = 0.05\n",
        "\n",
        "# --- Experience Replay (Optional) ---\n",
        "REPLAY_BUFFER_SIZE = 2000\n",
        "REPLAY_BATCH_SIZE_RATIO = 0.25\n",
        "REPLAY_LAMBDA = 0.1\n",
        "\n",
        "# --- Domain Classifier ---\n",
        "DC_HEAD_LR = 1e-3\n",
        "DC_HEAD_EPOCHS = 10\n",
        "\n",
        "# --- Robust Inference Pipeline ---\n",
        "INFER_DOMAIN_CONF_THRESH = 0.7\n",
        "INFER_EXPERT_CONF_THRESH = 0.6\n",
        "INFER_STAGE2_EXPERT_CONF_THRESH = 0.5\n",
        "INFER_K_EXPERTS_FOR_AVG = 3\n",
        "\n",
        "# --- Autocast Context for AMP ---\n",
        "if DEVICE.type == 'cuda':\n",
        "    autocast_ctx = partial(torch.amp.autocast, device_type='cuda', dtype=torch.float16, enabled=True)\n",
        "else:\n",
        "    import contextlib\n",
        "    autocast_ctx = contextlib.nullcontext\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA available. GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"All configurations set. Base model save dir: {MODEL_SAVE_DIR_BASE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Transforms Definition\n",
        "\n",
        "train_transform_strong = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform_weak = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "pil_load_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "])\n",
        "\n",
        "print(\"Transforms defined: train_transform_strong, val_test_transform_weak, pil_load_transform.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Dataset Classes\n",
        "\n",
        "class FixMatchOfficeHomeDataset(Dataset):\n",
        "    def __init__(self, root_dir, domain_name, transform_weak, transform_strong,\n",
        "                 split_ratios=(0.8, 0.1, 0.1), split_type='train',\n",
        "                 random_seed=RANDOM_SEED, class_to_idx_mapping=None):\n",
        "        self.base_dataset = OfficeHomeDomainDataset(\n",
        "            root_dir, domain_name, transform=None,\n",
        "            split_ratios=split_ratios, split_type=split_type,\n",
        "            random_seed=random_seed, class_to_idx_mapping=class_to_idx_mapping,\n",
        "            load_pil=True\n",
        "        )\n",
        "        self.transform_weak = transform_weak\n",
        "        self.transform_strong = transform_strong\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pil_image, label = self.base_dataset[idx]\n",
        "        img_weak = self.transform_weak(pil_image)\n",
        "        img_strong = self.transform_strong(pil_image)\n",
        "        return img_weak, img_strong\n",
        "\n",
        "\n",
        "class OfficeHomeDomainDataset(Dataset):\n",
        "    def __init__(self, root_dir, domain_name, transform=None,\n",
        "                 split_ratios=(0.8, 0.1, 0.1), split_type='train',\n",
        "                 random_seed=RANDOM_SEED, class_to_idx_mapping=None, load_pil=False):\n",
        "        self.domain_path = os.path.join(root_dir, domain_name)\n",
        "        self.transform = transform\n",
        "        self.load_pil = load_pil\n",
        "        self.images_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        if class_to_idx_mapping is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = {}\n",
        "            current_idx = 0\n",
        "            for class_name_iter in sorted(os.listdir(self.domain_path)):\n",
        "                if os.path.isdir(os.path.join(self.domain_path, class_name_iter)):\n",
        "                    if class_name_iter not in self.class_to_idx:\n",
        "                        self.class_to_idx[class_name_iter] = current_idx\n",
        "                        self.idx_to_class[current_idx] = class_name_iter\n",
        "                        current_idx += 1\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx_mapping\n",
        "            self.idx_to_class = {v: k for k, v in class_to_idx_mapping.items()}\n",
        "\n",
        "        for class_name in sorted(os.listdir(self.domain_path)):\n",
        "            class_label_idx = self.class_to_idx.get(class_name)\n",
        "            if class_label_idx is None:\n",
        "                continue\n",
        "            class_path = os.path.join(self.domain_path, class_name)\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue\n",
        "\n",
        "            domain_class_images_paths = []\n",
        "            for img_name in sorted(os.listdir(class_path)):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    domain_class_images_paths.append(os.path.join(class_path, img_name))\n",
        "\n",
        "            np.random.seed(random_seed)\n",
        "            np.random.shuffle(domain_class_images_paths)\n",
        "\n",
        "            n_total = len(domain_class_images_paths)\n",
        "            n_train = int(n_total * split_ratios[0])\n",
        "            n_val = int(n_total * split_ratios[1])\n",
        "\n",
        "            if split_type == 'train':\n",
        "                selected_paths = domain_class_images_paths[:n_train]\n",
        "            elif split_type == 'val':\n",
        "                selected_paths = domain_class_images_paths[n_train:n_train + n_val]\n",
        "            elif split_type == 'test':\n",
        "                selected_paths = domain_class_images_paths[n_train + n_val:]\n",
        "            elif split_type == 'all':\n",
        "                selected_paths = domain_class_images_paths\n",
        "            else:\n",
        "                raise ValueError(\"split_type must be 'train', 'val', 'test', or 'all'\")\n",
        "\n",
        "            self.images_paths.extend(selected_paths)\n",
        "            self.labels.extend([class_label_idx] * len(selected_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images_paths[idx]\n",
        "        image_pil = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.load_pil:\n",
        "            return image_pil, label\n",
        "\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image_pil)\n",
        "        else:\n",
        "            image_tensor = transforms.ToTensor()(image_pil)\n",
        "        return image_tensor, label\n",
        "\n",
        "print(\"Dataset classes defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Global Class Mapping & Initial Data Check\n",
        "\n",
        "print(\"Attempting to create GLOBAL_CLASS_TO_IDX from Art domain...\")\n",
        "try:\n",
        "    temp_art_dataset_for_map = OfficeHomeDomainDataset(DATA_DIR, 'Art', split_type='all')\n",
        "    GLOBAL_CLASS_TO_IDX = temp_art_dataset_for_map.class_to_idx\n",
        "    GLOBAL_IDX_TO_CLASS = temp_art_dataset_for_map.idx_to_class\n",
        "    assert len(GLOBAL_CLASS_TO_IDX) == NUM_CLASSES, \\\n",
        "        f\"Mismatch: Expected {NUM_CLASSES} classes, found {len(GLOBAL_CLASS_TO_IDX)} in Art domain.\"\n",
        "    print(f\"GLOBAL_CLASS_TO_IDX created successfully with {len(GLOBAL_CLASS_TO_IDX)} classes.\")\n",
        "    del temp_art_dataset_for_map\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not create GLOBAL_CLASS_TO_IDX from Art domain: {e}\")\n",
        "    GLOBAL_CLASS_TO_IDX = None\n",
        "\n",
        "if GLOBAL_CLASS_TO_IDX:\n",
        "    try:\n",
        "        art_train_dataset_check = OfficeHomeDomainDataset(\n",
        "            DATA_DIR, SOURCE_DOMAIN_NAME,\n",
        "            transform=train_transform_strong,\n",
        "            split_type='train',\n",
        "            class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "        )\n",
        "        art_val_dataset_check = OfficeHomeDomainDataset(\n",
        "            DATA_DIR, SOURCE_DOMAIN_NAME,\n",
        "            transform=val_test_transform_weak,\n",
        "            split_type='val',\n",
        "            class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "        )\n",
        "        if len(art_train_dataset_check) > 0 and len(art_val_dataset_check) > 0:\n",
        "            print(f\"Successfully loaded check datasets for '{SOURCE_DOMAIN_NAME}': \"\n",
        "                  f\"Train size {len(art_train_dataset_check)}, Val size {len(art_val_dataset_check)}\")\n",
        "        else:\n",
        "            print(f\"Warning: Check datasets for '{SOURCE_DOMAIN_NAME}' are empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading check datasets for '{SOURCE_DOMAIN_NAME}': {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: ResNet-50 Backbone Loading Function\n",
        "\n",
        "def load_frozen_resnet_backbone(device=DEVICE):\n",
        "    \"\"\"Load ResNet-50 backbone with pretrained weights, frozen parameters.\n",
        "    Returns feature extractor without the final FC layer.\n",
        "    \"\"\"\n",
        "    resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    \n",
        "    # Remove the final FC layer - we'll use our own head\n",
        "    resnet.fc = nn.Identity()\n",
        "    \n",
        "    # Freeze all parameters\n",
        "    for param in resnet.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    resnet = resnet.to(device)\n",
        "    resnet.eval()\n",
        "    print(f\"Loaded and froze ResNet-50 backbone\")\n",
        "    return resnet\n",
        "\n",
        "\n",
        "def get_resnet_features(resnet_model, images):\n",
        "    \"\"\"Extract features from ResNet backbone.\n",
        "    Returns pooled features of shape (B, 2048).\n",
        "    \"\"\"\n",
        "    x = resnet_model.conv1(images)\n",
        "    x = resnet_model.bn1(x)\n",
        "    x = resnet_model.relu(x)\n",
        "    x = resnet_model.maxpool(x)\n",
        "    x = resnet_model.layer1(x)\n",
        "    x = resnet_model.layer2(x)\n",
        "    x = resnet_model.layer3(x)\n",
        "    x = resnet_model.layer4(x)\n",
        "    x = resnet_model.avgpool(x)\n",
        "    x = torch.flatten(x, 1)  # (B, 2048)\n",
        "    return x\n",
        "\n",
        "\n",
        "print(\"ResNet-50 backbone loading and feature extraction functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: LoRA for Convolutional Layers (ResNet adaptation)\n",
        "\n",
        "class LoRAConv2d(nn.Module):\n",
        "    \"\"\"LoRA adapter for Conv2d layers.\n",
        "    Uses 1x1 convolutions to create low-rank factorization.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_layer, rank, alpha, lora_dropout_p=0.0):\n",
        "        super().__init__()\n",
        "        self.in_channels = conv_layer.in_channels\n",
        "        self.out_channels = conv_layer.out_channels\n",
        "        self.kernel_size = conv_layer.kernel_size\n",
        "        self.stride = conv_layer.stride\n",
        "        self.padding = conv_layer.padding\n",
        "        self.dilation = conv_layer.dilation\n",
        "        self.groups = conv_layer.groups\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.lora_dropout = nn.Dropout2d(p=lora_dropout_p)\n",
        "\n",
        "        # Original weight and bias (frozen)\n",
        "        self.weight = nn.Parameter(conv_layer.weight.detach().clone(), requires_grad=False)\n",
        "        if conv_layer.bias is not None:\n",
        "            self.bias = nn.Parameter(conv_layer.bias.detach().clone(), requires_grad=False)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # LoRA matrices using 1x1 convolutions\n",
        "        # Down-projection: in_channels -> rank\n",
        "        self.lora_A = nn.Conv2d(self.in_channels, rank, kernel_size=1, bias=False)\n",
        "        # Up-projection: rank -> out_channels\n",
        "        self.lora_B = nn.Conv2d(rank, self.out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        # Initialization\n",
        "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B.weight)\n",
        "\n",
        "        self.scaling = self.alpha / self.rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original convolution\n",
        "        out_original = F.conv2d(\n",
        "            x, self.weight, self.bias,\n",
        "            stride=self.stride, padding=self.padding,\n",
        "            dilation=self.dilation, groups=self.groups\n",
        "        )\n",
        "\n",
        "        # LoRA path: Apply 1x1 convs as low-rank adaptation\n",
        "        lora_out = self.lora_A(x)\n",
        "        lora_out = self.lora_dropout(lora_out)\n",
        "        lora_out = self.lora_B(lora_out)\n",
        "\n",
        "        # Match spatial dimensions if needed\n",
        "        if lora_out.shape[-2:] != out_original.shape[-2:]:\n",
        "            lora_out = F.adaptive_avg_pool2d(lora_out, out_original.shape[-2:])\n",
        "\n",
        "        return out_original + lora_out * self.scaling\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'in_ch={self.in_channels}, out_ch={self.out_channels}, rank={self.rank}, alpha={self.alpha}'\n",
        "\n",
        "\n",
        "def inject_lora_to_resnet(resnet_model, rank, alpha, lora_dropout_p=0.0,\n",
        "                          target_layers=['layer3', 'layer4']):\n",
        "    \"\"\"Inject LoRA into specified ResNet layers.\n",
        "    By default, targets the last two stages (layer3, layer4) for efficiency.\n",
        "    \"\"\"\n",
        "    injected_count = 0\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        if not hasattr(resnet_model, layer_name):\n",
        "            continue\n",
        "\n",
        "        layer = getattr(resnet_model, layer_name)\n",
        "\n",
        "        for block_idx, block in enumerate(layer):\n",
        "            # Inject into conv1, conv2, conv3 of each Bottleneck block\n",
        "            for conv_name in ['conv1', 'conv2', 'conv3']:\n",
        "                if hasattr(block, conv_name):\n",
        "                    original_conv = getattr(block, conv_name)\n",
        "                    if isinstance(original_conv, nn.Conv2d):\n",
        "                        lora_conv = LoRAConv2d(original_conv, rank, alpha, lora_dropout_p)\n",
        "                        setattr(block, conv_name, lora_conv)\n",
        "                        injected_count += 1\n",
        "\n",
        "            # Also inject into downsample if present\n",
        "            if block.downsample is not None:\n",
        "                for i, module in enumerate(block.downsample):\n",
        "                    if isinstance(module, nn.Conv2d):\n",
        "                        lora_conv = LoRAConv2d(module, rank, alpha, lora_dropout_p)\n",
        "                        block.downsample[i] = lora_conv\n",
        "                        injected_count += 1\n",
        "\n",
        "    if injected_count == 0:\n",
        "        print(\"WARNING: No Conv2d layers found or replaced with LoRA.\")\n",
        "    else:\n",
        "        print(f\"Injected LoRA (rank={rank}, alpha={alpha}) into {injected_count} Conv2d layers in {target_layers}.\")\n",
        "\n",
        "    return resnet_model\n",
        "\n",
        "\n",
        "def set_batchnorm_affine_trainable(model, trainable=True):\n",
        "    \"\"\"Set BatchNorm affine parameters (weight, bias) to trainable/frozen.\"\"\"\n",
        "    bn_param_count = 0\n",
        "    for name, mod in model.named_modules():\n",
        "        if isinstance(mod, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "            if hasattr(mod, 'weight') and mod.weight is not None:\n",
        "                mod.weight.requires_grad = trainable\n",
        "                if trainable:\n",
        "                    bn_param_count += mod.weight.numel()\n",
        "            if hasattr(mod, 'bias') and mod.bias is not None:\n",
        "                mod.bias.requires_grad = trainable\n",
        "                if trainable:\n",
        "                    bn_param_count += mod.bias.numel()\n",
        "    status = \"trainable\" if trainable else \"frozen\"\n",
        "    print(f\"Set BatchNorm affine parameters {status}. Total BN params affected: {bn_param_count:,}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"LoRAConv2d class and injection function defined.\")\n",
        "\n",
        "# Test LoRA for ResNet\n",
        "x_test = torch.randn(2, 64, 56, 56)\n",
        "conv_test = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "lora_conv_test = LoRAConv2d(conv_test, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT)\n",
        "out_test = lora_conv_test(x_test)\n",
        "print(f\"Test LoRA Conv output shape: {out_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: DomainSpecificHead for ResNet\n",
        "\n",
        "class DomainSpecificHead(nn.Module):\n",
        "    def __init__(self, in_features=RESNET_EMBED_DIM, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "print(\"DomainSpecificHead class defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: DAD Module - Timestep Embedding & p_theta Denoiser MLP\n",
        "\n",
        "class SinusoidalTimestepEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_period=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_period = max_period\n",
        "\n",
        "    def forward(self, timesteps):\n",
        "        if timesteps.ndim == 0:\n",
        "            timesteps = timesteps.unsqueeze(0)\n",
        "        device = timesteps.device\n",
        "        half_dim = self.dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(self.max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32) / half_dim\n",
        "        ).to(device)\n",
        "        args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if self.dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "\n",
        "class DAD_P_Theta_MLP(nn.Module):\n",
        "    def __init__(self, feature_dim=RESNET_EMBED_DIM,\n",
        "                 timestep_embed_dim=DAD_P_THETA_TIMESTEP_EMBED_DIM,\n",
        "                 ts_embed_hidden_mult=DAD_P_THETA_HIDDEN_DIM_MULT,\n",
        "                 mlp_hidden_dim=DAD_P_THETA_MLP_HIDDEN_DIM):\n",
        "        super().__init__()\n",
        "        self.timestep_embed_dim_actual = timestep_embed_dim\n",
        "\n",
        "        self.timestep_encoder = nn.Sequential(\n",
        "            SinusoidalTimestepEmbedding(timestep_embed_dim),\n",
        "            nn.Linear(timestep_embed_dim, timestep_embed_dim * ts_embed_hidden_mult),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(timestep_embed_dim * ts_embed_hidden_mult, timestep_embed_dim * ts_embed_hidden_mult)\n",
        "        )\n",
        "\n",
        "        combined_dim = feature_dim + (timestep_embed_dim * ts_embed_hidden_mult)\n",
        "\n",
        "        # Larger MLP for ResNet's 2048-dim features\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(combined_dim, mlp_hidden_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim * 2, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noisy_features, timesteps_long):\n",
        "        timesteps_long = timesteps_long.to(noisy_features.device)\n",
        "        ts_embedding = self.timestep_encoder(timesteps_long)\n",
        "\n",
        "        if ts_embedding.ndim == 1 and noisy_features.ndim > 1:\n",
        "            ts_embedding = ts_embedding.unsqueeze(0).expand(noisy_features.shape[0], -1)\n",
        "\n",
        "        combined_input = torch.cat([noisy_features, ts_embedding], dim=-1)\n",
        "        predicted_noise = self.mlp(combined_input)\n",
        "        return predicted_noise\n",
        "\n",
        "\n",
        "print(\"DAD p_theta MLP denoiser and Timestep Embedding defined.\")\n",
        "\n",
        "# Test p_theta\n",
        "p_theta_test = DAD_P_Theta_MLP().to(DEVICE)\n",
        "test_noisy_feat = torch.randn(BATCH_SIZE, RESNET_EMBED_DIM).to(DEVICE)\n",
        "test_timesteps = torch.randint(0, DAD_K_STEPS, (BATCH_SIZE,), dtype=torch.long).to(DEVICE)\n",
        "pred_noise_test = p_theta_test(test_noisy_feat, test_timesteps)\n",
        "print(f\"p_theta test output shape: {pred_noise_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: DAD Diffusion Utilities\n",
        "\n",
        "def make_dad_schedule(beta_start=DAD_BETA_START, beta_end=DAD_BETA_END, num_steps=DAD_K_STEPS, device=DEVICE):\n",
        "    betas = torch.linspace(beta_start, beta_end, num_steps, dtype=torch.float32, device=device)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    return betas, alphas, alphas_cumprod\n",
        "\n",
        "\n",
        "if DEVICE.type == 'cuda' and not torch.cuda.is_available():\n",
        "    print(\"Warning: DEVICE is cuda but not available. DAD schedules on CPU.\")\n",
        "    _sched_device = torch.device('cpu')\n",
        "else:\n",
        "    _sched_device = DEVICE\n",
        "\n",
        "DAD_BETAS, DAD_ALPHAS, DAD_ALPHAS_CUMPROD = make_dad_schedule(device=_sched_device)\n",
        "\n",
        "\n",
        "def q_sample_dad(x_start, k_indices, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    k_indices = k_indices.long().to(DAD_ALPHAS_CUMPROD.device)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = torch.sqrt(DAD_ALPHAS_CUMPROD[k_indices])\n",
        "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - DAD_ALPHAS_CUMPROD[k_indices])\n",
        "\n",
        "    if x_start.ndim > 1 and sqrt_alphas_cumprod_t.ndim == 1:\n",
        "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, *([1] * (x_start.ndim - 1)))\n",
        "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, *([1] * (x_start.ndim - 1)))\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "\n",
        "def q_reconstruct_dad(x_k, epsilon_theta_hat, k_indices):\n",
        "    k_indices = k_indices.long().to(DAD_ALPHAS_CUMPROD.device)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = torch.sqrt(DAD_ALPHAS_CUMPROD[k_indices])\n",
        "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - DAD_ALPHAS_CUMPROD[k_indices])\n",
        "\n",
        "    if x_k.ndim > 1 and sqrt_alphas_cumprod_t.ndim == 1:\n",
        "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, *([1] * (x_k.ndim - 1)))\n",
        "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, *([1] * (x_k.ndim - 1)))\n",
        "\n",
        "    x0_hat = (x_k - sqrt_one_minus_alphas_cumprod_t * epsilon_theta_hat) / sqrt_alphas_cumprod_t\n",
        "    return x0_hat\n",
        "\n",
        "\n",
        "print(\"DAD diffusion utilities defined.\")\n",
        "print(f\"DAD schedules created on device: {DAD_BETAS.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Utility Functions (EMA Update, Replay Buffer)\n",
        "\n",
        "def update_ema_teacher_components(student_resnet, student_head, teacher_resnet, teacher_head, decay):\n",
        "    with torch.no_grad():\n",
        "        for stud_param, teach_param in zip(student_resnet.parameters(), teacher_resnet.parameters()):\n",
        "            if stud_param.requires_grad:\n",
        "                teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
        "\n",
        "        for stud_param, teach_param in zip(student_head.parameters(), teacher_head.parameters()):\n",
        "            teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, buffer_size, device=DEVICE):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.device = device\n",
        "        self.buffer_images = []\n",
        "        self.buffer_labels = []\n",
        "        self.position = 0\n",
        "\n",
        "    def add(self, images_tensor, labels_tensor):\n",
        "        batch_size = images_tensor.size(0)\n",
        "        for i in range(batch_size):\n",
        "            img = images_tensor[i].cpu()\n",
        "            lbl = labels_tensor[i].cpu()\n",
        "            if len(self.buffer_images) < self.buffer_size:\n",
        "                self.buffer_images.append(img)\n",
        "                self.buffer_labels.append(lbl)\n",
        "            else:\n",
        "                self.buffer_images[self.position] = img\n",
        "                self.buffer_labels[self.position] = lbl\n",
        "            self.position = (self.position + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer_images) < batch_size:\n",
        "            if not self.buffer_images:\n",
        "                return None, None\n",
        "            indices = np.random.choice(len(self.buffer_images), len(self.buffer_images), replace=False)\n",
        "        else:\n",
        "            indices = np.random.choice(len(self.buffer_images), batch_size, replace=False)\n",
        "\n",
        "        sampled_images = torch.stack([self.buffer_images[i] for i in indices]).to(self.device)\n",
        "        sampled_labels = torch.stack([self.buffer_labels[i] for i in indices]).to(self.device)\n",
        "        return sampled_images, sampled_labels.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer_images)\n",
        "\n",
        "\n",
        "print(\"Utility functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3 - Source Domain (Art) Supervised Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Source Domain - Dataset & DataLoader\n",
        "\n",
        "print(f\"\\n--- Preparing Source Domain: {SOURCE_DOMAIN_NAME} ---\")\n",
        "\n",
        "source_domain_model_save_dir = os.path.join(MODEL_SAVE_DIR_BASE, SOURCE_DOMAIN_NAME)\n",
        "os.makedirs(source_domain_model_save_dir, exist_ok=True)\n",
        "\n",
        "if GLOBAL_CLASS_TO_IDX is None:\n",
        "    raise RuntimeError(\"GLOBAL_CLASS_TO_IDX is not defined. Cannot proceed.\")\n",
        "\n",
        "source_train_dataset = OfficeHomeDomainDataset(\n",
        "    DATA_DIR, SOURCE_DOMAIN_NAME,\n",
        "    transform=train_transform_strong,\n",
        "    split_type='train',\n",
        "    class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        ")\n",
        "source_val_dataset = OfficeHomeDomainDataset(\n",
        "    DATA_DIR, SOURCE_DOMAIN_NAME,\n",
        "    transform=val_test_transform_weak,\n",
        "    split_type='val',\n",
        "    class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        ")\n",
        "\n",
        "source_train_loader = DataLoader(\n",
        "    source_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
        ")\n",
        "source_val_loader = DataLoader(\n",
        "    source_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Source domain '{SOURCE_DOMAIN_NAME}': Train size {len(source_train_dataset)}, Val size {len(source_val_dataset)}\")\n",
        "print(f\"Train loader batches: {len(source_train_loader)}, Val loader batches: {len(source_val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Source Domain - Model Instantiation & Optimizer\n",
        "\n",
        "# 1. Load base frozen ResNet-50\n",
        "base_resnet_source_train = load_frozen_resnet_backbone(device=DEVICE)\n",
        "\n",
        "# 2. Create a new LoRA-adapted ResNet for the source domain\n",
        "source_resnet_lora = copy.deepcopy(base_resnet_source_train)\n",
        "source_resnet_lora = inject_lora_to_resnet(\n",
        "    source_resnet_lora, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT\n",
        ")\n",
        "source_resnet_lora = set_batchnorm_affine_trainable(source_resnet_lora, trainable=True)\n",
        "source_resnet_lora = source_resnet_lora.to(DEVICE)\n",
        "\n",
        "# 3. Instantiate Source-Specific Head\n",
        "source_head = DomainSpecificHead(in_features=RESNET_EMBED_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# 4. Optimizer\n",
        "params_to_train_source = []\n",
        "for name, param in source_resnet_lora.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        params_to_train_source.append(param)\n",
        "params_to_train_source.extend(list(source_head.parameters()))\n",
        "\n",
        "optimizer_source = optim.AdamW(params_to_train_source, lr=ART_LR_LORA_HEAD_BN, weight_decay=0.05)\n",
        "criterion_source = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "num_total_steps_source = ART_EPOCHS * len(source_train_loader)\n",
        "scheduler_source = optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_source,\n",
        "    lr_lambda=lambda step: (1 - step / num_total_steps_source) ** 0.9\n",
        ")\n",
        "\n",
        "grad_scaler_source = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "print(f\"Source expert models (LoRA ResNet, Head) instantiated for '{SOURCE_DOMAIN_NAME}'.\")\n",
        "trainable_params_count_source = sum(p.numel() for p in params_to_train_source)\n",
        "print(f\"Total trainable parameters for source expert: {trainable_params_count_source:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Source Domain - Training Loop\n",
        "\n",
        "print(f\"\\n--- Training Source Expert ({SOURCE_DOMAIN_NAME}) ---\")\n",
        "best_source_val_acc = 0.0\n",
        "\n",
        "for epoch in range(ART_EPOCHS):\n",
        "    source_resnet_lora.train()\n",
        "    source_head.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    progress_bar = tqdm(source_train_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [{SOURCE_DOMAIN_NAME} Train]\")\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer_source.zero_grad()\n",
        "\n",
        "        with autocast_ctx():\n",
        "            features = get_resnet_features(source_resnet_lora, images)\n",
        "            logits = source_head(features)\n",
        "            loss = criterion_source(logits, labels)\n",
        "\n",
        "        grad_scaler_source.scale(loss).backward()\n",
        "        grad_scaler_source.step(optimizer_source)\n",
        "        grad_scaler_source.update()\n",
        "        scheduler_source.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        progress_bar.set_postfix(\n",
        "            loss=loss.item(),\n",
        "            acc=correct_predictions.double().item() / total_samples if total_samples > 0 else 0.0,\n",
        "            lr=optimizer_source.param_groups[0]['lr']\n",
        "        )\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    source_resnet_lora.eval()\n",
        "    source_head.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(source_val_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [{SOURCE_DOMAIN_NAME} Val]\", leave=False):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            with autocast_ctx():\n",
        "                features = get_resnet_features(source_resnet_lora, images)\n",
        "                logits = source_head(features)\n",
        "                loss = criterion_source(logits, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(logits, 1)\n",
        "            val_correct_predictions += torch.sum(preds == labels.data)\n",
        "            val_total_samples += images.size(0)\n",
        "\n",
        "    epoch_val_loss = val_loss / val_total_samples\n",
        "    epoch_val_acc = val_correct_predictions.double() / val_total_samples\n",
        "    print(f\"Epoch {epoch+1} Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    if epoch_val_acc > best_source_val_acc:\n",
        "        best_source_val_acc = epoch_val_acc\n",
        "        torch.save(source_resnet_lora.state_dict(), os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_resnet_lora_best.pth\"))\n",
        "        torch.save(source_head.state_dict(), os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_head_best.pth\"))\n",
        "        print(f\"    -> New best Val Acc: {best_source_val_acc:.4f}. Models saved.\")\n",
        "\n",
        "print(f\"\\nSource domain '{SOURCE_DOMAIN_NAME}' training finished. Best Val Acc: {best_source_val_acc:.4f}\")\n",
        "\n",
        "# Load best models\n",
        "source_resnet_lora.load_state_dict(torch.load(os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_resnet_lora_best.pth\")))\n",
        "source_head.load_state_dict(torch.load(os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_head_best.pth\")))\n",
        "source_resnet_lora.eval()\n",
        "source_head.eval()\n",
        "print(\"Best source expert models loaded and set to eval mode.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Baseline Validation of Source Expert on All Domains\n",
        "\n",
        "print(f\"\\n\\n--- Baseline Validation of Source Expert ('{SOURCE_DOMAIN_NAME}') on All Domains ---\")\n",
        "\n",
        "if 'source_resnet_lora' not in globals() or source_resnet_lora is None or \\\n",
        "   'source_head' not in globals() or source_head is None:\n",
        "    print(f\"Warning: Source expert ({SOURCE_DOMAIN_NAME}) models not available. Skipping baseline validation.\")\n",
        "else:\n",
        "    source_resnet_lora.eval()\n",
        "    source_head.eval()\n",
        "\n",
        "    baseline_accuracies_source_expert = {}\n",
        "    criterion_val_baseline = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    for domain_name_eval in ALL_TRAINABLE_DOMAIN_NAMES:\n",
        "        print(f\"\\n  Validating Source Expert ('{SOURCE_DOMAIN_NAME}') on '{domain_name_eval}' domain...\")\n",
        "\n",
        "        try:\n",
        "            val_dataset_current_domain = OfficeHomeDomainDataset(\n",
        "                DATA_DIR, domain_name_eval,\n",
        "                transform=val_test_transform_weak,\n",
        "                split_type='val',\n",
        "                class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "            )\n",
        "            if len(val_dataset_current_domain) == 0:\n",
        "                print(f\"    Warning: Validation dataset for '{domain_name_eval}' is empty. Skipping.\")\n",
        "                baseline_accuracies_source_expert[domain_name_eval] = 0.0\n",
        "                continue\n",
        "\n",
        "            val_loader_current_domain = DataLoader(\n",
        "                val_dataset_current_domain,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                num_workers=NUM_WORKERS,\n",
        "                pin_memory=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR: Could not load validation dataset for '{domain_name_eval}': {e}. Skipping.\")\n",
        "            baseline_accuracies_source_expert[domain_name_eval] = -1.0\n",
        "            continue\n",
        "\n",
        "        val_loss_domain = 0.0\n",
        "        val_correct_predictions_domain = 0\n",
        "        val_total_samples_domain = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader_current_domain, desc=f\"Val on {domain_name_eval}\", leave=False):\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "                with autocast_ctx():\n",
        "                    features = get_resnet_features(source_resnet_lora, images)\n",
        "                    logits = source_head(features)\n",
        "                    loss = criterion_val_baseline(logits, labels)\n",
        "\n",
        "                val_loss_domain += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(logits, 1)\n",
        "                val_correct_predictions_domain += torch.sum(preds == labels.data)\n",
        "                val_total_samples_domain += images.size(0)\n",
        "\n",
        "        if val_total_samples_domain > 0:\n",
        "            epoch_val_loss_domain = val_loss_domain / val_total_samples_domain\n",
        "            epoch_val_acc_domain = val_correct_predictions_domain.double() / val_total_samples_domain\n",
        "            baseline_accuracies_source_expert[domain_name_eval] = epoch_val_acc_domain.item() * 100\n",
        "            print(f\"    '{domain_name_eval}' Val Loss: {epoch_val_loss_domain:.4f}, Val Acc: {epoch_val_acc_domain.item()*100:.2f}%\")\n",
        "        else:\n",
        "            print(f\"    No samples processed for '{domain_name_eval}' validation.\")\n",
        "            baseline_accuracies_source_expert[domain_name_eval] = 0.0\n",
        "\n",
        "    print(\"\\n--- Source Expert Baseline Performance Summary ---\")\n",
        "    avg_baseline_acc = 0\n",
        "    count_valid_domains = 0\n",
        "    for domain, acc in baseline_accuracies_source_expert.items():\n",
        "        if acc != -1.0:\n",
        "            print(f\"  Accuracy of '{SOURCE_DOMAIN_NAME}' expert on '{domain}': {acc:.2f}%\")\n",
        "            if domain != SOURCE_DOMAIN_NAME:\n",
        "                avg_baseline_acc += acc\n",
        "                count_valid_domains += 1\n",
        "        else:\n",
        "            print(f\"  Accuracy of '{SOURCE_DOMAIN_NAME}' expert on '{domain}': ERROR\")\n",
        "\n",
        "    if count_valid_domains > 0:\n",
        "        avg_target_acc = avg_baseline_acc / count_valid_domains\n",
        "        print(f\"  Average Target Domain Accuracy (Source Expert): {avg_target_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 16: Prepare for Continual Learning - Store Experts\n",
        "\n",
        "adapted_experts = {}\n",
        "\n",
        "current_expert_resnet = copy.deepcopy(source_resnet_lora).cpu()\n",
        "current_expert_head = copy.deepcopy(source_head).cpu()\n",
        "current_expert_domain_name = SOURCE_DOMAIN_NAME\n",
        "\n",
        "# Global frozen ResNet backbone for DC head\n",
        "base_resnet_frozen_global = load_frozen_resnet_backbone(device=DEVICE)\n",
        "\n",
        "global_replay_buffer = None\n",
        "\n",
        "print(f\"Preparation for continual learning complete. Current expert: '{current_expert_domain_name}'.\")\n",
        "print(f\"Base frozen ResNet ('base_resnet_frozen_global') loaded for future use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: Continual Adaptation Loop for Target Domains\n",
        "\n",
        "for target_domain_idx, target_domain_name_current_loop in enumerate(TARGET_DOMAIN_NAMES_ORDERED):\n",
        "    \n",
        "    TARGET_DOMAIN_NAME_CURRENT = target_domain_name_current_loop\n",
        "    if target_domain_idx == 0 and current_expert_domain_name == SOURCE_DOMAIN_NAME:\n",
        "        PREVIOUS_DOMAIN_NAME = SOURCE_DOMAIN_NAME\n",
        "    elif target_domain_idx > 0 and current_expert_domain_name == TARGET_DOMAIN_NAMES_ORDERED[target_domain_idx-1]:\n",
        "        PREVIOUS_DOMAIN_NAME = current_expert_domain_name\n",
        "    else:\n",
        "        PREVIOUS_DOMAIN_NAME = current_expert_domain_name\n",
        "\n",
        "    print(f\"\\n\\n=== Starting Adaptation: {PREVIOUS_DOMAIN_NAME} -> {TARGET_DOMAIN_NAME_CURRENT} ===\")\n",
        "\n",
        "    target_model_save_dir = os.path.join(MODEL_SAVE_DIR_BASE, TARGET_DOMAIN_NAME_CURRENT)\n",
        "    os.makedirs(target_model_save_dir, exist_ok=True)\n",
        "\n",
        "    # --- 1. Datasets & DataLoaders ---\n",
        "    print(f\"  Loading datasets for Previous ('{PREVIOUS_DOMAIN_NAME}') and Target ('{TARGET_DOMAIN_NAME_CURRENT}')...\")\n",
        "    previous_domain_train_dataset = OfficeHomeDomainDataset(\n",
        "        DATA_DIR, PREVIOUS_DOMAIN_NAME, transform=train_transform_strong,\n",
        "        split_type='train', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "    )\n",
        "    previous_domain_train_loader = DataLoader(\n",
        "        previous_domain_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
        "    )\n",
        "\n",
        "    target_unlabeled_loader_for_ema_shot = DataLoader(\n",
        "        OfficeHomeDomainDataset(DATA_DIR, TARGET_DOMAIN_NAME_CURRENT, transform=val_test_transform_weak,\n",
        "                                split_type='train', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
        "    )\n",
        "    fixmatch_target_dataset = FixMatchOfficeHomeDataset(\n",
        "        DATA_DIR, TARGET_DOMAIN_NAME_CURRENT,\n",
        "        transform_weak=val_test_transform_weak,\n",
        "        transform_strong=train_transform_strong,\n",
        "        split_type='train',\n",
        "        class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "    )\n",
        "    fixmatch_target_loader = DataLoader(\n",
        "        fixmatch_target_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
        "    )\n",
        "    target_val_dataset = OfficeHomeDomainDataset(\n",
        "        DATA_DIR, TARGET_DOMAIN_NAME_CURRENT, transform=val_test_transform_weak,\n",
        "        split_type='val', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
        "    )\n",
        "    target_val_loader = DataLoader(target_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"  Previous domain ('{PREVIOUS_DOMAIN_NAME}') train loader: {len(previous_domain_train_loader)} batches.\")\n",
        "    print(f\"  Target domain ('{TARGET_DOMAIN_NAME_CURRENT}'):\")\n",
        "    print(f\"    EMA/SHOT loader dataset size: {len(target_unlabeled_loader_for_ema_shot.dataset)}\")\n",
        "    print(f\"    FixMatch loader dataset size: {len(fixmatch_target_loader.dataset)}\")\n",
        "    print(f\"    Val dataset size: {len(target_val_dataset)}\")\n",
        "\n",
        "    # --- 2. Model Initialization ---\n",
        "    print(f\"  Initializing Student and Teacher models for '{TARGET_DOMAIN_NAME_CURRENT}'...\")\n",
        "    student_resnet = copy.deepcopy(base_resnet_frozen_global)\n",
        "    student_resnet = inject_lora_to_resnet(student_resnet, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT)\n",
        "\n",
        "    prev_expert_resnet_state_dict = current_expert_resnet.cpu().state_dict()\n",
        "    student_resnet_state_dict_new = student_resnet.state_dict()\n",
        "    load_dict_student_resnet_warm_start = {}\n",
        "    for k, v in prev_expert_resnet_state_dict.items():\n",
        "        if ('lora_' in k or ('bn' in k.lower() and (k.endswith('.weight') or k.endswith('.bias')))) and \\\n",
        "           k in student_resnet_state_dict_new and student_resnet_state_dict_new[k].shape == v.shape:\n",
        "            load_dict_student_resnet_warm_start[k] = v\n",
        "    if load_dict_student_resnet_warm_start:\n",
        "        missing_keys, unexpected_keys = student_resnet.load_state_dict(load_dict_student_resnet_warm_start, strict=False)\n",
        "        print(f\"  Loaded LoRA/BN from previous expert '{current_expert_domain_name}'. Missing: {len(missing_keys)}, Unexpected: {len(unexpected_keys)}\")\n",
        "    else:\n",
        "        print(f\"  Warning: No LoRA/BN weights loaded from previous expert '{current_expert_domain_name}'.\")\n",
        "\n",
        "    student_resnet = set_batchnorm_affine_trainable(student_resnet, trainable=True)\n",
        "    student_resnet = student_resnet.to(DEVICE)\n",
        "\n",
        "    student_head = DomainSpecificHead(in_features=RESNET_EMBED_DIM, num_classes=NUM_CLASSES)\n",
        "    student_head.load_state_dict(current_expert_head.cpu().state_dict())\n",
        "    student_head = student_head.to(DEVICE)\n",
        "\n",
        "    teacher_resnet = copy.deepcopy(student_resnet).to(DEVICE)\n",
        "    teacher_head = copy.deepcopy(student_head).to(DEVICE)\n",
        "    for param in teacher_resnet.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in teacher_head.parameters():\n",
        "        param.requires_grad = False\n",
        "    teacher_resnet.eval()\n",
        "    teacher_head.eval()\n",
        "    update_ema_teacher_components(student_resnet, student_head, teacher_resnet, teacher_head, 0.0)\n",
        "\n",
        "    p_theta_dad = DAD_P_Theta_MLP().to(DEVICE)\n",
        "    print(f\"  Student, Teacher, and DAD p_theta models ready for '{TARGET_DOMAIN_NAME_CURRENT}'.\")\n",
        "\n",
        "    # --- 3. Optimizers ---\n",
        "    params_to_train_student = []\n",
        "    for name, param in student_resnet.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            params_to_train_student.append(param)\n",
        "    params_to_train_student.extend(list(student_head.parameters()))\n",
        "\n",
        "    optimizer_student = optim.AdamW(params_to_train_student, lr=ADAPT_LR_LORA_HEAD_BN, weight_decay=0.05)\n",
        "    optimizer_p_theta = optim.SGD(p_theta_dad.parameters(), lr=ADAPT_LR_P_THETA, momentum=0.9, weight_decay=4.5e-3)\n",
        "\n",
        "    grad_scaler_adapt = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "    criterion_adapt_ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    criterion_adapt_mse = nn.MSELoss()\n",
        "    print(\"  Optimizers created.\")\n",
        "\n",
        "    # --- 4. DAD LTR Pre-training ---\n",
        "    print(f\"  Starting DAD LTR Pre-training for p_theta ({PREVIOUS_DOMAIN_NAME} -> {TARGET_DOMAIN_NAME_CURRENT})...\")\n",
        "    p_theta_dad.train()\n",
        "    student_resnet.eval()\n",
        "    student_head.eval()\n",
        "    for ltr_epoch in range(ADAPT_LTR_EPOCHS):\n",
        "        ltr_running_loss = 0.0\n",
        "        progress_bar_ltr = tqdm(target_unlabeled_loader_for_ema_shot, desc=f\"LTR Epoch {ltr_epoch+1}/{ADAPT_LTR_EPOCHS} for {TARGET_DOMAIN_NAME_CURRENT}\", leave=False)\n",
        "        for target_images_weak, _ in progress_bar_ltr:\n",
        "            target_images_weak = target_images_weak.to(DEVICE)\n",
        "            optimizer_p_theta.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                F_T_cls = get_resnet_features(student_resnet, target_images_weak).to(torch.float32)\n",
        "            t_indices = torch.randint(0, DAD_K_STEPS, (F_T_cls.size(0),), device=DEVICE, dtype=torch.long)\n",
        "            noise = torch.randn_like(F_T_cls)\n",
        "            F_T_noisy = q_sample_dad(F_T_cls, t_indices, noise)\n",
        "            predicted_noise = p_theta_dad(F_T_noisy, t_indices)\n",
        "            loss_ltr = criterion_adapt_mse(predicted_noise, noise)\n",
        "            loss_ltr.backward()\n",
        "            optimizer_p_theta.step()\n",
        "            ltr_running_loss += loss_ltr.item() * target_images_weak.size(0)\n",
        "            progress_bar_ltr.set_postfix(ltr_loss=loss_ltr.item())\n",
        "        avg_ltr_loss = ltr_running_loss / len(target_unlabeled_loader_for_ema_shot.dataset) if len(target_unlabeled_loader_for_ema_shot.dataset) > 0 else 0\n",
        "        print(f\"  LTR Epoch {ltr_epoch+1} ({TARGET_DOMAIN_NAME_CURRENT}) Avg Loss: {avg_ltr_loss:.4f}\")\n",
        "    print(f\"  DAD LTR Pre-training for '{TARGET_DOMAIN_NAME_CURRENT}' finished.\")\n",
        "\n",
        "    # --- 5. Main Adaptation Loop ---\n",
        "    print(f\"  Starting Main Adaptation Loop for '{TARGET_DOMAIN_NAME_CURRENT}'...\")\n",
        "    \n",
        "    best_target_val_acc = 0.0\n",
        "    val_loss_at_best_acc = float('inf')\n",
        "    patience_counter_adapt = 0\n",
        "\n",
        "    num_total_adapt_optimizer_steps_current = DAD_K_STEPS * (ADAPT_MLS_R_ITER + 1)\n",
        "    scheduler_student_adapt = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer_student, T_max=num_total_adapt_optimizer_steps_current, eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    source_iter = iter(previous_domain_train_loader)\n",
        "    target_iter_ema_shot = iter(target_unlabeled_loader_for_ema_shot)\n",
        "    fixmatch_iter = iter(fixmatch_target_loader)\n",
        "    outer_progress_bar = tqdm(range(DAD_K_STEPS), desc=f\"DAD Steps for {TARGET_DOMAIN_NAME_CURRENT}\")\n",
        "\n",
        "    for k_dad_step_idx in outer_progress_bar:\n",
        "        for mls_iter_idx in range(ADAPT_MLS_R_ITER):\n",
        "            try:\n",
        "                source_images, source_labels = next(source_iter)\n",
        "            except StopIteration:\n",
        "                source_iter = iter(previous_domain_train_loader)\n",
        "                source_images, source_labels = next(source_iter)\n",
        "            source_images, source_labels = source_images.to(DEVICE), source_labels.to(DEVICE)\n",
        "\n",
        "            p_theta_dad.train()\n",
        "            student_resnet.eval()\n",
        "            student_head.eval()\n",
        "            with torch.no_grad():\n",
        "                F_S_cls = get_resnet_features(student_resnet, source_images).to(torch.float32)\n",
        "            t_k_current_long = torch.full((F_S_cls.size(0),), k_dad_step_idx, device=DEVICE, dtype=torch.long)\n",
        "            noise_cd = torch.randn_like(F_S_cls)\n",
        "            F_S_cls_noisy_cd = q_sample_dad(F_S_cls, t_k_current_long, noise_cd)\n",
        "            predicted_noise_cd = p_theta_dad(F_S_cls_noisy_cd, t_k_current_long)\n",
        "            with torch.no_grad():\n",
        "                x0_hat_cd = q_reconstruct_dad(F_S_cls_noisy_cd, predicted_noise_cd, t_k_current_long)\n",
        "            with autocast_ctx():\n",
        "                logits_for_ptheta_update = student_head(x0_hat_cd)\n",
        "            loss_cd = criterion_adapt_ce(logits_for_ptheta_update, source_labels)\n",
        "            optimizer_p_theta.zero_grad()\n",
        "            loss_cd.backward()\n",
        "            optimizer_p_theta.step()\n",
        "\n",
        "            p_theta_dad.eval()\n",
        "            student_resnet.train()\n",
        "            student_head.train()\n",
        "            with torch.no_grad():\n",
        "                predicted_noise_dc = p_theta_dad(F_S_cls_noisy_cd, t_k_current_long)\n",
        "                x0_hat_dc = q_reconstruct_dad(F_S_cls_noisy_cd, predicted_noise_dc, t_k_current_long)\n",
        "            with autocast_ctx():\n",
        "                logits_for_student_update = student_head(x0_hat_dc.detach())\n",
        "                loss_dc_val = criterion_adapt_ce(logits_for_student_update, source_labels)\n",
        "            optimizer_student.zero_grad()\n",
        "            grad_scaler_adapt.scale(loss_dc_val).backward()\n",
        "            grad_scaler_adapt.step(optimizer_student)\n",
        "            grad_scaler_adapt.update()\n",
        "            scheduler_student_adapt.step()\n",
        "\n",
        "        current_mls_postfix = {\"MLS_D->C\": f\"{loss_dc_val.item():.3f}\", \"MLS_C->D\": f\"{loss_cd.item():.3f}\"}\n",
        "        outer_progress_bar.set_postfix(current_mls_postfix)\n",
        "\n",
        "        # EMA, FixMatch, SHOT updates\n",
        "        student_resnet.train()\n",
        "        student_head.train()\n",
        "        try:\n",
        "            target_images_for_ema_shot, _ = next(target_iter_ema_shot)\n",
        "        except StopIteration:\n",
        "            target_iter_ema_shot = iter(target_unlabeled_loader_for_ema_shot)\n",
        "            target_images_for_ema_shot, _ = next(target_iter_ema_shot)\n",
        "        target_images_for_ema_shot = target_images_for_ema_shot.to(DEVICE)\n",
        "        try:\n",
        "            target_images_weak_fixmatch, target_images_strong_fixmatch = next(fixmatch_iter)\n",
        "        except StopIteration:\n",
        "            fixmatch_iter = iter(fixmatch_target_loader)\n",
        "            target_images_weak_fixmatch, target_images_strong_fixmatch = next(fixmatch_iter)\n",
        "        target_images_weak_fixmatch = target_images_weak_fixmatch.to(DEVICE)\n",
        "        target_images_strong_fixmatch = target_images_strong_fixmatch.to(DEVICE)\n",
        "\n",
        "        optimizer_student.zero_grad()\n",
        "        loss_pl_val_ema = torch.tensor(0.0, device=DEVICE)\n",
        "        loss_fixmatch_val_calc = torch.tensor(0.0, device=DEVICE)\n",
        "        current_progress_ratio = k_dad_step_idx / max(1, DAD_K_STEPS - 1)\n",
        "        current_pseudo_label_thresh = PSEUDO_LABEL_THRESHOLD_START + \\\n",
        "                                     (PSEUDO_LABEL_THRESHOLD_END - PSEUDO_LABEL_THRESHOLD_START) * current_progress_ratio\n",
        "        num_pseudo_labels_ema = 0\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            teacher_resnet.eval()\n",
        "            teacher_head.eval()\n",
        "            F_T_teacher = get_resnet_features(teacher_resnet, target_images_for_ema_shot)\n",
        "            logits_teacher = teacher_head(F_T_teacher)\n",
        "            probs_teacher = F.softmax(logits_teacher, dim=1)\n",
        "            max_confidence_values, predicted_class_indices = torch.max(probs_teacher, dim=1)\n",
        "            confidence_mask_ema = max_confidence_values >= current_pseudo_label_thresh\n",
        "            pseudo_labels_for_loss = predicted_class_indices\n",
        "        if confidence_mask_ema.any():\n",
        "            num_pseudo_labels_ema = confidence_mask_ema.sum().item()\n",
        "            with autocast_ctx():\n",
        "                selected_target_images_ema = target_images_for_ema_shot[confidence_mask_ema]\n",
        "                F_T_student_ema = get_resnet_features(student_resnet, selected_target_images_ema)\n",
        "                logits_student_on_pseudo = student_head(F_T_student_ema)\n",
        "                loss_pl_val_ema = criterion_adapt_ce(logits_student_on_pseudo, pseudo_labels_for_loss[confidence_mask_ema])\n",
        "        num_pseudo_labels_fixmatch = 0\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            F_T_student_weak_fixmatch = get_resnet_features(student_resnet, target_images_weak_fixmatch)\n",
        "            logits_weak_student_fixmatch = student_head(F_T_student_weak_fixmatch)\n",
        "            probs_weak_student_fixmatch = F.softmax(logits_weak_student_fixmatch, dim=1)\n",
        "            max_probs_weak_fixmatch, pseudo_labels_weak_fixmatch = torch.max(probs_weak_student_fixmatch, dim=1)\n",
        "            confidence_mask_fixmatch = max_probs_weak_fixmatch >= FIXMATCH_CONF_THRESHOLD\n",
        "        if confidence_mask_fixmatch.any():\n",
        "            num_pseudo_labels_fixmatch = confidence_mask_fixmatch.sum().item()\n",
        "            with autocast_ctx():\n",
        "                selected_target_images_strong_fixmatch = target_images_strong_fixmatch[confidence_mask_fixmatch]\n",
        "                F_T_student_strong_fixmatch = get_resnet_features(student_resnet, selected_target_images_strong_fixmatch)\n",
        "                logits_student_on_strong_fixmatch = student_head(F_T_student_strong_fixmatch)\n",
        "                loss_fixmatch_val_calc = criterion_adapt_ce(logits_student_on_strong_fixmatch, pseudo_labels_weak_fixmatch[confidence_mask_fixmatch])\n",
        "                loss_fixmatch_val_calc = FIXMATCH_LAMBDA * loss_fixmatch_val_calc\n",
        "        loss_for_head_update = torch.tensor(0.0, device=DEVICE)\n",
        "        if loss_pl_val_ema.item() > 0:\n",
        "            loss_for_head_update += loss_pl_val_ema\n",
        "        if loss_fixmatch_val_calc.item() > 0:\n",
        "            loss_for_head_update += loss_fixmatch_val_calc\n",
        "        if loss_for_head_update.item() > 0:\n",
        "            grad_scaler_adapt.scale(loss_for_head_update).backward(retain_graph=True)\n",
        "\n",
        "        # SHOT loss\n",
        "        _temp_head_training_mode_shot = student_head.training\n",
        "        _temp_head_params_req_grad_shot = [p.requires_grad for p in student_head.parameters()]\n",
        "        student_head.eval()\n",
        "        for p_head in student_head.parameters():\n",
        "            p_head.requires_grad = False\n",
        "        with autocast_ctx():\n",
        "            F_T_student_shot = get_resnet_features(student_resnet, target_images_for_ema_shot)\n",
        "            logits_student_shot = student_head(F_T_student_shot)\n",
        "            probs_student_shot = F.softmax(logits_student_shot, dim=1)\n",
        "            loss_cond_ent = - (probs_student_shot * torch.log(probs_student_shot + 1e-9)).sum(1).mean()\n",
        "            mean_probs_shot = probs_student_shot.mean(0)\n",
        "            loss_ent_max = - (mean_probs_shot * torch.log(mean_probs_shot + 1e-9)).sum()\n",
        "            loss_shot_val_calculated = SHOT_LAMBDA_COND_ENT * loss_cond_ent + SHOT_LAMBDA_ENT_MAX * loss_ent_max\n",
        "        if loss_shot_val_calculated.item() != 0:\n",
        "            grad_scaler_adapt.scale(loss_shot_val_calculated).backward()\n",
        "        student_head.train(_temp_head_training_mode_shot)\n",
        "        for i_param, p_rg_status_shot in enumerate(_temp_head_params_req_grad_shot):\n",
        "            list(student_head.parameters())[i_param].requires_grad = p_rg_status_shot\n",
        "        if loss_for_head_update.item() > 0 or loss_shot_val_calculated.item() > 0:\n",
        "            grad_scaler_adapt.step(optimizer_student)\n",
        "            grad_scaler_adapt.update()\n",
        "        scheduler_student_adapt.step()\n",
        "        update_ema_teacher_components(student_resnet, student_head, teacher_resnet, teacher_head, EMA_DECAY)\n",
        "\n",
        "        # Validation at intervals\n",
        "        if (k_dad_step_idx + 1) % (DAD_K_STEPS // 10) == 0 or k_dad_step_idx == DAD_K_STEPS - 1:\n",
        "            student_resnet.eval()\n",
        "            student_head.eval()\n",
        "            val_loss_target_epoch = 0.0\n",
        "            val_correct_target_epoch = 0\n",
        "            val_total_target_epoch = 0\n",
        "            with torch.no_grad():\n",
        "                for images_val, labels_val in target_val_loader:\n",
        "                    images_val, labels_val = images_val.to(DEVICE), labels_val.to(DEVICE)\n",
        "                    with autocast_ctx():\n",
        "                        features_val = get_resnet_features(student_resnet, images_val)\n",
        "                        logits_val = student_head(features_val)\n",
        "                        loss_val = criterion_adapt_ce(logits_val, labels_val)\n",
        "                    val_loss_target_epoch += loss_val.item() * images_val.size(0)\n",
        "                    _, preds_val = torch.max(logits_val, 1)\n",
        "                    val_correct_target_epoch += torch.sum(preds_val == labels_val.data)\n",
        "                    val_total_target_epoch += images_val.size(0)\n",
        "            epoch_val_loss_target = val_loss_target_epoch / val_total_target_epoch if val_total_target_epoch > 0 else 0\n",
        "            epoch_val_acc_target = val_correct_target_epoch.double() / val_total_target_epoch if val_total_target_epoch > 0 else 0.0\n",
        "            print(f\"  DAD Step {k_dad_step_idx+1}/{DAD_K_STEPS} - Target '{TARGET_DOMAIN_NAME_CURRENT}' Val Loss: {epoch_val_loss_target:.4f}, Val Acc: {epoch_val_acc_target.item():.4f}\")\n",
        "\n",
        "            if epoch_val_acc_target.item() > best_target_val_acc and val_total_target_epoch > 0:\n",
        "                best_target_val_acc = epoch_val_acc_target.item()\n",
        "                val_loss_at_best_acc = epoch_val_loss_target\n",
        "                torch.save(student_resnet.state_dict(), os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_resnet_lora_best.pth\"))\n",
        "                torch.save(student_head.state_dict(), os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_head_best.pth\"))\n",
        "                print(f\"    -> New best Val Acc for '{TARGET_DOMAIN_NAME_CURRENT}': {best_target_val_acc:.4f}. Models saved.\")\n",
        "                patience_counter_adapt = 0\n",
        "            elif val_total_target_epoch > 0:\n",
        "                patience_counter_adapt += 1\n",
        "                print(f\"    Val acc did not improve. Patience: {patience_counter_adapt}/{EARLY_STOPPING_PATIENCE_ADAPT}\")\n",
        "\n",
        "        if patience_counter_adapt >= EARLY_STOPPING_PATIENCE_ADAPT:\n",
        "            print(f\"  Early stopping triggered for '{TARGET_DOMAIN_NAME_CURRENT}' after {k_dad_step_idx+1} DAD steps.\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nAdaptation to '{TARGET_DOMAIN_NAME_CURRENT}' finished. Best Val Acc: {best_target_val_acc:.4f}\")\n",
        "\n",
        "    if os.path.exists(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_resnet_lora_best.pth\")):\n",
        "        student_resnet.load_state_dict(torch.load(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_resnet_lora_best.pth\")))\n",
        "        student_head.load_state_dict(torch.load(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_head_best.pth\")))\n",
        "        print(f\"Loaded best saved models for {TARGET_DOMAIN_NAME_CURRENT}.\")\n",
        "    student_resnet.eval()\n",
        "    student_head.eval()\n",
        "\n",
        "    adapted_experts[TARGET_DOMAIN_NAME_CURRENT] = {\n",
        "        'resnet': copy.deepcopy(student_resnet).cpu(),\n",
        "        'head': copy.deepcopy(student_head).cpu()\n",
        "    }\n",
        "    current_expert_resnet = copy.deepcopy(student_resnet).cpu()\n",
        "    current_expert_head = copy.deepcopy(student_head).cpu()\n",
        "    current_expert_domain_name = TARGET_DOMAIN_NAME_CURRENT\n",
        "    print(f\"Best models for '{TARGET_DOMAIN_NAME_CURRENT}' stored. Ready for next adaptation.\")\n",
        "\n",
        "print(\"\\n\\n=== All Target Domain Adaptations Complete ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5 - Domain Classifier Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 18: Domain Classifier - Prepare Multi-Domain Dataset & DataLoader\n",
        "\n",
        "print(\"\\n\\n=== Part 5: Domain Classifier Training ===\")\n",
        "\n",
        "num_total_domains_for_dc = len(ALL_TRAINABLE_DOMAIN_NAMES)\n",
        "domain_name_to_idx_dc = {name: i for i, name in enumerate(ALL_TRAINABLE_DOMAIN_NAMES)}\n",
        "domain_idx_to_name_dc = {i: name for i, name in enumerate(ALL_TRAINABLE_DOMAIN_NAMES)}\n",
        "print(f\"Domain Classifier - Domain to Index Mapping: {domain_name_to_idx_dc}\")\n",
        "\n",
        "\n",
        "class MultiDomainDatasetForDC(Dataset):\n",
        "    def __init__(self, root_dir, all_domain_names, domain_to_idx_map,\n",
        "                 class_to_idx_overall_map, transform, split_type='train'):\n",
        "        self.images_paths = []\n",
        "        self.domain_labels = []\n",
        "\n",
        "        for domain_name in all_domain_names:\n",
        "            domain_idx = domain_to_idx_map.get(domain_name)\n",
        "            if domain_idx is None:\n",
        "                continue\n",
        "\n",
        "            temp_domain_dataset = OfficeHomeDomainDataset(\n",
        "                root_dir=root_dir, domain_name=domain_name,\n",
        "                transform=None,\n",
        "                split_type=split_type,\n",
        "                class_to_idx_mapping=class_to_idx_overall_map,\n",
        "                load_pil=False\n",
        "            )\n",
        "            self.images_paths.extend(temp_domain_dataset.images_paths)\n",
        "            self.domain_labels.extend([domain_idx] * len(temp_domain_dataset.images_paths))\n",
        "            print(f\"  Added {len(temp_domain_dataset.images_paths)} images from '{domain_name}' for DC training.\")\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images_paths[idx]\n",
        "        image_pil = Image.open(img_path).convert('RGB')\n",
        "        domain_label = self.domain_labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image_pil)\n",
        "        else:\n",
        "            image_tensor = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        return image_tensor, torch.tensor(domain_label).long()\n",
        "\n",
        "\n",
        "dc_train_dataset = MultiDomainDatasetForDC(\n",
        "    DATA_DIR, ALL_TRAINABLE_DOMAIN_NAMES, domain_name_to_idx_dc,\n",
        "    GLOBAL_CLASS_TO_IDX, transform=val_test_transform_weak, split_type='train'\n",
        ")\n",
        "dc_val_dataset = MultiDomainDatasetForDC(\n",
        "    DATA_DIR, ALL_TRAINABLE_DOMAIN_NAMES, domain_name_to_idx_dc,\n",
        "    GLOBAL_CLASS_TO_IDX, transform=val_test_transform_weak, split_type='val'\n",
        ")\n",
        "\n",
        "dc_train_loader = DataLoader(dc_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "dc_val_loader = DataLoader(dc_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Domain Classifier: Train size {len(dc_train_dataset)}, Val size {len(dc_val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 19: Domain Classifier - Model, Optimizer, Training Loop\n",
        "\n",
        "if 'base_resnet_frozen_global' not in globals() or base_resnet_frozen_global is None:\n",
        "    print(\"Re-loading base_resnet_frozen_global.\")\n",
        "    base_resnet_frozen_global = load_frozen_resnet_backbone(device=DEVICE)\n",
        "\n",
        "domain_classifier_head = DomainSpecificHead(\n",
        "    in_features=RESNET_EMBED_DIM,\n",
        "    num_classes=num_total_domains_for_dc\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer_dc = optim.AdamW(domain_classifier_head.parameters(), lr=DC_HEAD_LR, weight_decay=0.01)\n",
        "criterion_dc = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "grad_scaler_dc = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "scheduler_dc = optim.lr_scheduler.CosineAnnealingLR(optimizer_dc, T_max=DC_HEAD_EPOCHS * len(dc_train_loader))\n",
        "\n",
        "print(f\"Training Domain Classifier Head for {DC_HEAD_EPOCHS} epochs...\")\n",
        "best_dc_val_acc = 0.0\n",
        "dc_model_save_path = os.path.join(MODEL_SAVE_DIR_BASE, \"domain_classifier_head_best.pth\")\n",
        "\n",
        "for epoch in range(DC_HEAD_EPOCHS):\n",
        "    domain_classifier_head.train()\n",
        "    base_resnet_frozen_global.eval()\n",
        "\n",
        "    running_loss_dc = 0.0\n",
        "    correct_preds_dc = 0\n",
        "    total_samples_dc = 0\n",
        "\n",
        "    progress_bar_dc = tqdm(dc_train_loader, desc=f\"DC Epoch {epoch+1}/{DC_HEAD_EPOCHS} [Train]\")\n",
        "    for images, domain_labels_true in progress_bar_dc:\n",
        "        images, domain_labels_true = images.to(DEVICE), domain_labels_true.to(DEVICE)\n",
        "\n",
        "        optimizer_dc.zero_grad()\n",
        "        with autocast_ctx():\n",
        "            with torch.no_grad():\n",
        "                features_dc = get_resnet_features(base_resnet_frozen_global, images)\n",
        "            domain_logits = domain_classifier_head(features_dc)\n",
        "            loss_dc = criterion_dc(domain_logits, domain_labels_true)\n",
        "\n",
        "        grad_scaler_dc.scale(loss_dc).backward()\n",
        "        grad_scaler_dc.step(optimizer_dc)\n",
        "        grad_scaler_dc.update()\n",
        "        scheduler_dc.step()\n",
        "\n",
        "        running_loss_dc += loss_dc.item() * images.size(0)\n",
        "        _, preds_dc = torch.max(domain_logits, 1)\n",
        "        correct_preds_dc += torch.sum(preds_dc == domain_labels_true.data)\n",
        "        total_samples_dc += images.size(0)\n",
        "        progress_bar_dc.set_postfix(loss=loss_dc.item(), acc=correct_preds_dc.double().item()/total_samples_dc)\n",
        "\n",
        "    epoch_loss_dc = running_loss_dc / total_samples_dc\n",
        "    epoch_acc_dc = correct_preds_dc.double() / total_samples_dc\n",
        "    print(f\"DC Epoch {epoch+1} Train Loss: {epoch_loss_dc:.4f}, Train Acc: {epoch_acc_dc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    domain_classifier_head.eval()\n",
        "    val_loss_dc_epoch = 0.0\n",
        "    val_correct_dc = 0\n",
        "    val_total_dc = 0\n",
        "    with torch.no_grad():\n",
        "        for images_val, domain_labels_val_true in tqdm(dc_val_loader, desc=f\"DC Epoch {epoch+1}/{DC_HEAD_EPOCHS} [Val]\", leave=False):\n",
        "            images_val, domain_labels_val_true = images_val.to(DEVICE), domain_labels_val_true.to(DEVICE)\n",
        "            with autocast_ctx():\n",
        "                features_val_dc = get_resnet_features(base_resnet_frozen_global, images_val)\n",
        "                domain_logits_val = domain_classifier_head(features_val_dc)\n",
        "                loss_val_dc = criterion_dc(domain_logits_val, domain_labels_val_true)\n",
        "\n",
        "            val_loss_dc_epoch += loss_val_dc.item() * images_val.size(0)\n",
        "            _, preds_val_dc = torch.max(domain_logits_val, 1)\n",
        "            val_correct_dc += torch.sum(preds_val_dc == domain_labels_val_true.data)\n",
        "            val_total_dc += images_val.size(0)\n",
        "\n",
        "    epoch_val_loss_dc = val_loss_dc_epoch / val_total_dc if val_total_dc > 0 else 0\n",
        "    epoch_val_acc_dc = val_correct_dc.double() / val_total_dc if val_total_dc > 0 else 0.0\n",
        "    print(f\"DC Epoch {epoch+1} Val Loss: {epoch_val_loss_dc:.4f}, Val Acc: {epoch_val_acc_dc:.4f}\")\n",
        "\n",
        "    if epoch_val_acc_dc > best_dc_val_acc:\n",
        "        best_dc_val_acc = epoch_val_acc_dc\n",
        "        torch.save(domain_classifier_head.state_dict(), dc_model_save_path)\n",
        "        print(f\"    -> New best DC Val Acc: {best_dc_val_acc:.4f}. DC Head saved.\")\n",
        "\n",
        "print(f\"\\nDomain Classifier training finished. Best Val Acc: {best_dc_val_acc:.4f}\")\n",
        "if os.path.exists(dc_model_save_path):\n",
        "    domain_classifier_head.load_state_dict(torch.load(dc_model_save_path))\n",
        "    domain_classifier_head.eval()\n",
        "    print(\"Best Domain Classifier Head loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 6 - Robust Inference Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 20: Robust Inference - TTA Definitions & Expert Population\n",
        "\n",
        "print(\"\\n\\n=== Part 6: Robust Inference Pipeline Setup & Evaluation ===\")\n",
        "print(\"\\n--- Defining Test-Time Augmentations and Populating Task Experts ---\")\n",
        "\n",
        "# TTA Definitions\n",
        "tta_lite_transforms = [\n",
        "    val_test_transform_weak,\n",
        "    transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=1.0),\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "]\n",
        "\n",
        "tta_full_transforms_manual = [\n",
        "    val_test_transform_weak,\n",
        "    transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
        "    transforms.Compose([transforms.CenterCrop(int(IMAGE_SIZE * 0.875)), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
        "    transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
        "    transforms.Compose([transforms.RandomRotation(degrees=(-15, 15)), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
        "]\n",
        "\n",
        "NUM_TTA_LITE_AUGMENTATIONS = len(tta_lite_transforms)\n",
        "NUM_TTA_FULL_AUGMENTATIONS = len(tta_full_transforms_manual)\n",
        "print(f\"TTA Lite augmentations: {NUM_TTA_LITE_AUGMENTATIONS}, Full TTA augmentations: {NUM_TTA_FULL_AUGMENTATIONS}\")\n",
        "\n",
        "# Populate all_task_experts dictionary\n",
        "all_task_experts = {}\n",
        "print(\"\\nAttempting to populate all_task_experts...\")\n",
        "\n",
        "# Add Source Expert\n",
        "if 'source_resnet_lora' in globals() and source_resnet_lora is not None and \\\n",
        "   'source_head' in globals() and source_head is not None:\n",
        "    try:\n",
        "        all_task_experts[SOURCE_DOMAIN_NAME] = {\n",
        "            'resnet': copy.deepcopy(source_resnet_lora).to(DEVICE).eval(),\n",
        "            'head': copy.deepcopy(source_head).to(DEVICE).eval()\n",
        "        }\n",
        "        print(f\"  Successfully added Source Expert: '{SOURCE_DOMAIN_NAME}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error adding Source Expert '{SOURCE_DOMAIN_NAME}': {e}\")\n",
        "else:\n",
        "    print(f\"  Warning: Source expert for '{SOURCE_DOMAIN_NAME}' not fully available. Skipping.\")\n",
        "\n",
        "# Add Adapted Target Experts\n",
        "if 'adapted_experts' in globals() and isinstance(adapted_experts, dict) and adapted_experts:\n",
        "    print(f\"  Found adapted_experts with keys: {list(adapted_experts.keys())}\")\n",
        "    for domain_name_adapted, expert_models_adapted in adapted_experts.items():\n",
        "        if isinstance(expert_models_adapted, dict) and \\\n",
        "           'resnet' in expert_models_adapted and expert_models_adapted['resnet'] is not None and \\\n",
        "           'head' in expert_models_adapted and expert_models_adapted['head'] is not None:\n",
        "            try:\n",
        "                all_task_experts[domain_name_adapted] = {\n",
        "                    'resnet': copy.deepcopy(expert_models_adapted['resnet']).to(DEVICE).eval(),\n",
        "                    'head': copy.deepcopy(expert_models_adapted['head']).to(DEVICE).eval()\n",
        "                }\n",
        "                print(f\"    Successfully added Adapted Expert: '{domain_name_adapted}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error adding Adapted Expert '{domain_name_adapted}': {e}\")\n",
        "        else:\n",
        "            print(f\"    Warning: Incomplete models for adapted expert '{domain_name_adapted}'. Skipping.\")\n",
        "else:\n",
        "    print(\"  Warning: `adapted_experts` dictionary not found or empty.\")\n",
        "\n",
        "if not all_task_experts:\n",
        "    print(\"\\nCRITICAL WARNING: all_task_experts is EMPTY. Inference will fail.\")\n",
        "else:\n",
        "    print(f\"\\nSuccessfully populated all_task_experts with {len(all_task_experts)} expert(s): {list(all_task_experts.keys())}\")\n",
        "\n",
        "# Ensure base_resnet_frozen_global and domain_classifier_head are ready\n",
        "if 'base_resnet_frozen_global' not in globals() or base_resnet_frozen_global is None:\n",
        "    print(\"CRITICAL WARNING: base_resnet_frozen_global not found for inference pipeline.\")\n",
        "else:\n",
        "    base_resnet_frozen_global.to(DEVICE).eval()\n",
        "\n",
        "if 'domain_classifier_head' not in globals() or domain_classifier_head is None:\n",
        "    print(\"CRITICAL WARNING: domain_classifier_head not found for inference pipeline.\")\n",
        "else:\n",
        "    domain_classifier_head.to(DEVICE).eval()\n",
        "\n",
        "print(\"Expert population and TTA definitions complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 21: Robust Inference Pipeline Function Definition\n",
        "\n",
        "def robust_inference_pipeline(image_pil, base_resnet, dc_head, task_experts_dict,\n",
        "                              domain_map_idx_to_name_dc,\n",
        "                              num_total_classes=NUM_CLASSES,\n",
        "                              tta_lite_transforms_list=tta_lite_transforms,\n",
        "                              tta_full_transforms_list=tta_full_transforms_manual,\n",
        "                              domain_confidence_thresh=INFER_DOMAIN_CONF_THRESH,\n",
        "                              expert_confidence_thresh=INFER_EXPERT_CONF_THRESH,\n",
        "                              stage2_expert_confidence_thresh=INFER_STAGE2_EXPERT_CONF_THRESH,\n",
        "                              k_experts_for_avg=INFER_K_EXPERTS_FOR_AVG,\n",
        "                              device=DEVICE):\n",
        "    if base_resnet is not None:\n",
        "        base_resnet.to(device).eval()\n",
        "    if dc_head is not None:\n",
        "        dc_head.to(device).eval()\n",
        "\n",
        "    if not base_resnet or not dc_head:\n",
        "        print(\"Error: Base ResNet or Domain Classifier Head not provided.\")\n",
        "        return None, -1.0\n",
        "    if not task_experts_dict:\n",
        "        print(\"Error: task_experts_dict is empty.\")\n",
        "        return None, -1.0\n",
        "\n",
        "    initial_transformed_image = val_test_transform_weak(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "        base_features = get_resnet_features(base_resnet, initial_transformed_image)\n",
        "        domain_logits = dc_head(base_features)\n",
        "        domain_probs = F.softmax(domain_logits, dim=-1).squeeze(0)\n",
        "\n",
        "    top_domain_prob, predicted_domain_idx_tensor = torch.max(domain_probs, dim=-1)\n",
        "    predicted_domain_idx = predicted_domain_idx_tensor.item()\n",
        "    predicted_domain_name = domain_map_idx_to_name_dc.get(predicted_domain_idx, f\"UnknownDomain{predicted_domain_idx}\")\n",
        "\n",
        "    selected_domain_names_for_next_stage = []\n",
        "    expert_weights_for_next_stage = torch.tensor([], device=device)\n",
        "\n",
        "    if top_domain_prob >= domain_confidence_thresh and predicted_domain_name in task_experts_dict:\n",
        "        expert_resnet = task_experts_dict[predicted_domain_name]['resnet']\n",
        "        expert_head = task_experts_dict[predicted_domain_name]['head']\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            expert_features = get_resnet_features(expert_resnet, initial_transformed_image)\n",
        "            task_logits = expert_head(expert_features)\n",
        "            task_probs = F.softmax(task_logits, dim=-1).squeeze(0)\n",
        "        top_task_prob, final_label_idx_tensor = torch.max(task_probs, dim=-1)\n",
        "        if top_task_prob >= expert_confidence_thresh:\n",
        "            return final_label_idx_tensor.item(), top_task_prob.item()\n",
        "        else:\n",
        "            selected_domain_names_for_next_stage = [predicted_domain_name]\n",
        "            expert_weights_for_next_stage = torch.tensor([1.0], device=device)\n",
        "    else:\n",
        "        num_available_experts = len([name for name in domain_map_idx_to_name_dc.values() if name in task_experts_dict])\n",
        "        actual_k = min(k_experts_for_avg, num_available_experts)\n",
        "        if actual_k == 0:\n",
        "            return None, -1.0\n",
        "\n",
        "        top_k_domain_probs, top_k_domain_indices = torch.topk(domain_probs, actual_k, dim=-1)\n",
        "        selected_domain_names_raw = [domain_map_idx_to_name_dc.get(idx.item(), f\"Err{idx.item()}\") for idx in top_k_domain_indices]\n",
        "\n",
        "        valid_indices = [i for i, name in enumerate(selected_domain_names_raw) if name in task_experts_dict]\n",
        "        if not valid_indices:\n",
        "            return None, -1.0\n",
        "\n",
        "        selected_domain_names_for_next_stage = [selected_domain_names_raw[i] for i in valid_indices]\n",
        "        expert_weights_for_next_stage = top_k_domain_probs[valid_indices]\n",
        "        if expert_weights_for_next_stage.sum() > 1e-6:\n",
        "            expert_weights_for_next_stage = expert_weights_for_next_stage / expert_weights_for_next_stage.sum()\n",
        "        else:\n",
        "            expert_weights_for_next_stage = torch.ones(len(selected_domain_names_for_next_stage), device=device) / max(1, len(selected_domain_names_for_next_stage))\n",
        "\n",
        "    # Stage 2: TTA-Lite\n",
        "    aggregated_task_probs_stage2 = torch.zeros(num_total_classes, device=device)\n",
        "    num_tta_lite = len(tta_lite_transforms_list)\n",
        "    for tta_transform in tta_lite_transforms_list:\n",
        "        aug_image_tensor = tta_transform(image_pil).unsqueeze(0).to(device)\n",
        "        current_aug_weighted_probs = torch.zeros(num_total_classes, device=device)\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            for i, domain_name in enumerate(selected_domain_names_for_next_stage):\n",
        "                weight = expert_weights_for_next_stage[i]\n",
        "                expert_resnet = task_experts_dict[domain_name]['resnet']\n",
        "                expert_head = task_experts_dict[domain_name]['head']\n",
        "                exp_feat = get_resnet_features(expert_resnet, aug_image_tensor)\n",
        "                task_logits_expert_aug = expert_head(exp_feat)\n",
        "                current_aug_weighted_probs += weight * F.softmax(task_logits_expert_aug.squeeze(0), dim=-1)\n",
        "        aggregated_task_probs_stage2 += current_aug_weighted_probs\n",
        "\n",
        "    final_averaged_probs_stage2 = aggregated_task_probs_stage2 / max(1, num_tta_lite)\n",
        "    top_task_prob_stage2, final_label_idx_stage2_tensor = torch.max(final_averaged_probs_stage2, dim=-1)\n",
        "\n",
        "    if top_task_prob_stage2 >= stage2_expert_confidence_thresh:\n",
        "        return final_label_idx_stage2_tensor.item(), top_task_prob_stage2.item()\n",
        "\n",
        "    # Stage 3: TTA-Full\n",
        "    aggregated_task_probs_stage3 = torch.zeros(num_total_classes, device=device)\n",
        "    num_tta_full = len(tta_full_transforms_list)\n",
        "    for tta_transform_full in tta_full_transforms_list:\n",
        "        aug_image_tensor_full = tta_transform_full(image_pil).unsqueeze(0).to(device)\n",
        "        current_aug_weighted_probs_full = torch.zeros(num_total_classes, device=device)\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            for i, domain_name in enumerate(selected_domain_names_for_next_stage):\n",
        "                weight = expert_weights_for_next_stage[i]\n",
        "                expert_resnet = task_experts_dict[domain_name]['resnet']\n",
        "                expert_head = task_experts_dict[domain_name]['head']\n",
        "                exp_feat_full = get_resnet_features(expert_resnet, aug_image_tensor_full)\n",
        "                task_logits_expert_full_aug = expert_head(exp_feat_full)\n",
        "                current_aug_weighted_probs_full += weight * F.softmax(task_logits_expert_full_aug.squeeze(0), dim=-1)\n",
        "        aggregated_task_probs_stage3 += current_aug_weighted_probs_full\n",
        "\n",
        "    ultimate_final_probs = aggregated_task_probs_stage3 / max(1, num_tta_full)\n",
        "    top_task_prob_stage3, ultimate_final_label_idx_tensor = torch.max(ultimate_final_probs, dim=-1)\n",
        "\n",
        "    return ultimate_final_label_idx_tensor.item(), top_task_prob_stage3.item()\n",
        "\n",
        "\n",
        "print(\"Robust inference pipeline function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 22: Evaluate Robust Inference Pipeline on Combined Validation Set\n",
        "\n",
        "print(\"\\n--- Evaluating Robust Inference Pipeline on Combined Validation Set ---\")\n",
        "\n",
        "prereq_missing = False\n",
        "if 'domain_classifier_head' not in globals() or domain_classifier_head is None:\n",
        "    print(\"ERROR: Domain Classifier Head not trained/loaded.\")\n",
        "    prereq_missing = True\n",
        "if 'all_task_experts' not in globals() or not all_task_experts:\n",
        "    print(\"ERROR: No task experts available.\")\n",
        "    prereq_missing = True\n",
        "if 'base_resnet_frozen_global' not in globals() or base_resnet_frozen_global is None:\n",
        "    print(\"ERROR: Base ResNet backbone not loaded.\")\n",
        "    prereq_missing = True\n",
        "if 'GLOBAL_CLASS_TO_IDX' not in globals() or GLOBAL_CLASS_TO_IDX is None:\n",
        "    print(\"ERROR: GLOBAL_CLASS_TO_IDX not defined.\")\n",
        "    prereq_missing = True\n",
        "if 'domain_idx_to_name_dc' not in globals() or domain_idx_to_name_dc is None:\n",
        "    print(\"ERROR: domain_idx_to_name_dc mapping not defined.\")\n",
        "    prereq_missing = True\n",
        "\n",
        "if prereq_missing:\n",
        "    print(\"Skipping robust inference pipeline evaluation due to missing prerequisites.\")\n",
        "else:\n",
        "    class CombinedValDatasetForRobustEval(Dataset):\n",
        "        def __init__(self, root_dir, all_domain_names_list, class_to_idx_map_overall, split_type='val'):\n",
        "            self.images_pil = []\n",
        "            self.true_class_labels = []\n",
        "            self.original_domain_names = []\n",
        "\n",
        "            for domain_name_iter in all_domain_names_list:\n",
        "                try:\n",
        "                    domain_val_dataset_temp = OfficeHomeDomainDataset(\n",
        "                        root_dir=root_dir, domain_name=domain_name_iter,\n",
        "                        transform=None,\n",
        "                        split_type=split_type,\n",
        "                        class_to_idx_mapping=class_to_idx_map_overall,\n",
        "                        load_pil=True\n",
        "                    )\n",
        "\n",
        "                    for i in range(len(domain_val_dataset_temp)):\n",
        "                        pil_img, class_lbl = domain_val_dataset_temp[i]\n",
        "                        self.images_pil.append(pil_img)\n",
        "                        self.true_class_labels.append(class_lbl)\n",
        "                        self.original_domain_names.append(domain_name_iter)\n",
        "\n",
        "                    print(f\"  Added {len(domain_val_dataset_temp)} validation images from '{domain_name_iter}'.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Warning: Error loading val data for '{domain_name_iter}': {e}. Skipping.\")\n",
        "\n",
        "            if not self.images_pil:\n",
        "                raise RuntimeError(\"No validation images found.\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.images_pil)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return (self.images_pil[idx],\n",
        "                    torch.tensor(self.true_class_labels[idx]).long(),\n",
        "                    self.original_domain_names[idx])\n",
        "\n",
        "    try:\n",
        "        combined_val_dataset_robust = CombinedValDatasetForRobustEval(\n",
        "            root_dir=DATA_DIR,\n",
        "            all_domain_names_list=ALL_TRAINABLE_DOMAIN_NAMES,\n",
        "            class_to_idx_map_overall=GLOBAL_CLASS_TO_IDX,\n",
        "            split_type='val'\n",
        "        )\n",
        "        print(f\"Total combined validation images for robust eval: {len(combined_val_dataset_robust)}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error creating combined validation dataset: {e}. Aborting.\")\n",
        "        combined_val_dataset_robust = None\n",
        "\n",
        "    if combined_val_dataset_robust and len(combined_val_dataset_robust) > 0:\n",
        "        all_predictions_robust = []\n",
        "        all_true_labels_robust = []\n",
        "        all_original_domains_robust = []\n",
        "\n",
        "        for i in tqdm(range(len(combined_val_dataset_robust)), desc=\"Robust Pipeline Eval\"):\n",
        "            pil_image, true_class_label, original_domain = combined_val_dataset_robust[i]\n",
        "\n",
        "            predicted_label_idx, confidence = robust_inference_pipeline(\n",
        "                image_pil=pil_image,\n",
        "                base_resnet=base_resnet_frozen_global,\n",
        "                dc_head=domain_classifier_head,\n",
        "                task_experts_dict=all_task_experts,\n",
        "                domain_map_idx_to_name_dc=domain_idx_to_name_dc,\n",
        "                device=DEVICE\n",
        "            )\n",
        "            all_predictions_robust.append(predicted_label_idx if predicted_label_idx is not None else -1)\n",
        "            all_true_labels_robust.append(true_class_label.item())\n",
        "            all_original_domains_robust.append(original_domain)\n",
        "\n",
        "        print(\"\\n--- Robust Inference Pipeline Evaluation Results ---\")\n",
        "        overall_correct_robust = 0\n",
        "        overall_total_robust = 0\n",
        "        domain_wise_stats_robust = {name: {'correct': 0, 'total': 0} for name in ALL_TRAINABLE_DOMAIN_NAMES}\n",
        "\n",
        "        for i in range(len(all_predictions_robust)):\n",
        "            pred_idx, true_idx, domain_name = all_predictions_robust[i], all_true_labels_robust[i], all_original_domains_robust[i]\n",
        "            if pred_idx != -1:\n",
        "                overall_total_robust += 1\n",
        "                domain_wise_stats_robust[domain_name]['total'] += 1\n",
        "                if pred_idx == true_idx:\n",
        "                    overall_correct_robust += 1\n",
        "                    domain_wise_stats_robust[domain_name]['correct'] += 1\n",
        "\n",
        "        overall_accuracy_robust = (overall_correct_robust / overall_total_robust * 100) if overall_total_robust > 0 else 0.0\n",
        "        print(f\"Overall Accuracy (Robust Pipeline): {overall_accuracy_robust:.2f}% ({overall_correct_robust}/{overall_total_robust})\")\n",
        "        print(\"\\nDomain-wise Accuracies (Robust Pipeline):\")\n",
        "        for domain_name, stats in domain_wise_stats_robust.items():\n",
        "            acc = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0.0\n",
        "            print(f\"  Domain '{domain_name}': {acc:.2f}% ({stats['correct']}/{stats['total']})\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
