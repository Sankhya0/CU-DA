{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:34.452936Z",
     "iopub.status.busy": "2025-05-16T10:06:34.452548Z",
     "iopub.status.idle": "2025-05-16T10:06:45.117339Z",
     "shell.execute_reply": "2025-05-16T10:06:45.116589Z",
     "shell.execute_reply.started": "2025-05-16T10:06:34.452920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import os\n",
    "import numpy as np\n",
    "import math # For sinusoidal embedding, sqrt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "from functools import partial # For default args in transforms\n",
    "from einops import rearrange, repeat # If using einops\n",
    "\n",
    "# For AMP\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:45.119044Z",
     "iopub.status.busy": "2025-05-16T10:06:45.118735Z",
     "iopub.status.idle": "2025-05-16T10:06:45.203535Z",
     "shell.execute_reply": "2025-05-16T10:06:45.202707Z",
     "shell.execute_reply.started": "2025-05-16T10:06:45.119027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Global Hyperparameters & Configuration\n",
    "\n",
    "# --- System ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"/kaggle/input/officehome/OfficeHome\" \n",
    "MODEL_SAVE_DIR_BASE = \"/kaggle/working/models/\" \n",
    "os.makedirs(MODEL_SAVE_DIR_BASE, exist_ok=True)\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# --- Dataset & DataLoader ---\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32 \n",
    "NUM_WORKERS = 2\n",
    "NUM_CLASSES = 65 \n",
    "\n",
    "# --- ViT Backbone ---\n",
    "VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "VIT_EMBED_DIM = 768 \n",
    "\n",
    "# --- LoRA ---\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32.0\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# --- DAD Module ---\n",
    "DAD_K_STEPS = 200       \n",
    "DAD_P_THETA_TIMESTEP_EMBED_DIM = 128\n",
    "DAD_P_THETA_HIDDEN_DIM_MULT = 4 \n",
    "DAD_P_THETA_MLP_HIDDEN_DIM = 1024\n",
    "DAD_BETA_START = 1e-4\n",
    "DAD_BETA_END = 2e-2\n",
    "\n",
    "# --- Training (General) ---\n",
    "SOURCE_DOMAIN_NAME = 'Art'\n",
    "TARGET_DOMAIN_NAMES_ORDERED = ['Clipart', 'Product', 'RealWorld']\n",
    "ALL_TRAINABLE_DOMAIN_NAMES = [SOURCE_DOMAIN_NAME] + TARGET_DOMAIN_NAMES_ORDERED\n",
    "\n",
    "# --- Source Domain Training (Art) ---\n",
    "ART_EPOCHS = 10\n",
    "ART_LR_LORA_HEAD_LN = 5e-4 \n",
    "\n",
    "# --- Continual Adaptation (Per Target Domain) ---\n",
    "ADAPT_MLS_R_ITER = 10   \n",
    "ADAPT_LR_LORA_HEAD_LN = 1e-4\n",
    "ADAPT_LR_P_THETA = 1e-4 \n",
    "ADAPT_LTR_EPOCHS = 5 \n",
    "EARLY_STOPPING_PATIENCE_ADAPT = 3 # << NEW: Patience for early stopping\n",
    "\n",
    "# --- EMA Teacher & Pseudo-Labeling ---\n",
    "EMA_DECAY = 0.999\n",
    "PSEUDO_LABEL_THRESHOLD_START = 0.7 \n",
    "PSEUDO_LABEL_THRESHOLD_END = 0.9   \n",
    "\n",
    "# --- FixMatch ---\n",
    "FIXMATCH_CONF_THRESHOLD = 0.95\n",
    "FIXMATCH_LAMBDA = 1.0 \n",
    "\n",
    "# --- SHOT ---\n",
    "SHOT_LAMBDA_COND_ENT = 0.05\n",
    "SHOT_LAMBDA_ENT_MAX = 0.05\n",
    "\n",
    "# --- Experience Replay (Optional) ---\n",
    "REPLAY_BUFFER_SIZE = 2000\n",
    "REPLAY_BATCH_SIZE_RATIO = 0.25 \n",
    "REPLAY_LAMBDA = 0.1 \n",
    "\n",
    "# --- Domain Classifier (for Robust Inference) ---\n",
    "DC_HEAD_LR = 1e-3\n",
    "DC_HEAD_EPOCHS = 10\n",
    "\n",
    "# --- Robust Inference Pipeline ---\n",
    "INFER_DOMAIN_CONF_THRESH = 0.7\n",
    "INFER_EXPERT_CONF_THRESH = 0.6\n",
    "INFER_STAGE2_EXPERT_CONF_THRESH = 0.5\n",
    "INFER_K_EXPERTS_FOR_AVG = 3\n",
    "\n",
    "# --- Autocast Context for AMP ---\n",
    "# Corrected autocast_ctx definition\n",
    "if DEVICE.type == 'cuda':\n",
    "    autocast_ctx = partial(torch.amp.autocast, device_type='cuda', dtype=torch.float16, enabled=True)\n",
    "else:\n",
    "    # For CPU, autocast might not be beneficial or might need bfloat16 if supported\n",
    "    # For simplicity, creating a no-op context manager for CPU\n",
    "    import contextlib\n",
    "    autocast_ctx = contextlib.nullcontext\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available(): print(f\"CUDA available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"All configurations set. Base model save dir: {MODEL_SAVE_DIR_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:45.207356Z",
     "iopub.status.busy": "2025-05-16T10:06:45.207134Z",
     "iopub.status.idle": "2025-05-16T10:06:45.213791Z",
     "shell.execute_reply": "2025-05-16T10:06:45.213109Z",
     "shell.execute_reply.started": "2025-05-16T10:06:45.207340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Transforms Definition\n",
    "\n",
    "# --- Base Transforms (from original notebook) ---\n",
    "# For training student models (strong augmentation for FixMatch)\n",
    "train_transform_strong = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Moderate\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    # For FixMatch Strong Aug: RandAugment\n",
    "    # transforms.RandAugment(num_ops=2, magnitude=10), # N=2, M=10 from blueprint\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0), # Or Cutout\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# For validation, LTR target features, EMA teacher input (weak augmentation for FixMatch)\n",
    "val_test_transform_weak = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # Standard weak aug\n",
    "    # transforms.RandomCrop(IMAGE_SIZE, padding=int(IMAGE_SIZE*0.125), padding_mode='reflect'), # Optional crop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transform for loading PIL images without ToTensor or Normalize (for CombinedValDataset)\n",
    "pil_load_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)) # Just resize, keep as PIL\n",
    "])\n",
    "\n",
    "print(\"Transforms defined: train_transform_strong, val_test_transform_weak, pil_load_transform.\")\n",
    "# Note: RandAugment and RandomErasing/Cutout for strong FixMatch augs need to be added if torchvision version supports them easily,\n",
    "# or implemented manually/imported from timm.data.auto_augment.\n",
    "# For now, using the original notebook's train_transform as a proxy for strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:45.214745Z",
     "iopub.status.busy": "2025-05-16T10:06:45.214575Z",
     "iopub.status.idle": "2025-05-16T10:06:45.238060Z",
     "shell.execute_reply": "2025-05-16T10:06:45.237317Z",
     "shell.execute_reply.started": "2025-05-16T10:06:45.214731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5 (or a new cell for FixMatch Dataset)\n",
    "class FixMatchOfficeHomeDataset(Dataset):\n",
    "    def __init__(self, root_dir, domain_name, transform_weak, transform_strong,\n",
    "                 split_ratios=(0.8, 0.1, 0.1), split_type='train',\n",
    "                 random_seed=RANDOM_SEED, class_to_idx_mapping=None):\n",
    "        # Use OfficeHomeDomainDataset to get image paths and labels internally\n",
    "        self.base_dataset = OfficeHomeDomainDataset(\n",
    "            root_dir, domain_name, transform=None, # No transform initially\n",
    "            split_ratios=split_ratios, split_type=split_type,\n",
    "            random_seed=random_seed, class_to_idx_mapping=class_to_idx_mapping,\n",
    "            load_pil=True # Important: get PIL images\n",
    "        )\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pil_image, label = self.base_dataset[idx] # Gets PIL image and label\n",
    "\n",
    "        img_weak = self.transform_weak(pil_image)\n",
    "        img_strong = self.transform_strong(pil_image)\n",
    "        \n",
    "        # Label is not strictly needed for unlabeled target, but dataset provides it\n",
    "        return img_weak, img_strong #, label (optional)\n",
    "\n",
    "class OfficeHomeDomainDataset(Dataset):\n",
    "    def __init__(self, root_dir, domain_name, transform=None, # General transform\n",
    "                 split_ratios=(0.8, 0.1, 0.1), split_type='train', \n",
    "                 random_seed=RANDOM_SEED, class_to_idx_mapping=None, load_pil=False):\n",
    "        self.domain_path = os.path.join(root_dir, domain_name)\n",
    "        self.transform = transform\n",
    "        self.load_pil = load_pil # If true, returns PIL image and label\n",
    "        \n",
    "        self.images_paths = [] # Store paths\n",
    "        self.labels = []\n",
    "        \n",
    "        if class_to_idx_mapping is None:\n",
    "            self.class_to_idx = {}\n",
    "            self.idx_to_class = {}\n",
    "            current_idx = 0\n",
    "            for class_name_iter in sorted(os.listdir(self.domain_path)):\n",
    "                if os.path.isdir(os.path.join(self.domain_path, class_name_iter)):\n",
    "                    if class_name_iter not in self.class_to_idx:\n",
    "                        self.class_to_idx[class_name_iter] = current_idx\n",
    "                        self.idx_to_class[current_idx] = class_name_iter\n",
    "                        current_idx += 1\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx_mapping\n",
    "            self.idx_to_class = {v: k for k, v in class_to_idx_mapping.items()}\n",
    "\n",
    "        for class_name in sorted(os.listdir(self.domain_path)):\n",
    "            class_label_idx = self.class_to_idx.get(class_name)\n",
    "            if class_label_idx is None: continue # Skip if class not in provided mapping\n",
    "\n",
    "            class_path = os.path.join(self.domain_path, class_name)\n",
    "            if not os.path.isdir(class_path): continue\n",
    "            \n",
    "            domain_class_images_paths = []\n",
    "            for img_name in sorted(os.listdir(class_path)):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    domain_class_images_paths.append(os.path.join(class_path, img_name))\n",
    "            \n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(domain_class_images_paths)\n",
    "            \n",
    "            n_total = len(domain_class_images_paths)\n",
    "            n_train = int(n_total * split_ratios[0])\n",
    "            n_val = int(n_total * split_ratios[1])\n",
    "\n",
    "            if split_type == 'train': selected_paths = domain_class_images_paths[:n_train]\n",
    "            elif split_type == 'val': selected_paths = domain_class_images_paths[n_train : n_train + n_val]\n",
    "            elif split_type == 'test': selected_paths = domain_class_images_paths[n_train + n_val:]\n",
    "            elif split_type == 'all': selected_paths = domain_class_images_paths # For unlabeled target data\n",
    "            else: raise ValueError(\"split_type must be 'train', 'val', 'test', or 'all'\")\n",
    "\n",
    "            self.images_paths.extend(selected_paths)\n",
    "            self.labels.extend([class_label_idx] * len(selected_paths))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images_paths[idx]\n",
    "        image_pil = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.load_pil:\n",
    "            return image_pil, label # Return PIL image and label\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else: # If no transform, attempt to convert to tensor (basic)\n",
    "            image_tensor = transforms.ToTensor()(image_pil) \n",
    "        return image_tensor, label\n",
    "\n",
    "print(\"OfficeHomeDomainDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:45.238972Z",
     "iopub.status.busy": "2025-05-16T10:06:45.238752Z",
     "iopub.status.idle": "2025-05-16T10:06:46.097427Z",
     "shell.execute_reply": "2025-05-16T10:06:46.096684Z",
     "shell.execute_reply.started": "2025-05-16T10:06:45.238957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Global Class Mapping & Initial Data Check\n",
    "\n",
    "# Create a global class_to_idx mapping from one domain (e.g., Art)\n",
    "# This ensures all domains use the same integer labels for the same class names.\n",
    "print(\"Attempting to create GLOBAL_CLASS_TO_IDX from Art domain...\")\n",
    "try:\n",
    "    temp_art_dataset_for_map = OfficeHomeDomainDataset(DATA_DIR, 'Art', split_type='all')\n",
    "    GLOBAL_CLASS_TO_IDX = temp_art_dataset_for_map.class_to_idx\n",
    "    GLOBAL_IDX_TO_CLASS = temp_art_dataset_for_map.idx_to_class\n",
    "    assert len(GLOBAL_CLASS_TO_IDX) == NUM_CLASSES, \\\n",
    "        f\"Mismatch: Expected {NUM_CLASSES} classes, found {len(GLOBAL_CLASS_TO_IDX)} in Art domain.\"\n",
    "    print(f\"GLOBAL_CLASS_TO_IDX created successfully with {len(GLOBAL_CLASS_TO_IDX)} classes.\")\n",
    "    del temp_art_dataset_for_map\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not create GLOBAL_CLASS_TO_IDX from Art domain: {e}\")\n",
    "    print(\"Please ensure DATA_DIR is correct and Art domain exists. Cannot proceed without class mapping.\")\n",
    "    GLOBAL_CLASS_TO_IDX = None # Critical failure\n",
    "\n",
    "# Test loading Art dataset (example)\n",
    "if GLOBAL_CLASS_TO_IDX:\n",
    "    try:\n",
    "        art_train_dataset_check = OfficeHomeDomainDataset(DATA_DIR, SOURCE_DOMAIN_NAME, \n",
    "                                                          transform=train_transform_strong, \n",
    "                                                          split_type='train',\n",
    "                                                          class_to_idx_mapping=GLOBAL_CLASS_TO_IDX)\n",
    "        art_val_dataset_check = OfficeHomeDomainDataset(DATA_DIR, SOURCE_DOMAIN_NAME, \n",
    "                                                        transform=val_test_transform_weak, \n",
    "                                                        split_type='val',\n",
    "                                                        class_to_idx_mapping=GLOBAL_CLASS_TO_IDX)\n",
    "        if len(art_train_dataset_check) > 0 and len(art_val_dataset_check) > 0:\n",
    "            print(f\"Successfully loaded check datasets for '{SOURCE_DOMAIN_NAME}': \"\n",
    "                  f\"Train size {len(art_train_dataset_check)}, Val size {len(art_val_dataset_check)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Check datasets for '{SOURCE_DOMAIN_NAME}' are empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading check datasets for '{SOURCE_DOMAIN_NAME}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.098627Z",
     "iopub.status.busy": "2025-05-16T10:06:46.098168Z",
     "iopub.status.idle": "2025-05-16T10:06:46.103693Z",
     "shell.execute_reply": "2025-05-16T10:06:46.102992Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.098600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: ViT Backbone Loading Function\n",
    "def load_frozen_vit_backbone(model_name=VIT_MODEL_NAME, device=DEVICE):\n",
    "    vit_backbone = timm.create_model(model_name, pretrained=True, num_classes=0) # num_classes=0 removes head\n",
    "    for param in vit_backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    vit_backbone = vit_backbone.to(device)\n",
    "    vit_backbone.eval()\n",
    "    print(f\"Loaded and froze ViT backbone: {model_name}\")\n",
    "    return vit_backbone\n",
    "\n",
    "# Load it once globally\n",
    "# base_vit_frozen_global = load_frozen_vit_backbone() # Will be used for DC head and as base for LoRA experts\n",
    "print(\"ViT backbone loading function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.104744Z",
     "iopub.status.busy": "2025-05-16T10:06:46.104452Z",
     "iopub.status.idle": "2025-05-16T10:06:46.197310Z",
     "shell.execute_reply": "2025-05-16T10:06:46.196623Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.104724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: LoRALinear and LoRA Injection Function (Updated for alpha, dropout)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, alpha, lora_dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout_p)\n",
    "\n",
    "        # Original weight and bias (frozen)\n",
    "        self.weight = nn.Parameter(linear_layer.weight.detach().clone(), requires_grad=False)\n",
    "        if linear_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(linear_layer.bias.detach().clone(), requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # LoRA matrices A and B\n",
    "        self.lora_A = nn.Parameter(torch.zeros(self.rank, self.in_features)) \n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.out_features, self.rank))\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5)) \n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.scaling = self.alpha / self.rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_original = F.linear(x, self.weight, self.bias)\n",
    "        lora_effect = self.lora_B @ self.lora_dropout(self.lora_A @ x.transpose(-2, -1)) # Adapting for (B, N, D) input\n",
    "        lora_effect = lora_effect.transpose(-2, -1)\n",
    "        return out_original + lora_effect * self.scaling\n",
    "        # Original notebook had: lora_adaptation = (x @ self.lora_A.t()) @ self.lora_B.t()\n",
    "        # This assumes x is (B, D_in). For ViT attention, x is (B, Num_Patches, D_in)\n",
    "        # If linear_layer is applied to last dim: (B, N, D_in) @ A.T (D_in, R) -> (B, N, R)\n",
    "        # Then (B, N, R) @ B.T (R, D_out) -> (B, N, D_out)\n",
    "        # So, (x @ self.lora_A.t()) @ self.lora_B.t() should be correct if x is (B, ..., D_in)\n",
    "        # Let's stick to the original notebook's LoRA forward for now, assuming it handles ViT's shapes.\n",
    "        # Re-checking ViT structure: qkv is Linear(dim, dim * 3). Input to qkv is (B, N, D).\n",
    "        # So x is (B, N, D_in). Output should be (B, N, D_out).\n",
    "        # x @ self.lora_A.t() -> (B, N, D_in) @ (D_in, R) -> (B, N, R)\n",
    "        # (B, N, R) @ self.lora_B.t() -> (B, N, R) @ (R, D_out) -> (B, N, D_out). This is correct.\n",
    "        # The dropout should be applied to the output of lora_A multiplication.\n",
    "        # lora_A_x = x @ self.lora_A.t()\n",
    "        # lora_A_x_dropped = self.lora_dropout(lora_A_x)\n",
    "        # lora_adaptation = lora_A_x_dropped @ self.lora_B.t()\n",
    "        # return out_original + lora_adaptation * self.scaling\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, rank={self.rank}, alpha={self.alpha}'\n",
    "\n",
    "def inject_lora_to_vit_attention(vit_model, rank, alpha, lora_dropout_p=0.0):\n",
    "    # Freezes base model params implicitly by not making them requires_grad=True later\n",
    "    # The base model should already be frozen before calling this.\n",
    "    injected_count = 0\n",
    "    for block_idx, block in enumerate(vit_model.blocks):\n",
    "        # Inject into QKV\n",
    "        original_qkv = block.attn.qkv\n",
    "        if isinstance(original_qkv, nn.Linear):\n",
    "            block.attn.qkv = LoRALinear(original_qkv, rank, alpha, lora_dropout_p)\n",
    "            injected_count += 1\n",
    "        \n",
    "        # Optionally, inject into attention projection output\n",
    "        # original_proj = block.attn.proj\n",
    "        # if isinstance(original_proj, nn.Linear):\n",
    "        #    block.attn.proj = LoRALinear(original_proj, rank, alpha, lora_dropout_p)\n",
    "        #    injected_count += 1 # Count as one layer if qkv is one, or more if separate q,k,v\n",
    "    \n",
    "    if injected_count == 0:\n",
    "        print(\"WARNING: No QKV layers found or replaced with LoRA.\")\n",
    "    else:\n",
    "        print(f\"Injected LoRA (rank={rank}, alpha={alpha}) into {injected_count} attention layers (QKV combined).\")\n",
    "    return vit_model\n",
    "\n",
    "print(\"LoRALinear class and injection function defined.\")\n",
    "# Test LoRA forward logic correction\n",
    "x_test = torch.randn(2, 197, 768) # B, N, D_in\n",
    "linear_test = nn.Linear(768, 768*3)\n",
    "lora_linear_test = LoRALinear(linear_test, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT)\n",
    "out_test = lora_linear_test(x_test)\n",
    "print(f\"Test LoRA output shape: {out_test.shape}\") # Expected (2, 197, 768*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.198345Z",
     "iopub.status.busy": "2025-05-16T10:06:46.198056Z",
     "iopub.status.idle": "2025-05-16T10:06:46.204931Z",
     "shell.execute_reply": "2025-05-16T10:06:46.204171Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.198322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: DomainSpecificHead and LayerNormTrainable function\n",
    "\n",
    "class DomainSpecificHead(nn.Module):\n",
    "    def __init__(self, in_features=VIT_EMBED_DIM, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # The ViT's own norm layer (vit_model.norm) will be applied to features BEFORE this head.\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x_normed_cls_token): # Expects already normed CLS token\n",
    "        return self.fc(x_normed_cls_token)\n",
    "\n",
    "def set_layernorm_affine_trainable(model, trainable=True):\n",
    "    ln_param_count = 0\n",
    "    for name, mod in model.named_modules():\n",
    "        if isinstance(mod, nn.LayerNorm):\n",
    "            if hasattr(mod, 'weight') and mod.weight is not None:\n",
    "                mod.weight.requires_grad = trainable\n",
    "                if trainable: ln_param_count += mod.weight.numel()\n",
    "            if hasattr(mod, 'bias') and mod.bias is not None:\n",
    "                mod.bias.requires_grad = trainable\n",
    "                if trainable: ln_param_count += mod.bias.numel()\n",
    "    status = \"trainable\" if trainable else \"frozen\"\n",
    "    print(f\"Set LayerNorm affine parameters {status}. Total LN params affected: {ln_param_count:,}\")\n",
    "    return model\n",
    "    \n",
    "print(\"DomainSpecificHead class and set_layernorm_affine_trainable function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.208017Z",
     "iopub.status.busy": "2025-05-16T10:06:46.207695Z",
     "iopub.status.idle": "2025-05-16T10:06:46.600678Z",
     "shell.execute_reply": "2025-05-16T10:06:46.599852Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.208001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: DAD Module - Timestep Embedding & p_theta Denoiser MLP\n",
    "\n",
    "class SinusoidalTimestepEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_period=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period\n",
    "\n",
    "    def forward(self, timesteps): # timesteps: (B,) or scalar, dtype=long\n",
    "        if timesteps.ndim == 0: timesteps = timesteps.unsqueeze(0)\n",
    "        device = timesteps.device\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(self.max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32) / half_dim\n",
    "        ).to(device)\n",
    "        args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if self.dim % 2: # Odd dimension\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding # Shape: (B, dim)\n",
    "\n",
    "class DAD_P_Theta_MLP(nn.Module):\n",
    "    def __init__(self, feature_dim=VIT_EMBED_DIM, \n",
    "                 timestep_embed_dim=DAD_P_THETA_TIMESTEP_EMBED_DIM,\n",
    "                 ts_embed_hidden_mult=DAD_P_THETA_HIDDEN_DIM_MULT,\n",
    "                 mlp_hidden_dim=DAD_P_THETA_MLP_HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.timestep_embed_dim_actual = timestep_embed_dim\n",
    "        \n",
    "        self.timestep_encoder = nn.Sequential(\n",
    "            SinusoidalTimestepEmbedding(timestep_embed_dim),\n",
    "            nn.Linear(timestep_embed_dim, timestep_embed_dim * ts_embed_hidden_mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(timestep_embed_dim * ts_embed_hidden_mult, timestep_embed_dim * ts_embed_hidden_mult) # Projected dim\n",
    "        )\n",
    "        \n",
    "        combined_dim = feature_dim + (timestep_embed_dim * ts_embed_hidden_mult)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(combined_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, feature_dim) # Predicts epsilon (noise)\n",
    "        )\n",
    "\n",
    "    def forward(self, noisy_features, timesteps_long): # timesteps_long is (B,) or scalar, dtype=long\n",
    "        # Ensure timesteps_long is on the same device as noisy_features for SinusoidalTimestepEmbedding\n",
    "        timesteps_long = timesteps_long.to(noisy_features.device)\n",
    "        \n",
    "        ts_embedding = self.timestep_encoder(timesteps_long) # (B, ts_embed_dim_actual * ts_embed_hidden_mult)\n",
    "        \n",
    "        # Ensure ts_embedding is broadcastable if noisy_features has more batch dims (unlikely for CLS token)\n",
    "        if ts_embedding.ndim == 1 and noisy_features.ndim > 1 : # scalar timestep input, batched features\n",
    "             ts_embedding = ts_embedding.unsqueeze(0).expand(noisy_features.shape[0], -1)\n",
    "\n",
    "        combined_input = torch.cat([noisy_features, ts_embedding], dim=-1)\n",
    "        predicted_noise = self.mlp(combined_input)\n",
    "        return predicted_noise\n",
    "\n",
    "print(\"DAD p_theta MLP denoiser and Timestep Embedding defined.\")\n",
    "# Test p_theta\n",
    "p_theta_test = DAD_P_Theta_MLP().to(DEVICE)\n",
    "test_noisy_feat = torch.randn(BATCH_SIZE, VIT_EMBED_DIM).to(DEVICE)\n",
    "test_timesteps = torch.randint(0, DAD_K_STEPS, (BATCH_SIZE,), dtype=torch.long).to(DEVICE)\n",
    "pred_noise_test = p_theta_test(test_noisy_feat, test_timesteps)\n",
    "print(f\"p_theta test output shape: {pred_noise_test.shape}\") # Expected (BATCH_SIZE, VIT_EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.601867Z",
     "iopub.status.busy": "2025-05-16T10:06:46.601547Z",
     "iopub.status.idle": "2025-05-16T10:06:46.661698Z",
     "shell.execute_reply": "2025-05-16T10:06:46.660951Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.601849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: DAD Diffusion Utilities (q_sample, q_reconstruct, schedules)\n",
    "\n",
    "def make_dad_schedule(beta_start=DAD_BETA_START, beta_end=DAD_BETA_END, num_steps=DAD_K_STEPS, device=DEVICE):\n",
    "    betas = torch.linspace(beta_start, beta_end, num_steps, dtype=torch.float32, device=device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    return betas, alphas, alphas_cumprod\n",
    "\n",
    "# Make schedules globally accessible after defining DAD_K_STEPS\n",
    "# These will be on CPU by default if DEVICE is not yet cuda during this cell's run,\n",
    "# so ensure they are moved to device when used or defined after DEVICE is set.\n",
    "# Or, pass device to functions that use them.\n",
    "# For simplicity, define them here assuming DEVICE is set.\n",
    "if DEVICE.type == 'cuda' and not torch.cuda.is_available(): # Fallback if cuda specified but not avail\n",
    "    print(\"Warning: DEVICE is cuda but not available. DAD schedules on CPU.\")\n",
    "    _sched_device = torch.device('cpu')\n",
    "else:\n",
    "    _sched_device = DEVICE\n",
    "\n",
    "DAD_BETAS, DAD_ALPHAS, DAD_ALPHAS_CUMPROD = make_dad_schedule(device=_sched_device)\n",
    "\n",
    "def q_sample_dad(x_start, k_indices, noise=None): # k_indices are 0-indexed timesteps (B,)\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    # Ensure k_indices is long and on the same device as alphas_cumprod\n",
    "    k_indices = k_indices.long().to(DAD_ALPHAS_CUMPROD.device)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(DAD_ALPHAS_CUMPROD[k_indices])\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - DAD_ALPHAS_CUMPROD[k_indices])\n",
    "\n",
    "    # Reshape for broadcasting: (B,) -> (B, 1) if x_start is (B, D)\n",
    "    if x_start.ndim > 1 and sqrt_alphas_cumprod_t.ndim == 1:\n",
    "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, *([1]*(x_start.ndim-1)))\n",
    "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, *([1]*(x_start.ndim-1)))\n",
    "        \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def q_reconstruct_dad(x_k, epsilon_theta_hat, k_indices): # k_indices are 0-indexed timesteps (B,)\n",
    "    k_indices = k_indices.long().to(DAD_ALPHAS_CUMPROD.device)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(DAD_ALPHAS_CUMPROD[k_indices])\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - DAD_ALPHAS_CUMPROD[k_indices])\n",
    "\n",
    "    if x_k.ndim > 1 and sqrt_alphas_cumprod_t.ndim == 1:\n",
    "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, *([1]*(x_k.ndim-1)))\n",
    "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, *([1]*(x_k.ndim-1)))\n",
    "\n",
    "    x0_hat = (x_k - sqrt_one_minus_alphas_cumprod_t * epsilon_theta_hat) / sqrt_alphas_cumprod_t\n",
    "    return x0_hat\n",
    "\n",
    "print(\"DAD diffusion utilities (schedules, q_sample, q_reconstruct) defined.\")\n",
    "print(f\"DAD schedules (betas, alphas, alphas_cumprod) created on device: {DAD_BETAS.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.662661Z",
     "iopub.status.busy": "2025-05-16T10:06:46.662472Z",
     "iopub.status.idle": "2025-05-16T10:06:46.672543Z",
     "shell.execute_reply": "2025-05-16T10:06:46.671792Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.662644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: Utility Functions (EMA Update, Replay Buffer (optional))\n",
    "\n",
    "def update_ema_teacher_components(student_vit, student_head, teacher_vit, teacher_head, decay):\n",
    "    with torch.no_grad():\n",
    "        # Update ViT parameters (LoRA, LayerNorm affine)\n",
    "        for stud_param, teach_param in zip(student_vit.parameters(), teacher_vit.parameters()):\n",
    "            if stud_param.requires_grad: # Only update trainable student params into teacher\n",
    "                if teach_param.requires_grad:\n",
    "                    print(\"Warning: Teacher ViT parameter has requires_grad=True during EMA update.\")\n",
    "                teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "        \n",
    "        # Update Head parameters\n",
    "        for stud_param, teach_param in zip(student_head.parameters(), teacher_head.parameters()):\n",
    "            # Head params are usually all trainable in student, and all frozen in teacher\n",
    "            if teach_param.requires_grad:\n",
    "                print(\"Warning: Teacher Head parameter has requires_grad=True during EMA update.\")\n",
    "            teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "\n",
    "print(\"Utility functions (EMA update, Replay Buffer) defined.\")\n",
    "\n",
    "# Optional: Replay Buffer Class\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, buffer_size, device=DEVICE):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.buffer_images = []\n",
    "        self.buffer_labels = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, images_tensor, labels_tensor): # Expects tensors\n",
    "        batch_size = images_tensor.size(0)\n",
    "        for i in range(batch_size):\n",
    "            img = images_tensor[i].cpu() # Store on CPU to save GPU VRAM\n",
    "            lbl = labels_tensor[i].cpu()\n",
    "            if len(self.buffer_images) < self.buffer_size:\n",
    "                self.buffer_images.append(img)\n",
    "                self.buffer_labels.append(lbl)\n",
    "            else:\n",
    "                self.buffer_images[self.position] = img\n",
    "                self.buffer_labels[self.position] = lbl\n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer_images) < batch_size:\n",
    "            # Not enough samples, return what's available or None\n",
    "            if not self.buffer_images: return None, None\n",
    "            indices = np.random.choice(len(self.buffer_images), len(self.buffer_images), replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(len(self.buffer_images), batch_size, replace=False)\n",
    "        \n",
    "        sampled_images = torch.stack([self.buffer_images[i] for i in indices]).to(self.device)\n",
    "        sampled_labels = torch.stack([self.buffer_labels[i] for i in indices]).to(self.device)\n",
    "        return sampled_images, sampled_labels.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer_images)\n",
    "\n",
    "print(\"Utility functions (EMA update, Replay Buffer) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Source Domain (Art) Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.673457Z",
     "iopub.status.busy": "2025-05-16T10:06:46.673224Z",
     "iopub.status.idle": "2025-05-16T10:06:46.793560Z",
     "shell.execute_reply": "2025-05-16T10:06:46.793051Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.673444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 13: Source Domain - Dataset & DataLoader (Art)\n",
    "\n",
    "print(f\"\\n--- Preparing Source Domain: {SOURCE_DOMAIN_NAME} ---\")\n",
    "\n",
    "source_domain_model_save_dir = os.path.join(MODEL_SAVE_DIR_BASE, SOURCE_DOMAIN_NAME)\n",
    "os.makedirs(source_domain_model_save_dir, exist_ok=True)\n",
    "\n",
    "if GLOBAL_CLASS_TO_IDX is None:\n",
    "    raise RuntimeError(\"GLOBAL_CLASS_TO_IDX is not defined. Cannot proceed with source domain training.\")\n",
    "\n",
    "source_train_dataset = OfficeHomeDomainDataset(\n",
    "    DATA_DIR, SOURCE_DOMAIN_NAME,\n",
    "    transform=train_transform_strong, # Use strong aug for source training too\n",
    "    split_type='train',\n",
    "    class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    ")\n",
    "source_val_dataset = OfficeHomeDomainDataset(\n",
    "    DATA_DIR, SOURCE_DOMAIN_NAME,\n",
    "    transform=val_test_transform_weak,\n",
    "    split_type='val',\n",
    "    class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    ")\n",
    "\n",
    "source_train_loader = DataLoader(\n",
    "    source_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
    ")\n",
    "source_val_loader = DataLoader(\n",
    "    source_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Source domain '{SOURCE_DOMAIN_NAME}': Train size {len(source_train_dataset)}, Val size {len(source_val_dataset)}\")\n",
    "print(f\"Train loader batches: {len(source_train_loader)}, Val loader batches: {len(source_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:46.794669Z",
     "iopub.status.busy": "2025-05-16T10:06:46.794186Z",
     "iopub.status.idle": "2025-05-16T10:06:49.704695Z",
     "shell.execute_reply": "2025-05-16T10:06:49.703941Z",
     "shell.execute_reply.started": "2025-05-16T10:06:46.794652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: Source Domain - Model Instantiation & Optimizer\n",
    "\n",
    "# 1. Load base frozen ViT\n",
    "base_vit_source_train = load_frozen_vit_backbone(device=DEVICE) # Fresh frozen backbone\n",
    "\n",
    "# 2. Create a new LoRA-adapted ViT for the source domain\n",
    "source_vit_lora = copy.deepcopy(base_vit_source_train) # Start with frozen base\n",
    "source_vit_lora = inject_lora_to_vit_attention(source_vit_lora, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT)\n",
    "source_vit_lora = set_layernorm_affine_trainable(source_vit_lora, trainable=True) # Make LN affine trainable\n",
    "source_vit_lora = source_vit_lora.to(DEVICE)\n",
    "\n",
    "# 3. Instantiate Source-Specific Head\n",
    "source_head = DomainSpecificHead(in_features=VIT_EMBED_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# 4. Optimizer for Source LoRA parameters, Source Head, and LayerNorm affine parameters\n",
    "params_to_train_source = []\n",
    "for name, param in source_vit_lora.named_parameters():\n",
    "    if param.requires_grad: # LoRA A/B and LayerNorm affine params\n",
    "        params_to_train_source.append(param)\n",
    "params_to_train_source.extend(list(source_head.parameters())) # All head params\n",
    "\n",
    "optimizer_source = optim.AdamW(params_to_train_source, lr=ART_LR_LORA_HEAD_LN, weight_decay=0.05)\n",
    "criterion_source = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# LR Scheduler (Poly Decay)\n",
    "num_total_steps_source = ART_EPOCHS * len(source_train_loader)\n",
    "scheduler_source = optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_source,\n",
    "    lr_lambda=lambda step: (1 - step / num_total_steps_source) ** 0.9 # Poly power 0.9\n",
    ")\n",
    "\n",
    "grad_scaler_source = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "print(f\"Source expert models (LoRA ViT, Head) instantiated for '{SOURCE_DOMAIN_NAME}'.\")\n",
    "trainable_params_count_source = sum(p.numel() for p in params_to_train_source)\n",
    "print(f\"Total trainable parameters for source expert: {trainable_params_count_source:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:06:49.706081Z",
     "iopub.status.busy": "2025-05-16T10:06:49.705598Z",
     "iopub.status.idle": "2025-05-16T10:11:27.837974Z",
     "shell.execute_reply": "2025-05-16T10:11:27.837079Z",
     "shell.execute_reply.started": "2025-05-16T10:06:49.706061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: Source Domain - Training Loop\n",
    "\n",
    "print(f\"\\n--- Training Source Expert ({SOURCE_DOMAIN_NAME}) ---\")\n",
    "best_source_val_acc = 0.0\n",
    "\n",
    "for epoch in range(ART_EPOCHS):\n",
    "    source_vit_lora.train()\n",
    "    source_head.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(source_train_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [{SOURCE_DOMAIN_NAME} Train]\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer_source.zero_grad()\n",
    "        \n",
    "        with autocast_ctx():\n",
    "            # Features from LoRA-ViT\n",
    "                all_features = source_vit_lora.forward_features(images) # Shape: (B, Num_Tokens, Embed_Dim)\n",
    "                cls_features = all_features[:, 0] # Shape: (B, Embed_Dim)\n",
    "\n",
    "                # Apply ViT's own final norm before the head\n",
    "                normed_cls_features = source_vit_lora.norm(cls_features) # Shape: (B, Embed_Dim)\n",
    "                logits = source_head(normed_cls_features) # Shape: (B, NUM_CLASSES)\n",
    "                \n",
    "                # # Debug shapes:\n",
    "                # print(f\"images shape: {images.shape}\")\n",
    "                # print(f\"all_features shape: {all_features.shape}\")\n",
    "                # print(f\"cls_features shape: {cls_features.shape}\")\n",
    "                # print(f\"normed_cls_features shape: {normed_cls_features.shape}\")\n",
    "                # print(f\"logits shape: {logits.shape}\")\n",
    "                # print(f\"labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "\n",
    "                loss = criterion_source(logits, labels)\n",
    "        \n",
    "        grad_scaler_source.scale(loss).backward()\n",
    "        grad_scaler_source.step(optimizer_source)\n",
    "        grad_scaler_source.update()\n",
    "        scheduler_source.step() # Step LR scheduler\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += images.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions.double().item()/total_samples if total_samples > 0 else 0.0, lr=optimizer_source.param_groups[0]['lr'])\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, LR: {optimizer_source.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Validation\n",
    "    source_vit_lora.eval()\n",
    "    source_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(source_val_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [{SOURCE_DOMAIN_NAME} Val]\", leave=False):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with autocast_ctx(): # Apply autocast fix here too\n",
    "                all_features_val = source_vit_lora.forward_features(images) # Shape: (B, Num_Tokens, Embed_Dim)\n",
    "                cls_features_val = all_features_val[:, 0] # Shape: (B, Embed_Dim)\n",
    "                normed_cls_features_val = source_vit_lora.norm(cls_features_val) # Shape: (B, Embed_Dim)\n",
    "                logits = source_head(normed_cls_features_val) # Shape: (B, NUM_CLASSES)\n",
    "                loss = criterion_source(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            val_correct_predictions += torch.sum(preds == labels.data)\n",
    "            val_total_samples += images.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_loss / val_total_samples\n",
    "    epoch_val_acc = val_correct_predictions.double() / val_total_samples\n",
    "    print(f\"Epoch {epoch+1} Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    if epoch_val_acc > best_source_val_acc:\n",
    "        best_source_val_acc = epoch_val_acc\n",
    "        torch.save(source_vit_lora.state_dict(), os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_vit_lora_best.pth\"))\n",
    "        torch.save(source_head.state_dict(), os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_head_best.pth\"))\n",
    "        print(f\"    -> New best Val Acc: {best_source_val_acc:.4f}. Models saved.\")\n",
    "        \n",
    "print(f\"\\nSource domain '{SOURCE_DOMAIN_NAME}' training finished. Best Val Acc: {best_source_val_acc:.4f}\")\n",
    "# Load best models for subsequent use\n",
    "source_vit_lora.load_state_dict(torch.load(os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_vit_lora_best.pth\")))\n",
    "source_head.load_state_dict(torch.load(os.path.join(source_domain_model_save_dir, f\"{SOURCE_DOMAIN_NAME.lower()}_head_best.pth\")))\n",
    "source_vit_lora.eval()\n",
    "source_head.eval()\n",
    "print(\"Best source expert models loaded and set to eval mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.712Z",
     "iopub.execute_input": "2025-05-16T10:11:27.840025Z",
     "iopub.status.busy": "2025-05-16T10:11:27.839150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15b: Baseline Validation of Source Expert on All Domains\n",
    "\n",
    "print(f\"\\n\\n--- Baseline Validation of Source Expert ('{SOURCE_DOMAIN_NAME}') on All Domains ---\")\n",
    "\n",
    "if 'source_vit_lora' not in globals() or source_vit_lora is None or \\\n",
    "   'source_head' not in globals() or source_head is None:\n",
    "    print(f\"Warning: Source expert ({SOURCE_DOMAIN_NAME}) models not available. Skipping baseline validation.\")\n",
    "else:\n",
    "    source_vit_lora.eval() # Ensure in eval mode\n",
    "    source_head.eval()   # Ensure in eval mode\n",
    "\n",
    "    baseline_accuracies_source_expert = {}\n",
    "    criterion_val_baseline = nn.CrossEntropyLoss(label_smoothing=0.1) # Use a fresh criterion for validation\n",
    "\n",
    "    for domain_name_eval in ALL_TRAINABLE_DOMAIN_NAMES:\n",
    "        print(f\"\\n  Validating Source Expert ('{SOURCE_DOMAIN_NAME}') on '{domain_name_eval}' domain...\")\n",
    "\n",
    "        try:\n",
    "            val_dataset_current_domain = OfficeHomeDomainDataset(\n",
    "                DATA_DIR, domain_name_eval,\n",
    "                transform=val_test_transform_weak, # Standard validation transform\n",
    "                split_type='val', # Use the 10% validation split\n",
    "                class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    "            )\n",
    "            if len(val_dataset_current_domain) == 0:\n",
    "                print(f\"    Warning: Validation dataset for '{domain_name_eval}' is empty. Skipping.\")\n",
    "                baseline_accuracies_source_expert[domain_name_eval] = 0.0\n",
    "                continue\n",
    "\n",
    "            val_loader_current_domain = DataLoader(\n",
    "                val_dataset_current_domain,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR: Could not load validation dataset for '{domain_name_eval}': {e}. Skipping.\")\n",
    "            baseline_accuracies_source_expert[domain_name_eval] = -1.0 # Indicate error\n",
    "            continue\n",
    "        \n",
    "        val_loss_domain = 0.0\n",
    "        val_correct_predictions_domain = 0\n",
    "        val_total_samples_domain = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader_current_domain, desc=f\"Val on {domain_name_eval}\", leave=False):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                with autocast_ctx():\n",
    "                    # Pass images through the Source LoRA-ViT\n",
    "                    all_features = source_vit_lora.forward_features(images)\n",
    "                    cls_features = all_features[:, 0]\n",
    "                    normed_cls_features = source_vit_lora.norm(cls_features)\n",
    "                    # Then through the Source Head\n",
    "                    logits = source_head(normed_cls_features)\n",
    "                    loss = criterion_val_baseline(logits, labels)\n",
    "\n",
    "                val_loss_domain += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                val_correct_predictions_domain += torch.sum(preds == labels.data)\n",
    "                val_total_samples_domain += images.size(0)\n",
    "\n",
    "        if val_total_samples_domain > 0:\n",
    "            epoch_val_loss_domain = val_loss_domain / val_total_samples_domain\n",
    "            epoch_val_acc_domain = val_correct_predictions_domain.double() / val_total_samples_domain\n",
    "            baseline_accuracies_source_expert[domain_name_eval] = epoch_val_acc_domain.item() * 100 # Store as percentage\n",
    "            print(f\"    '{domain_name_eval}' Val Loss: {epoch_val_loss_domain:.4f}, Val Acc: {epoch_val_acc_domain.item()*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"    No samples processed for '{domain_name_eval}' validation.\")\n",
    "            baseline_accuracies_source_expert[domain_name_eval] = 0.0\n",
    "\n",
    "    # --- Print Summary of Baseline Accuracies ---\n",
    "    print(\"\\n--- Source Expert Baseline Performance Summary ---\")\n",
    "    avg_baseline_acc = 0\n",
    "    count_valid_domains = 0\n",
    "    for domain, acc in baseline_accuracies_source_expert.items():\n",
    "        if acc != -1.0: # Check for loading errors\n",
    "            print(f\"  Accuracy of '{SOURCE_DOMAIN_NAME}' expert on '{domain}': {acc:.2f}%\")\n",
    "            if domain != SOURCE_DOMAIN_NAME: # Calculate average target accuracy (excluding source-on-source)\n",
    "                avg_baseline_acc += acc\n",
    "                count_valid_domains +=1\n",
    "        else:\n",
    "            print(f\"  Accuracy of '{SOURCE_DOMAIN_NAME}' expert on '{domain}': ERROR (Dataset issue)\")\n",
    "    \n",
    "    if count_valid_domains > 0:\n",
    "        avg_target_acc = avg_baseline_acc / count_valid_domains\n",
    "        print(f\"  Average Target Domain Accuracy (Source Expert): {avg_target_acc:.2f}%\")\n",
    "    else:\n",
    "        print(\"  Could not calculate average target domain accuracy.\")\n",
    "\n",
    "    # Sanity check (optional, based on expected performance)\n",
    "    # if SOURCE_DOMAIN_NAME in baseline_accuracies_source_expert and baseline_accuracies_source_expert[SOURCE_DOMAIN_NAME] < 10.0: # Very low\n",
    "    #     print(f\"WARNING: Source expert validation accuracy on its own domain ('{SOURCE_DOMAIN_NAME}') is very low ({baseline_accuracies_source_expert[SOURCE_DOMAIN_NAME]:.2f}%). Training might have issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 16: Prepare for Continual Learning - Store Experts\n",
    "\n",
    "# This dictionary will store the best trained expert (LoRA ViT + Head) for each domain\n",
    "# It will be populated as we adapt to each new domain.\n",
    "# For inference pipeline, this will be part of `all_task_experts`.\n",
    "adapted_experts = {} \n",
    "\n",
    "# Store the source expert (which is already trained and loaded as best)\n",
    "# This is the starting point for the first adaptation.\n",
    "# For the robust inference pipeline, this source expert will also be needed.\n",
    "# We can add it to `all_task_experts` later in the inference setup cell.\n",
    "\n",
    "# Keep track of the \"current best expert\" to initialize the next adaptation stage\n",
    "# Initially, this is the source expert.\n",
    "current_expert_vit = copy.deepcopy(source_vit_lora).cpu() # Move to CPU to save GPU VRAM if not immediately needed\n",
    "current_expert_head = copy.deepcopy(source_head).cpu()\n",
    "current_expert_domain_name = SOURCE_DOMAIN_NAME\n",
    "\n",
    "# Global frozen ViT backbone (without LoRA) for DC head and as base for new LoRA experts\n",
    "base_vit_frozen_global = load_frozen_vit_backbone(device=DEVICE)\n",
    "\n",
    "# Optional: Initialize Replay Buffer if used\n",
    "# global_replay_buffer = ExperienceReplayBuffer(REPLAY_BUFFER_SIZE, device=DEVICE) if REPLAY_BUFFER_SIZE > 0 else None\n",
    "global_replay_buffer = None # Disable by default for simplicity first\n",
    "\n",
    "print(f\"Preparation for continual learning complete. Current expert: '{current_expert_domain_name}'.\")\n",
    "print(f\"Base frozen ViT ('base_vit_frozen_global') loaded for future use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-05-16T10:43:57.950349Z",
     "shell.execute_reply": "2025-05-16T10:43:57.949359Z",
     "shell.execute_reply.started": "2025-05-16T10:11:48.867638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 17: Continual Adaptation Loop for Target Domains\n",
    "\n",
    "# Ensure current_expert_vit, current_expert_head, and current_expert_domain_name \n",
    "# are initialized from the source domain training (after Cell 16)\n",
    "\n",
    "for target_domain_idx, target_domain_name_current_loop in enumerate(TARGET_DOMAIN_NAMES_ORDERED):\n",
    "    \n",
    "    TARGET_DOMAIN_NAME_CURRENT = target_domain_name_current_loop\n",
    "    if target_domain_idx == 0 and current_expert_domain_name == SOURCE_DOMAIN_NAME:\n",
    "        PREVIOUS_DOMAIN_NAME = SOURCE_DOMAIN_NAME\n",
    "    elif target_domain_idx > 0 and current_expert_domain_name == TARGET_DOMAIN_NAMES_ORDERED[target_domain_idx-1]:\n",
    "         PREVIOUS_DOMAIN_NAME = current_expert_domain_name\n",
    "    else:\n",
    "        # This case should ideally not happen if current_expert_domain_name is updated correctly\n",
    "        print(f\"Warning: Mismatch or unexpected previous domain. current_expert_domain_name='{current_expert_domain_name}', expected previous for '{TARGET_DOMAIN_NAME_CURRENT}'\")\n",
    "        PREVIOUS_DOMAIN_NAME = current_expert_domain_name # Fallback\n",
    "\n",
    "    print(f\"\\n\\n=== Starting Adaptation: {PREVIOUS_DOMAIN_NAME} -> {TARGET_DOMAIN_NAME_CURRENT} ===\")\n",
    "\n",
    "    target_model_save_dir = os.path.join(MODEL_SAVE_DIR_BASE, TARGET_DOMAIN_NAME_CURRENT)\n",
    "    os.makedirs(target_model_save_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1. Datasets & DataLoaders ---\n",
    "    print(f\"  Loading datasets for Previous ('{PREVIOUS_DOMAIN_NAME}') and Target ('{TARGET_DOMAIN_NAME_CURRENT}')...\")\n",
    "    previous_domain_train_dataset = OfficeHomeDomainDataset(\n",
    "        DATA_DIR, PREVIOUS_DOMAIN_NAME, transform=train_transform_strong,\n",
    "        split_type='train', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    "    )\n",
    "    previous_domain_train_loader = DataLoader(\n",
    "        previous_domain_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    target_unlabeled_loader_for_ema_shot = DataLoader( \n",
    "        OfficeHomeDomainDataset(DATA_DIR, TARGET_DOMAIN_NAME_CURRENT, transform=val_test_transform_weak, \n",
    "                                split_type='train', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    fixmatch_target_dataset = FixMatchOfficeHomeDataset(\n",
    "        DATA_DIR, TARGET_DOMAIN_NAME_CURRENT,\n",
    "        transform_weak=val_test_transform_weak, \n",
    "        transform_strong=train_transform_strong, \n",
    "        split_type='train', \n",
    "        class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    "    )\n",
    "    fixmatch_target_loader = DataLoader(\n",
    "        fixmatch_target_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    target_val_dataset = OfficeHomeDomainDataset(\n",
    "        DATA_DIR, TARGET_DOMAIN_NAME_CURRENT, transform=val_test_transform_weak,\n",
    "        split_type='val', class_to_idx_mapping=GLOBAL_CLASS_TO_IDX\n",
    "    )\n",
    "    target_val_loader = DataLoader(target_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    print(f\"  Previous domain ('{PREVIOUS_DOMAIN_NAME}') train loader: {len(previous_domain_train_loader)} batches.\")\n",
    "    print(f\"  Target domain ('{TARGET_DOMAIN_NAME_CURRENT}'):\")\n",
    "    print(f\"    EMA/SHOT loader dataset size: {len(target_unlabeled_loader_for_ema_shot.dataset)}\")\n",
    "    print(f\"    FixMatch loader dataset size: {len(fixmatch_target_loader.dataset)}\")\n",
    "    print(f\"    Val dataset size: {len(target_val_dataset)}\")\n",
    "\n",
    "    # --- 2. Model Initialization ---\n",
    "    print(f\"  Initializing Student and Teacher models for '{TARGET_DOMAIN_NAME_CURRENT}'...\")\n",
    "    student_vit = copy.deepcopy(base_vit_frozen_global) \n",
    "    student_vit = inject_lora_to_vit_attention(student_vit, rank=LORA_RANK, alpha=LORA_ALPHA, lora_dropout_p=LORA_DROPOUT)\n",
    "    \n",
    "    prev_expert_vit_state_dict = current_expert_vit.cpu().state_dict()\n",
    "    student_vit_state_dict_new = student_vit.state_dict()\n",
    "    load_dict_student_vit_warm_start = {}\n",
    "    for k, v in prev_expert_vit_state_dict.items():\n",
    "        if ('lora_' in k or ('.norm' in k and (k.endswith('.weight') or k.endswith('.bias')))) and \\\n",
    "           k in student_vit_state_dict_new and student_vit_state_dict_new[k].shape == v.shape:\n",
    "            load_dict_student_vit_warm_start[k] = v\n",
    "    if load_dict_student_vit_warm_start:\n",
    "        missing_keys_vit, unexpected_keys_vit = student_vit.load_state_dict(load_dict_student_vit_warm_start, strict=False)\n",
    "        print(f\"  Loaded LoRA/LN from previous expert '{current_expert_domain_name}'. Missing: {len(missing_keys_vit)}, Unexpected: {len(unexpected_keys_vit)}\")\n",
    "    else:\n",
    "        print(f\"  Warning: No LoRA/LN weights loaded from previous expert '{current_expert_domain_name}'. Starting fresh LoRA/LN for student ViT.\")\n",
    "\n",
    "    student_vit = set_layernorm_affine_trainable(student_vit, trainable=True) \n",
    "    student_vit = student_vit.to(DEVICE)\n",
    "\n",
    "    student_head = DomainSpecificHead(in_features=VIT_EMBED_DIM, num_classes=NUM_CLASSES)\n",
    "    student_head.load_state_dict(current_expert_head.cpu().state_dict()) \n",
    "    student_head = student_head.to(DEVICE)\n",
    "\n",
    "    teacher_vit = copy.deepcopy(student_vit).to(DEVICE)\n",
    "    teacher_head = copy.deepcopy(student_head).to(DEVICE)\n",
    "    for param in teacher_vit.parameters(): param.requires_grad = False\n",
    "    for param in teacher_head.parameters(): param.requires_grad = False\n",
    "    teacher_vit.eval(); teacher_head.eval()\n",
    "    update_ema_teacher_components(student_vit, student_head, teacher_vit, teacher_head, 0.0) \n",
    "\n",
    "    p_theta_dad = DAD_P_Theta_MLP().to(DEVICE)\n",
    "    print(f\"  Student, Teacher, and DAD p_theta models ready for '{TARGET_DOMAIN_NAME_CURRENT}'.\")\n",
    "\n",
    "    # --- 3. Optimizers ---\n",
    "    params_to_train_student = []\n",
    "    for name, param in student_vit.named_parameters():\n",
    "        if param.requires_grad: params_to_train_student.append(param)\n",
    "    params_to_train_student.extend(list(student_head.parameters()))\n",
    "\n",
    "    optimizer_student = optim.AdamW(params_to_train_student, lr=ADAPT_LR_LORA_HEAD_LN, weight_decay=0.05)\n",
    "    optimizer_p_theta = optim.SGD(p_theta_dad.parameters(), lr=ADAPT_LR_P_THETA, momentum=0.9, weight_decay=4.5e-3)\n",
    "    \n",
    "    grad_scaler_adapt = GradScaler(enabled=(DEVICE.type == 'cuda')) \n",
    "    criterion_adapt_ce = nn.CrossEntropyLoss(label_smoothing=0.1) # Added label smoothing\n",
    "    criterion_adapt_mse = nn.MSELoss() \n",
    "    print(\"  Optimizers created.\")\n",
    "\n",
    "    # --- 4. DAD LTR Pre-training ---\n",
    "    print(f\"  Starting DAD LTR Pre-training for p_theta ({PREVIOUS_DOMAIN_NAME} -> {TARGET_DOMAIN_NAME_CURRENT})...\")\n",
    "    p_theta_dad.train()\n",
    "    student_vit.eval(); student_head.eval() \n",
    "    for ltr_epoch in range(ADAPT_LTR_EPOCHS):\n",
    "        ltr_running_loss = 0.0\n",
    "        progress_bar_ltr = tqdm(target_unlabeled_loader_for_ema_shot, desc=f\"LTR Epoch {ltr_epoch+1}/{ADAPT_LTR_EPOCHS} for {TARGET_DOMAIN_NAME_CURRENT}\", leave=False)\n",
    "        for target_images_weak, _ in progress_bar_ltr: \n",
    "            target_images_weak = target_images_weak.to(DEVICE)\n",
    "            optimizer_p_theta.zero_grad()\n",
    "            with torch.no_grad(): \n",
    "                all_target_features = student_vit.forward_features(target_images_weak) \n",
    "                F_T_cls = all_target_features[:, 0].to(torch.float32) \n",
    "            t_indices = torch.randint(0, DAD_K_STEPS, (F_T_cls.size(0),), device=DEVICE, dtype=torch.long)\n",
    "            noise = torch.randn_like(F_T_cls)\n",
    "            F_T_noisy = q_sample_dad(F_T_cls, t_indices, noise)\n",
    "            predicted_noise = p_theta_dad(F_T_noisy, t_indices) \n",
    "            loss_ltr = criterion_adapt_mse(predicted_noise, noise)\n",
    "            loss_ltr.backward()\n",
    "            optimizer_p_theta.step() \n",
    "            ltr_running_loss += loss_ltr.item() * target_images_weak.size(0)\n",
    "            progress_bar_ltr.set_postfix(ltr_loss=loss_ltr.item())\n",
    "        avg_ltr_loss = ltr_running_loss / len(target_unlabeled_loader_for_ema_shot.dataset) if len(target_unlabeled_loader_for_ema_shot.dataset) > 0 else 0\n",
    "        print(f\"  LTR Epoch {ltr_epoch+1} ({TARGET_DOMAIN_NAME_CURRENT}) Avg Loss: {avg_ltr_loss:.4f}\")\n",
    "    print(f\"  DAD LTR Pre-training for '{TARGET_DOMAIN_NAME_CURRENT}' finished.\")\n",
    "\n",
    "    # --- 5. Main Adaptation Loop (formerly Cell 18) ---\n",
    "    print(f\"  Starting Main Adaptation Loop for '{TARGET_DOMAIN_NAME_CURRENT}' ({DAD_K_STEPS} DAD steps, {ADAPT_MLS_R_ITER} MLS iters/step)...\")\n",
    "    \n",
    "    # best_target_val_loss = float('inf') # Moved to top of this cell block\n",
    "    # best_target_val_acc_at_best_loss = 0.0 # Moved to top\n",
    "    best_target_val_acc = 0.0 # << CHANGED: Save based on accuracy\n",
    "    val_loss_at_best_acc = float('inf') # Track loss when best acc is found\n",
    "    patience_counter_adapt = 0\n",
    "\n",
    "\n",
    "    num_total_adapt_optimizer_steps_current = DAD_K_STEPS * (ADAPT_MLS_R_ITER + 1) \n",
    "    scheduler_student_adapt = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_student, T_max=num_total_adapt_optimizer_steps_current, eta_min=1e-6 # Added eta_min\n",
    "    )\n",
    "    print(f\"  Student optimizer LR scheduler for '{TARGET_DOMAIN_NAME_CURRENT}': CosineAnnealingLR with T_max={num_total_adapt_optimizer_steps_current}\")\n",
    "\n",
    "    source_iter = iter(previous_domain_train_loader)\n",
    "    target_iter_ema_shot = iter(target_unlabeled_loader_for_ema_shot) \n",
    "    fixmatch_iter = iter(fixmatch_target_loader) \n",
    "    outer_progress_bar = tqdm(range(DAD_K_STEPS), desc=f\"DAD Steps for {TARGET_DOMAIN_NAME_CURRENT}\")\n",
    "\n",
    "    for k_dad_step_idx in outer_progress_bar: \n",
    "        for mls_iter_idx in range(ADAPT_MLS_R_ITER):\n",
    "            try:\n",
    "                source_images, source_labels = next(source_iter)\n",
    "            except StopIteration:\n",
    "                source_iter = iter(previous_domain_train_loader)\n",
    "                source_images, source_labels = next(source_iter)\n",
    "            source_images, source_labels = source_images.to(DEVICE), source_labels.to(DEVICE)\n",
    "\n",
    "            p_theta_dad.train(); student_vit.eval(); student_head.eval()\n",
    "            with torch.no_grad():\n",
    "                all_source_features = student_vit.forward_features(source_images) \n",
    "                F_S_cls = all_source_features[:, 0].to(torch.float32) \n",
    "            t_k_current_long = torch.full((F_S_cls.size(0),), k_dad_step_idx, device=DEVICE, dtype=torch.long)\n",
    "            noise_cd = torch.randn_like(F_S_cls)\n",
    "            F_S_cls_noisy_cd = q_sample_dad(F_S_cls, t_k_current_long, noise_cd)\n",
    "            predicted_noise_cd = p_theta_dad(F_S_cls_noisy_cd, t_k_current_long)\n",
    "            with torch.no_grad():\n",
    "                 x0_hat_cd = q_reconstruct_dad(F_S_cls_noisy_cd, predicted_noise_cd, t_k_current_long)\n",
    "            with autocast_ctx():\n",
    "                logits_for_ptheta_update = student_head(student_vit.norm(x0_hat_cd)) \n",
    "            loss_cd = criterion_adapt_ce(logits_for_ptheta_update, source_labels)\n",
    "            optimizer_p_theta.zero_grad(); loss_cd.backward(); optimizer_p_theta.step()\n",
    "\n",
    "            p_theta_dad.eval(); student_vit.train(); student_head.train()\n",
    "            with torch.no_grad():\n",
    "                predicted_noise_dc = p_theta_dad(F_S_cls_noisy_cd, t_k_current_long)\n",
    "                x0_hat_dc = q_reconstruct_dad(F_S_cls_noisy_cd, predicted_noise_dc, t_k_current_long)\n",
    "            with autocast_ctx():\n",
    "                logits_for_student_update = student_head(student_vit.norm(x0_hat_dc.detach())) \n",
    "                loss_dc_val = criterion_adapt_ce(logits_for_student_update, source_labels)\n",
    "                if global_replay_buffer and len(global_replay_buffer) >= (BATCH_SIZE * REPLAY_BATCH_SIZE_RATIO):\n",
    "                    replayed_images, replayed_labels = global_replay_buffer.sample(int(BATCH_SIZE * REPLAY_BATCH_SIZE_RATIO))\n",
    "                    all_replayed_F_S_cls = student_vit.forward_features(replayed_images)\n",
    "                    cls_replayed_F_S_cls = all_replayed_F_S_cls[:,0] \n",
    "                    replayed_logits = student_head(student_vit.norm(cls_replayed_F_S_cls))\n",
    "                    loss_replay = criterion_adapt_ce(replayed_logits, replayed_labels)\n",
    "                    loss_dc_val += REPLAY_LAMBDA * loss_replay\n",
    "            optimizer_student.zero_grad(); grad_scaler_adapt.scale(loss_dc_val).backward(); grad_scaler_adapt.step(optimizer_student); grad_scaler_adapt.update()\n",
    "            scheduler_student_adapt.step() \n",
    "            if global_replay_buffer: global_replay_buffer.add(source_images, source_labels)\n",
    "        \n",
    "        current_mls_postfix = {\"MLS_D->C\": f\"{loss_dc_val.item():.3f}\", \"MLS_C->D\": f\"{loss_cd.item():.3f}\"} # Corrected key\n",
    "        outer_progress_bar.set_postfix(current_mls_postfix)\n",
    "\n",
    "        student_vit.train(); student_head.train()\n",
    "        try:\n",
    "            target_images_for_ema_shot, _ = next(target_iter_ema_shot)\n",
    "        except StopIteration:\n",
    "            target_iter_ema_shot = iter(target_unlabeled_loader_for_ema_shot)\n",
    "            target_images_for_ema_shot, _ = next(target_iter_ema_shot)\n",
    "        target_images_for_ema_shot = target_images_for_ema_shot.to(DEVICE)\n",
    "        try:\n",
    "            target_images_weak_fixmatch, target_images_strong_fixmatch = next(fixmatch_iter)\n",
    "        except StopIteration:\n",
    "            fixmatch_iter = iter(fixmatch_target_loader)\n",
    "            target_images_weak_fixmatch, target_images_strong_fixmatch = next(fixmatch_iter)\n",
    "        target_images_weak_fixmatch = target_images_weak_fixmatch.to(DEVICE)\n",
    "        target_images_strong_fixmatch = target_images_strong_fixmatch.to(DEVICE)\n",
    "\n",
    "        optimizer_student.zero_grad() \n",
    "        loss_pl_val_ema = torch.tensor(0.0, device=DEVICE) \n",
    "        loss_fixmatch_val_calc = torch.tensor(0.0, device=DEVICE) \n",
    "        current_progress_ratio = k_dad_step_idx / max(1, DAD_K_STEPS - 1)\n",
    "        current_pseudo_label_thresh = PSEUDO_LABEL_THRESHOLD_START + \\\n",
    "                                     (PSEUDO_LABEL_THRESHOLD_END - PSEUDO_LABEL_THRESHOLD_START) * current_progress_ratio\n",
    "        num_pseudo_labels_ema = 0\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            teacher_vit.eval(); teacher_head.eval()\n",
    "            all_F_T_teacher = teacher_vit.forward_features(target_images_for_ema_shot) \n",
    "            cls_F_T_teacher = all_F_T_teacher[:, 0] \n",
    "            norm_cls_F_T_teacher = teacher_vit.norm(cls_F_T_teacher)\n",
    "            logits_teacher = teacher_head(norm_cls_F_T_teacher) \n",
    "            probs_teacher = F.softmax(logits_teacher, dim=1) \n",
    "            max_confidence_values, predicted_class_indices = torch.max(probs_teacher, dim=1) \n",
    "            confidence_mask_ema = max_confidence_values >= current_pseudo_label_thresh\n",
    "            pseudo_labels_for_loss = predicted_class_indices \n",
    "        if confidence_mask_ema.any():\n",
    "            num_pseudo_labels_ema = confidence_mask_ema.sum().item()\n",
    "            with autocast_ctx():\n",
    "                selected_target_images_ema = target_images_for_ema_shot[confidence_mask_ema]\n",
    "                all_F_T_student_ema = student_vit.forward_features(selected_target_images_ema)\n",
    "                cls_F_T_student_ema = all_F_T_student_ema[:, 0] \n",
    "                logits_student_on_pseudo = student_head(student_vit.norm(cls_F_T_student_ema))\n",
    "                loss_pl_val_ema = criterion_adapt_ce(logits_student_on_pseudo, pseudo_labels_for_loss[confidence_mask_ema])\n",
    "        num_pseudo_labels_fixmatch = 0\n",
    "        with torch.no_grad(), autocast_ctx(): \n",
    "            all_F_T_student_weak_fixmatch = student_vit.forward_features(target_images_weak_fixmatch)\n",
    "            cls_F_T_student_weak_fixmatch = all_F_T_student_weak_fixmatch[:, 0] \n",
    "            logits_weak_student_fixmatch = student_head(student_vit.norm(cls_F_T_student_weak_fixmatch))\n",
    "            probs_weak_student_fixmatch = F.softmax(logits_weak_student_fixmatch, dim=1)\n",
    "            max_probs_weak_fixmatch, pseudo_labels_weak_fixmatch = torch.max(probs_weak_student_fixmatch, dim=1)\n",
    "            confidence_mask_fixmatch = max_probs_weak_fixmatch >= FIXMATCH_CONF_THRESHOLD\n",
    "        if confidence_mask_fixmatch.any():\n",
    "            num_pseudo_labels_fixmatch = confidence_mask_fixmatch.sum().item()\n",
    "            with autocast_ctx(): \n",
    "                selected_target_images_strong_fixmatch = target_images_strong_fixmatch[confidence_mask_fixmatch]\n",
    "                all_F_T_student_strong_fixmatch = student_vit.forward_features(selected_target_images_strong_fixmatch)\n",
    "                cls_F_T_student_strong_fixmatch = all_F_T_student_strong_fixmatch[:, 0] \n",
    "                logits_student_on_strong_fixmatch = student_head(student_vit.norm(cls_F_T_student_strong_fixmatch))\n",
    "                loss_fixmatch_val_calc = criterion_adapt_ce(logits_student_on_strong_fixmatch, pseudo_labels_weak_fixmatch[confidence_mask_fixmatch])\n",
    "                loss_fixmatch_val_calc = FIXMATCH_LAMBDA * loss_fixmatch_val_calc\n",
    "        loss_for_head_update = torch.tensor(0.0, device=DEVICE)\n",
    "        if loss_pl_val_ema.item() > 0: loss_for_head_update += loss_pl_val_ema\n",
    "        if loss_fixmatch_val_calc.item() > 0: loss_for_head_update += loss_fixmatch_val_calc\n",
    "        if loss_for_head_update.item() > 0:\n",
    "            grad_scaler_adapt.scale(loss_for_head_update).backward(retain_graph=True) \n",
    "        _temp_head_training_mode_shot = student_head.training\n",
    "        _temp_head_params_req_grad_shot = [p.requires_grad for p in student_head.parameters()]\n",
    "        student_head.eval() \n",
    "        for p_head in student_head.parameters(): p_head.requires_grad = False\n",
    "        with autocast_ctx():\n",
    "            all_F_T_student_shot = student_vit.forward_features(target_images_for_ema_shot) \n",
    "            cls_F_T_student_shot = all_F_T_student_shot[:, 0] \n",
    "            logits_student_shot = student_head(student_vit.norm(cls_F_T_student_shot)) \n",
    "            probs_student_shot = F.softmax(logits_student_shot, dim=1)\n",
    "            loss_cond_ent = - (probs_student_shot * torch.log(probs_student_shot + 1e-9)).sum(1).mean()\n",
    "            mean_probs_shot = probs_student_shot.mean(0)\n",
    "            loss_ent_max = - (mean_probs_shot * torch.log(mean_probs_shot + 1e-9)).sum() \n",
    "            loss_shot_val_calculated = SHOT_LAMBDA_COND_ENT * loss_cond_ent + SHOT_LAMBDA_ENT_MAX * loss_ent_max\n",
    "        if loss_shot_val_calculated.item() != 0: \n",
    "            grad_scaler_adapt.scale(loss_shot_val_calculated).backward() \n",
    "        student_head.train(_temp_head_training_mode_shot)\n",
    "        for i_param, p_rg_status_shot in enumerate(_temp_head_params_req_grad_shot):\n",
    "            list(student_head.parameters())[i_param].requires_grad = p_rg_status_shot\n",
    "        if loss_for_head_update.item() > 0 or loss_shot_val_calculated.item() > 0:\n",
    "            grad_scaler_adapt.step(optimizer_student) \n",
    "            grad_scaler_adapt.update()\n",
    "        scheduler_student_adapt.step() \n",
    "        update_ema_teacher_components(student_vit, student_head, teacher_vit, teacher_head, EMA_DECAY)\n",
    "        current_target_postfix = {\n",
    "            \"PL_EMA\": f\"{loss_pl_val_ema.item():.3f}({num_pseudo_labels_ema})\",\n",
    "            \"PL_FixM\": f\"{loss_fixmatch_val_calc.item() / FIXMATCH_LAMBDA if FIXMATCH_LAMBDA > 0 and loss_fixmatch_val_calc.item() > 0 else 0.0:.3f}({num_pseudo_labels_fixmatch})\",\n",
    "            \"SHOT\": f\"{loss_shot_val_calculated.item():.3f}\",\n",
    "            \"LR_stud\": f\"{optimizer_student.param_groups[0]['lr']:.2e}\"}\n",
    "        combined_postfix = {**(current_mls_postfix if isinstance(current_mls_postfix, dict) else {}), **current_target_postfix}\n",
    "        outer_progress_bar.set_postfix(combined_postfix)\n",
    "\n",
    "        if (k_dad_step_idx + 1) % (DAD_K_STEPS // 10) == 0 or k_dad_step_idx == DAD_K_STEPS - 1: \n",
    "            student_vit.eval(); student_head.eval()\n",
    "            val_loss_target_epoch = 0.0; val_correct_target_epoch = 0; val_total_target_epoch = 0\n",
    "            val_progress_bar = tqdm(target_val_loader, desc=f\"Val {TARGET_DOMAIN_NAME_CURRENT} (DAD {k_dad_step_idx+1})\", leave=False)\n",
    "            with torch.no_grad():\n",
    "                for images_val, labels_val in val_progress_bar:\n",
    "                    images_val, labels_val = images_val.to(DEVICE), labels_val.to(DEVICE)\n",
    "                    with autocast_ctx():\n",
    "                        all_features_val = student_vit.forward_features(images_val)\n",
    "                        cls_features_val = all_features_val[:,0] \n",
    "                        logits_val = student_head(student_vit.norm(cls_features_val))\n",
    "                        loss_val = criterion_adapt_ce(logits_val, labels_val)\n",
    "                    val_loss_target_epoch += loss_val.item() * images_val.size(0)\n",
    "                    _, preds_val = torch.max(logits_val, 1)\n",
    "                    val_correct_target_epoch += torch.sum(preds_val == labels_val.data)\n",
    "                    val_total_target_epoch += images_val.size(0)\n",
    "            epoch_val_loss_target = val_loss_target_epoch / val_total_target_epoch if val_total_target_epoch > 0 else 0\n",
    "            epoch_val_acc_target = val_correct_target_epoch.double() / val_total_target_epoch if val_total_target_epoch > 0 else 0.0\n",
    "            current_val_postfix_info = {\n",
    "                \"ValAcc\": f\"{epoch_val_acc_target.item():.4f}\", \n",
    "                \"ValLoss\": f\"{epoch_val_loss_target:.4f}\"}\n",
    "            existing_postfix_before_val = outer_progress_bar.postfix if isinstance(outer_progress_bar.postfix, dict) else {}\n",
    "            outer_progress_bar.set_postfix({**existing_postfix_before_val, **current_val_postfix_info})\n",
    "            print(f\"  DAD Step {k_dad_step_idx+1}/{DAD_K_STEPS} - Target '{TARGET_DOMAIN_NAME_CURRENT}' Val Loss: {epoch_val_loss_target:.4f}, Val Acc: {epoch_val_acc_target.item():.4f}\")\n",
    "\n",
    "            # << CHANGED: Save based on best validation accuracy >>\n",
    "            if epoch_val_acc_target.item() > best_target_val_acc and val_total_target_epoch > 0 : \n",
    "                best_target_val_acc = epoch_val_acc_target.item()\n",
    "                val_loss_at_best_acc = epoch_val_loss_target \n",
    "                torch.save(student_vit.state_dict(), os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_vit_lora_best.pth\"))\n",
    "                torch.save(student_head.state_dict(), os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_head_best.pth\"))\n",
    "                print(f\"    -> New best Val Acc for '{TARGET_DOMAIN_NAME_CURRENT}': {best_target_val_acc:.4f} (Loss: {val_loss_at_best_acc:.4f}). Models saved.\")\n",
    "                patience_counter_adapt = 0 # Reset patience\n",
    "            elif val_total_target_epoch > 0: # Accuracy did not improve\n",
    "                patience_counter_adapt += 1\n",
    "                print(f\"    Val acc did not improve. Patience: {patience_counter_adapt}/{EARLY_STOPPING_PATIENCE_ADAPT}\")\n",
    "        \n",
    "        if patience_counter_adapt >= EARLY_STOPPING_PATIENCE_ADAPT:\n",
    "            print(f\"  Early stopping triggered for '{TARGET_DOMAIN_NAME_CURRENT}' after {k_dad_step_idx+1} DAD steps.\")\n",
    "            break # Break the outer DAD step loop for the current target domain\n",
    "\n",
    "    print(f\"\\nAdaptation to '{TARGET_DOMAIN_NAME_CURRENT}' finished. Best Val Acc: {best_target_val_acc:.4f} (at Val Loss: {val_loss_at_best_acc:.4f})\")\n",
    "\n",
    "    if os.path.exists(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_vit_lora_best.pth\")):\n",
    "        student_vit.load_state_dict(torch.load(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_vit_lora_best.pth\")))\n",
    "        student_head.load_state_dict(torch.load(os.path.join(target_model_save_dir, f\"{TARGET_DOMAIN_NAME_CURRENT.lower()}_head_best.pth\")))\n",
    "        print(f\"Loaded best saved models (by accuracy) for {TARGET_DOMAIN_NAME_CURRENT}.\")\n",
    "    else:\n",
    "        print(f\"Warning: No best saved models found for {TARGET_DOMAIN_NAME_CURRENT}. Using last state of student model.\")\n",
    "    student_vit.eval(); student_head.eval()\n",
    "\n",
    "    adapted_experts[TARGET_DOMAIN_NAME_CURRENT] = {\n",
    "        'vit': copy.deepcopy(student_vit).cpu(), \n",
    "        'head': copy.deepcopy(student_head).cpu()\n",
    "    }\n",
    "    current_expert_vit = copy.deepcopy(student_vit).cpu()\n",
    "    current_expert_head = copy.deepcopy(student_head).cpu()\n",
    "    current_expert_domain_name = TARGET_DOMAIN_NAME_CURRENT \n",
    "    print(f\"Best models for '{TARGET_DOMAIN_NAME_CURRENT}' loaded and stored. Ready for next adaptation or evaluation.\")\n",
    "\n",
    "# End of the loop over TARGET_DOMAIN_NAMES_ORDERED\n",
    "print(\"\\n\\n=== All Target Domain Adaptations Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:43:57.952129Z",
     "iopub.status.busy": "2025-05-16T10:43:57.951892Z",
     "iopub.status.idle": "2025-05-16T10:43:58.403858Z",
     "shell.execute_reply": "2025-05-16T10:43:58.403324Z",
     "shell.execute_reply.started": "2025-05-16T10:43:57.952097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell X1: Domain Classifier - Prepare Multi-Domain Dataset & DataLoader\n",
    "\n",
    "print(\"\\n\\n=== Part 5: Domain Classifier Training ===\")\n",
    "\n",
    "# ALL_TRAINABLE_DOMAIN_NAMES was defined in Cell 3: [SOURCE_DOMAIN_NAME] + TARGET_DOMAIN_NAMES_ORDERED\n",
    "# e.g., ['Art', 'Clipart', 'Product', 'RealWorld']\n",
    "num_total_domains_for_dc = len(ALL_TRAINABLE_DOMAIN_NAMES)\n",
    "\n",
    "# Create a mapping from domain name to an integer index for the DC\n",
    "domain_name_to_idx_dc = {name: i for i, name in enumerate(ALL_TRAINABLE_DOMAIN_NAMES)}\n",
    "domain_idx_to_name_dc = {i: name for i, name in enumerate(ALL_TRAINABLE_DOMAIN_NAMES)}\n",
    "print(f\"Domain Classifier - Domain to Index Mapping: {domain_name_to_idx_dc}\")\n",
    "\n",
    "class MultiDomainDatasetForDC(Dataset):\n",
    "    def __init__(self, root_dir, all_domain_names, domain_to_idx_map, \n",
    "                 class_to_idx_overall_map, transform, split_type='train'):\n",
    "        self.images_paths = []\n",
    "        self.domain_labels = [] # Integer domain labels for DC\n",
    "\n",
    "        for domain_name in all_domain_names:\n",
    "            domain_idx = domain_to_idx_map.get(domain_name)\n",
    "            if domain_idx is None:\n",
    "                print(f\"Warning: Domain '{domain_name}' not in domain_to_idx_map. Skipping for DC dataset.\")\n",
    "                continue\n",
    "            \n",
    "            # Use OfficeHomeDomainDataset to get image paths for the specified split\n",
    "            # We use the 'train' split of each domain to train the DC\n",
    "            # We could also use 'val' or a combined split if desired\n",
    "            temp_domain_dataset = OfficeHomeDomainDataset(\n",
    "                root_dir=root_dir, domain_name=domain_name,\n",
    "                transform=None, # We'll apply the DC's transform later\n",
    "                split_type=split_type, # e.g., 'train' to use 80% of each domain\n",
    "                class_to_idx_mapping=class_to_idx_overall_map,\n",
    "                load_pil=False # We just need paths\n",
    "            )\n",
    "            self.images_paths.extend(temp_domain_dataset.images_paths)\n",
    "            self.domain_labels.extend([domain_idx] * len(temp_domain_dataset.images_paths))\n",
    "            print(f\"  Added {len(temp_domain_dataset.images_paths)} images from '{domain_name}' ({split_type} split) for DC training.\")\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images_paths[idx]\n",
    "        image_pil = Image.open(img_path).convert('RGB')\n",
    "        domain_label = self.domain_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else:\n",
    "            image_tensor = transforms.ToTensor()(image_pil)\n",
    "        \n",
    "        return image_tensor, torch.tensor(domain_label).long()\n",
    "\n",
    "# Use val_test_transform_weak for DC training, as it's about general domain features\n",
    "dc_train_dataset = MultiDomainDatasetForDC(\n",
    "    DATA_DIR, ALL_TRAINABLE_DOMAIN_NAMES, domain_name_to_idx_dc,\n",
    "    GLOBAL_CLASS_TO_IDX, transform=val_test_transform_weak, split_type='train'\n",
    ")\n",
    "# Optional: Create a validation set for the DC from the 'val' splits\n",
    "dc_val_dataset = MultiDomainDatasetForDC(\n",
    "    DATA_DIR, ALL_TRAINABLE_DOMAIN_NAMES, domain_name_to_idx_dc,\n",
    "    GLOBAL_CLASS_TO_IDX, transform=val_test_transform_weak, split_type='val'\n",
    ")\n",
    "\n",
    "dc_train_loader = DataLoader(dc_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "dc_val_loader = DataLoader(dc_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Domain Classifier: Train size {len(dc_train_dataset)}, Val size {len(dc_val_dataset)}\")\n",
    "print(f\"DC Train loader batches: {len(dc_train_loader)}, DC Val loader batches: {len(dc_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.713Z",
     "iopub.execute_input": "2025-05-16T10:43:58.404882Z",
     "iopub.status.busy": "2025-05-16T10:43:58.404635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell X2: Domain Classifier - Model, Optimizer, Training Loop\n",
    "\n",
    "# base_vit_frozen_global was loaded in Cell 16 (original frozen ViT without LoRA)\n",
    "if 'base_vit_frozen_global' not in globals() or base_vit_frozen_global is None:\n",
    "    print(\"ERROR: base_vit_frozen_global not found. Re-loading.\")\n",
    "    base_vit_frozen_global = load_frozen_vit_backbone(device=DEVICE)\n",
    "\n",
    "domain_classifier_head = DomainSpecificHead(\n",
    "    in_features=VIT_EMBED_DIM, \n",
    "    num_classes=num_total_domains_for_dc # Output is number of domains\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer_dc = optim.AdamW(domain_classifier_head.parameters(), lr=DC_HEAD_LR, weight_decay=0.01)\n",
    "criterion_dc = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "grad_scaler_dc = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "# LR Scheduler for DC (optional, e.g., CosineAnnealing)\n",
    "scheduler_dc = optim.lr_scheduler.CosineAnnealingLR(optimizer_dc, T_max=DC_HEAD_EPOCHS * len(dc_train_loader))\n",
    "\n",
    "\n",
    "print(f\"Training Domain Classifier Head for {DC_HEAD_EPOCHS} epochs...\")\n",
    "best_dc_val_acc = 0.0\n",
    "dc_model_save_path = os.path.join(MODEL_SAVE_DIR_BASE, \"domain_classifier_head_best.pth\")\n",
    "\n",
    "for epoch in range(DC_HEAD_EPOCHS):\n",
    "    domain_classifier_head.train()\n",
    "    base_vit_frozen_global.eval() # Ensure backbone is frozen and in eval\n",
    "\n",
    "    running_loss_dc = 0.0\n",
    "    correct_preds_dc = 0\n",
    "    total_samples_dc = 0\n",
    "\n",
    "    progress_bar_dc = tqdm(dc_train_loader, desc=f\"DC Epoch {epoch+1}/{DC_HEAD_EPOCHS} [Train]\")\n",
    "    for images, domain_labels_true in progress_bar_dc:\n",
    "        images, domain_labels_true = images.to(DEVICE), domain_labels_true.to(DEVICE)\n",
    "\n",
    "        optimizer_dc.zero_grad()\n",
    "        with autocast_ctx():\n",
    "            with torch.no_grad(): # Backbone features are fixed\n",
    "                all_features_dc = base_vit_frozen_global.forward_features(images)\n",
    "                cls_features_dc = all_features_dc[:, 0] # Select CLS token\n",
    "            \n",
    "            # Norm from the backbone before passing to DC head\n",
    "            normed_cls_features_dc = base_vit_frozen_global.norm(cls_features_dc)\n",
    "            domain_logits = domain_classifier_head(normed_cls_features_dc)\n",
    "            loss_dc = criterion_dc(domain_logits, domain_labels_true)\n",
    "        \n",
    "        grad_scaler_dc.scale(loss_dc).backward()\n",
    "        grad_scaler_dc.step(optimizer_dc)\n",
    "        grad_scaler_dc.update()\n",
    "        scheduler_dc.step()\n",
    "\n",
    "        running_loss_dc += loss_dc.item() * images.size(0)\n",
    "        _, preds_dc = torch.max(domain_logits, 1)\n",
    "        correct_preds_dc += torch.sum(preds_dc == domain_labels_true.data)\n",
    "        total_samples_dc += images.size(0)\n",
    "        progress_bar_dc.set_postfix(loss=loss_dc.item(), acc=correct_preds_dc.double().item()/total_samples_dc if total_samples_dc > 0 else 0.0)\n",
    "\n",
    "    epoch_loss_dc = running_loss_dc / total_samples_dc\n",
    "    epoch_acc_dc = correct_preds_dc.double() / total_samples_dc\n",
    "    print(f\"DC Epoch {epoch+1} Train Loss: {epoch_loss_dc:.4f}, Train Acc: {epoch_acc_dc:.4f}\")\n",
    "\n",
    "    # Validation for DC\n",
    "    domain_classifier_head.eval()\n",
    "    val_loss_dc_epoch = 0.0\n",
    "    val_correct_dc = 0\n",
    "    val_total_dc = 0\n",
    "    with torch.no_grad():\n",
    "        for images_val, domain_labels_val_true in tqdm(dc_val_loader, desc=f\"DC Epoch {epoch+1}/{DC_HEAD_EPOCHS} [Val]\", leave=False):\n",
    "            images_val, domain_labels_val_true = images_val.to(DEVICE), domain_labels_val_true.to(DEVICE)\n",
    "            with autocast_ctx():\n",
    "                all_features_val_dc = base_vit_frozen_global.forward_features(images_val)\n",
    "                cls_features_val_dc = all_features_val_dc[:, 0]\n",
    "                normed_cls_features_val_dc = base_vit_frozen_global.norm(cls_features_val_dc)\n",
    "                domain_logits_val = domain_classifier_head(normed_cls_features_val_dc)\n",
    "                loss_val_dc = criterion_dc(domain_logits_val, domain_labels_val_true)\n",
    "\n",
    "            val_loss_dc_epoch += loss_val_dc.item() * images_val.size(0)\n",
    "            _, preds_val_dc = torch.max(domain_logits_val, 1)\n",
    "            val_correct_dc += torch.sum(preds_val_dc == domain_labels_val_true.data)\n",
    "            val_total_dc += images_val.size(0)\n",
    "    \n",
    "    epoch_val_loss_dc = val_loss_dc_epoch / val_total_dc if val_total_dc > 0 else 0\n",
    "    epoch_val_acc_dc = val_correct_dc.double() / val_total_dc if val_total_dc > 0 else 0.0\n",
    "    print(f\"DC Epoch {epoch+1} Val Loss: {epoch_val_loss_dc:.4f}, Val Acc: {epoch_val_acc_dc:.4f}\")\n",
    "\n",
    "    if epoch_val_acc_dc > best_dc_val_acc:\n",
    "        best_dc_val_acc = epoch_val_acc_dc\n",
    "        torch.save(domain_classifier_head.state_dict(), dc_model_save_path)\n",
    "        print(f\"    -> New best DC Val Acc: {best_dc_val_acc:.4f}. DC Head saved to {dc_model_save_path}\")\n",
    "\n",
    "print(f\"\\nDomain Classifier training finished. Best Val Acc: {best_dc_val_acc:.4f}\")\n",
    "# Load the best DC head for inference\n",
    "if os.path.exists(dc_model_save_path):\n",
    "    domain_classifier_head.load_state_dict(torch.load(dc_model_save_path))\n",
    "    domain_classifier_head.eval()\n",
    "    print(\"Best Domain Classifier Head loaded and set to eval mode.\")\n",
    "else:\n",
    "    print(f\"Warning: Best DC Head model not found at {dc_model_save_path}. Using last state.\")\n",
    "    domain_classifier_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell X3: Robust Inference - TTA Definitions & Expert Population\n",
    "\n",
    "print(\"\\n\\n=== Part 6: Robust Inference Pipeline Setup & Evaluation ===\")\n",
    "print(\"\\n--- Defining Test-Time Augmentations and Populating Task Experts ---\")\n",
    "\n",
    "# --- 1. Test-Time Augmentation (TTA) Definitions ---\n",
    "# (Copy TTA definitions for tta_lite_transforms and tta_full_transforms_manual from your Cell 17 Updated)\n",
    "# Example:\n",
    "tta_lite_transforms = [\n",
    "    val_test_transform_weak, # Original (resized, normalized, maybe HFlip)\n",
    "    transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0), \n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "]\n",
    "# ... (define tta_full_transforms_manual similarly) ...\n",
    "tta_full_transforms_manual = [\n",
    "    val_test_transform_weak, \n",
    "    transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    transforms.Compose([transforms.CenterCrop(int(IMAGE_SIZE * 0.875)), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    transforms.Compose([transforms.RandomRotation(degrees=(-15, 15)), transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "]\n",
    "\n",
    "NUM_TTA_LITE_AUGMENTATIONS = len(tta_lite_transforms)\n",
    "NUM_TTA_FULL_AUGMENTATIONS = len(tta_full_transforms_manual)\n",
    "print(f\"TTA Lite augmentations: {NUM_TTA_LITE_AUGMENTATIONS}, Full TTA augmentations: {NUM_TTA_FULL_AUGMENTATIONS}\")\n",
    "\n",
    "# --- 2. Populate all_task_experts dictionary ---\n",
    "all_task_experts = {}\n",
    "print(\"\\nAttempting to populate all_task_experts...\")\n",
    "\n",
    "# 2.1. Add Source Expert (Art)\n",
    "if 'source_vit_lora' in globals() and source_vit_lora is not None and \\\n",
    "   'source_head' in globals() and source_head is not None:\n",
    "    try:\n",
    "        # Ensure they are on CPU if stored there, then move to DEVICE for inference\n",
    "        all_task_experts[SOURCE_DOMAIN_NAME] = {\n",
    "            'vit': copy.deepcopy(source_vit_lora).to(DEVICE).eval(),\n",
    "            'head': copy.deepcopy(source_head).to(DEVICE).eval()\n",
    "        }\n",
    "        print(f\"  Successfully added Source Expert: '{SOURCE_DOMAIN_NAME}'\")\n",
    "    except Exception as e: print(f\"  Error adding Source Expert '{SOURCE_DOMAIN_NAME}': {e}\")\n",
    "else: print(f\"  Warning: Source expert for '{SOURCE_DOMAIN_NAME}' not fully available. Skipping.\")\n",
    "\n",
    "# 2.2. Add Adapted Target Experts (from `adapted_experts` dictionary filled in Part 4)\n",
    "if 'adapted_experts' in globals() and isinstance(adapted_experts, dict) and adapted_experts:\n",
    "    print(f\"  Found adapted_experts with keys: {list(adapted_experts.keys())}\")\n",
    "    for domain_name_adapted, expert_models_adapted in adapted_experts.items():\n",
    "        if isinstance(expert_models_adapted, dict) and \\\n",
    "           'vit' in expert_models_adapted and expert_models_adapted['vit'] is not None and \\\n",
    "           'head' in expert_models_adapted and expert_models_adapted['head'] is not None:\n",
    "            try:\n",
    "                all_task_experts[domain_name_adapted] = {\n",
    "                    'vit': copy.deepcopy(expert_models_adapted['vit']).to(DEVICE).eval(), # Models in adapted_experts were on CPU\n",
    "                    'head': copy.deepcopy(expert_models_adapted['head']).to(DEVICE).eval()\n",
    "                }\n",
    "                print(f\"    Successfully added Adapted Expert: '{domain_name_adapted}'\")\n",
    "            except Exception as e: print(f\"    Error adding Adapted Expert '{domain_name_adapted}': {e}\")\n",
    "        else: print(f\"    Warning: Incomplete models for adapted expert '{domain_name_adapted}'. Skipping.\")\n",
    "else: print(\"  Warning: `adapted_experts` dictionary not found or empty. No adapted experts added.\")\n",
    "\n",
    "if not all_task_experts:\n",
    "    print(\"\\nCRITICAL WARNING: all_task_experts is EMPTY. Inference will fail.\")\n",
    "else:\n",
    "    print(f\"\\nSuccessfully populated all_task_experts with {len(all_task_experts)} expert(s): {list(all_task_experts.keys())}\")\n",
    "\n",
    "# Ensure base_vit_frozen_global and domain_classifier_head are ready\n",
    "if 'base_vit_frozen_global' not in globals() or base_vit_frozen_global is None:\n",
    "    print(\"CRITICAL WARNING: base_vit_frozen_global not found for inference pipeline.\")\n",
    "else: base_vit_frozen_global.to(DEVICE).eval()\n",
    "\n",
    "if 'domain_classifier_head' not in globals() or domain_classifier_head is None:\n",
    "    print(\"CRITICAL WARNING: domain_classifier_head not found for inference pipeline.\")\n",
    "else: domain_classifier_head.to(DEVICE).eval()\n",
    "\n",
    "print(\"Expert population and TTA definitions complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell X4: Robust Inference Pipeline Function Definition\n",
    "\n",
    "def robust_inference_pipeline(image_pil, base_vit, dc_head, task_experts_dict,\n",
    "                              domain_map_idx_to_name_dc, # Renamed for clarity\n",
    "                              num_total_classes=NUM_CLASSES, # Pass NUM_CLASSES\n",
    "                              tta_lite_transforms_list=tta_lite_transforms, # Use global TTA lists\n",
    "                              tta_full_transforms_list=tta_full_transforms_manual,\n",
    "                              domain_confidence_thresh=INFER_DOMAIN_CONF_THRESH,\n",
    "                              expert_confidence_thresh=INFER_EXPERT_CONF_THRESH,\n",
    "                              stage2_expert_confidence_thresh=INFER_STAGE2_EXPERT_CONF_THRESH,\n",
    "                              k_experts_for_avg=INFER_K_EXPERTS_FOR_AVG,\n",
    "                              device=DEVICE):\n",
    "    # Ensure models are in eval mode and on the correct device (mostly handled at population)\n",
    "    if base_vit is not None: base_vit.to(device).eval()\n",
    "    if dc_head is not None: dc_head.to(device).eval()\n",
    "\n",
    "    if not base_vit or not dc_head:\n",
    "        print(\"Error: Base ViT or Domain Classifier Head not provided to pipeline.\")\n",
    "        return None, -1.0\n",
    "    if not task_experts_dict:\n",
    "        print(\"Error: task_experts_dict is empty in pipeline.\")\n",
    "        return None, -1.0\n",
    "        \n",
    "    # Initial transform for Stage 1 (domain classification)\n",
    "    # Use the standard val_test_transform_weak for this initial, non-TTA step\n",
    "    initial_transformed_image = val_test_transform_weak(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast_ctx():\n",
    "        # Use g_function for base_vit to get CLS, then norm\n",
    "        base_all_features = base_vit.forward_features(initial_transformed_image)\n",
    "        base_cls_features = base_all_features[:, 0]\n",
    "        base_normed_cls_features = base_vit.norm(base_cls_features)\n",
    "        domain_logits = dc_head(base_normed_cls_features) # DC head expects normed features\n",
    "        domain_probs = F.softmax(domain_logits, dim=-1).squeeze(0) # Squeeze batch dim if B=1\n",
    "    \n",
    "    top_domain_prob, predicted_domain_idx_tensor = torch.max(domain_probs, dim=-1)\n",
    "    predicted_domain_idx = predicted_domain_idx_tensor.item()\n",
    "    predicted_domain_name = domain_map_idx_to_name_dc.get(predicted_domain_idx, f\"UnknownDomainIdx{predicted_domain_idx}\")\n",
    "\n",
    "    # Stage 1 Logic\n",
    "    selected_domain_names_for_next_stage = []\n",
    "    expert_weights_for_next_stage = torch.tensor([], device=device)\n",
    "\n",
    "    if top_domain_prob >= domain_confidence_thresh and predicted_domain_name in task_experts_dict:\n",
    "        expert_vit = task_experts_dict[predicted_domain_name]['vit']\n",
    "        expert_head = task_experts_dict[predicted_domain_name]['head']\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            # Use g_function and f_function for expert\n",
    "            expert_all_features = expert_vit.forward_features(initial_transformed_image)\n",
    "            expert_cls_features = expert_all_features[:,0]\n",
    "            expert_normed_cls_features = expert_vit.norm(expert_cls_features) # Expert ViT has its own norm\n",
    "            task_logits = expert_head(expert_normed_cls_features)\n",
    "            task_probs = F.softmax(task_logits, dim=-1).squeeze(0)\n",
    "        top_task_prob, final_label_idx_tensor = torch.max(task_probs, dim=-1)\n",
    "        if top_task_prob >= expert_confidence_thresh:\n",
    "            return final_label_idx_tensor.item(), top_task_prob.item()\n",
    "        else: # Fall through to Stage 2 with this single expert\n",
    "            selected_domain_names_for_next_stage = [predicted_domain_name]\n",
    "            expert_weights_for_next_stage = torch.tensor([1.0], device=device)\n",
    "    else: # Low domain confidence or no expert for top predicted domain\n",
    "        num_available_experts_in_map = len([name for name in domain_map_idx_to_name_dc.values() if name in task_experts_dict])\n",
    "        actual_k = min(k_experts_for_avg, num_available_experts_in_map)\n",
    "        if actual_k == 0: return None, -1.0 \n",
    "\n",
    "        top_k_domain_probs, top_k_domain_indices = torch.topk(domain_probs, actual_k, dim=-1)\n",
    "        \n",
    "        selected_domain_names_for_stage2_raw = [domain_map_idx_to_name_dc.get(idx.item(), f\"ErrDomain{idx.item()}\") for idx in top_k_domain_indices]\n",
    "        \n",
    "        valid_indices_for_stage2 = [i for i, name in enumerate(selected_domain_names_for_stage2_raw) if name in task_experts_dict]\n",
    "        if not valid_indices_for_stage2: return None, -1.0\n",
    "\n",
    "        selected_domain_names_for_next_stage = [selected_domain_names_for_stage2_raw[i] for i in valid_indices_for_stage2]\n",
    "        expert_weights_for_next_stage = top_k_domain_probs[valid_indices_for_stage2]\n",
    "        if expert_weights_for_next_stage.sum() > 1e-6 : \n",
    "            expert_weights_for_next_stage = expert_weights_for_next_stage / expert_weights_for_next_stage.sum()\n",
    "        else: \n",
    "             expert_weights_for_next_stage = torch.ones(len(selected_domain_names_for_next_stage), device=device) / max(1, len(selected_domain_names_for_next_stage))\n",
    "\n",
    "    # Stage 2 Logic (TTA-Lite)\n",
    "    aggregated_task_probs_stage2 = torch.zeros(num_total_classes, device=device)\n",
    "    num_tta_lite = len(tta_lite_transforms_list)\n",
    "    for tta_transform in tta_lite_transforms_list:\n",
    "        aug_image_tensor = tta_transform(image_pil).unsqueeze(0).to(device)\n",
    "        current_aug_weighted_probs = torch.zeros(num_total_classes, device=device)\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            for i, domain_name in enumerate(selected_domain_names_for_next_stage):\n",
    "                weight = expert_weights_for_next_stage[i]\n",
    "                expert_vit = task_experts_dict[domain_name]['vit']\n",
    "                expert_head = task_experts_dict[domain_name]['head']\n",
    "                \n",
    "                exp_all_feat_s2 = expert_vit.forward_features(aug_image_tensor)\n",
    "                exp_cls_feat_s2 = exp_all_feat_s2[:,0]\n",
    "                exp_norm_cls_feat_s2 = expert_vit.norm(exp_cls_feat_s2)\n",
    "                task_logits_expert_aug = expert_head(exp_norm_cls_feat_s2)\n",
    "                current_aug_weighted_probs += weight * F.softmax(task_logits_expert_aug.squeeze(0), dim=-1)\n",
    "        aggregated_task_probs_stage2 += current_aug_weighted_probs\n",
    "    \n",
    "    final_averaged_probs_stage2 = aggregated_task_probs_stage2 / max(1, num_tta_lite)\n",
    "    top_task_prob_stage2, final_label_idx_stage2_tensor = torch.max(final_averaged_probs_stage2, dim=-1)\n",
    "\n",
    "    if top_task_prob_stage2 >= stage2_expert_confidence_thresh:\n",
    "        return final_label_idx_stage2_tensor.item(), top_task_prob_stage2.item()\n",
    "    # else: Fall through to Stage 3 with the same selected experts and weights\n",
    "\n",
    "    # Stage 3 Logic (TTA-Full)\n",
    "    aggregated_task_probs_stage3 = torch.zeros(num_total_classes, device=device)\n",
    "    num_tta_full = len(tta_full_transforms_list)\n",
    "    for tta_transform_full in tta_full_transforms_list:\n",
    "        aug_image_tensor_full = tta_transform_full(image_pil).unsqueeze(0).to(device)\n",
    "        current_aug_weighted_probs_full = torch.zeros(num_total_classes, device=device)\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            for i, domain_name in enumerate(selected_domain_names_for_next_stage): # Same experts as stage 2\n",
    "                weight = expert_weights_for_next_stage[i] # Same weights as stage 2\n",
    "                expert_vit = task_experts_dict[domain_name]['vit']\n",
    "                expert_head = task_experts_dict[domain_name]['head']\n",
    "\n",
    "                exp_all_feat_s3 = expert_vit.forward_features(aug_image_tensor_full)\n",
    "                exp_cls_feat_s3 = exp_all_feat_s3[:,0]\n",
    "                exp_norm_cls_feat_s3 = expert_vit.norm(exp_cls_feat_s3)\n",
    "                task_logits_expert_full_aug = expert_head(exp_norm_cls_feat_s3)\n",
    "                current_aug_weighted_probs_full += weight * F.softmax(task_logits_expert_full_aug.squeeze(0), dim=-1)\n",
    "        aggregated_task_probs_stage3 += current_aug_weighted_probs_full\n",
    "    \n",
    "    ultimate_final_probs = aggregated_task_probs_stage3 / max(1, num_tta_full)\n",
    "    top_task_prob_stage3, ultimate_final_label_idx_tensor = torch.max(ultimate_final_probs, dim=-1)\n",
    "    \n",
    "    return ultimate_final_label_idx_tensor.item(), top_task_prob_stage3.item()\n",
    "\n",
    "print(\"Robust inference pipeline function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T11:27:27.714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell X5: Evaluate Robust Inference Pipeline on Combined Validation Set\n",
    "\n",
    "print(\"\\n--- Evaluating Robust Inference Pipeline on Combined Validation Set ---\")\n",
    "\n",
    "# Check prerequisites\n",
    "prereq_missing = False\n",
    "if 'domain_classifier_head' not in globals() or domain_classifier_head is None:\n",
    "    print(\"ERROR: Domain Classifier Head not trained/loaded.\")\n",
    "    prereq_missing = True\n",
    "if 'all_task_experts' not in globals() or not all_task_experts:\n",
    "    print(\"ERROR: No task experts available.\")\n",
    "    prereq_missing = True\n",
    "if 'base_vit_frozen_global' not in globals() or base_vit_frozen_global is None:\n",
    "    print(\"ERROR: Base ViT backbone not loaded.\")\n",
    "    prereq_missing = True\n",
    "if 'GLOBAL_CLASS_TO_IDX' not in globals() or GLOBAL_CLASS_TO_IDX is None:\n",
    "    print(\"ERROR: GLOBAL_CLASS_TO_IDX not defined.\")\n",
    "    prereq_missing = True\n",
    "if 'domain_idx_to_name_dc' not in globals() or domain_idx_to_name_dc is None: # Check for DC specific map\n",
    "    print(\"ERROR: domain_idx_to_name_dc mapping not defined (from DC training).\")\n",
    "    prereq_missing = True\n",
    "\n",
    "if prereq_missing:\n",
    "    print(\"Skipping robust inference pipeline evaluation due to missing prerequisites.\")\n",
    "else:\n",
    "    # --- 1. Prepare Combined Validation Dataset ---\n",
    "    class CombinedValDatasetForRobustEval(Dataset): # Renamed for clarity\n",
    "        def __init__(self, root_dir, all_domain_names_list, \n",
    "                     class_to_idx_map_overall, split_type='val'):\n",
    "            self.images_pil = [] \n",
    "            self.true_class_labels = []\n",
    "            self.original_domain_names = [] \n",
    "\n",
    "            for domain_name_iter in all_domain_names_list:\n",
    "                try:\n",
    "                    # Use OfficeHomeDomainDataset to get image paths and class labels\n",
    "                    domain_val_dataset_temp = OfficeHomeDomainDataset(\n",
    "                        root_dir=root_dir, domain_name=domain_name_iter,\n",
    "                        transform=None, # Load PIL\n",
    "                        split_type=split_type, \n",
    "                        class_to_idx_mapping=class_to_idx_map_overall,\n",
    "                        load_pil=True # Get PIL images\n",
    "                    )\n",
    "                    \n",
    "                    for i in range(len(domain_val_dataset_temp)):\n",
    "                        pil_img, class_lbl = domain_val_dataset_temp[i]\n",
    "                        self.images_pil.append(pil_img)\n",
    "                        self.true_class_labels.append(class_lbl) # Already an int\n",
    "                        self.original_domain_names.append(domain_name_iter)\n",
    "                        \n",
    "                    print(f\"  Added {len(domain_val_dataset_temp)} validation images from '{domain_name_iter}' for robust eval.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Error loading val data for '{domain_name_iter}' (robust eval): {e}. Skipping.\")\n",
    "            \n",
    "            if not self.images_pil:\n",
    "                raise RuntimeError(\"No validation images found for combined robust evaluation.\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images_pil)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Domain label for DC is not needed here as pipeline handles it internally\n",
    "            return (self.images_pil[idx], \n",
    "                    torch.tensor(self.true_class_labels[idx]).long(),\n",
    "                    self.original_domain_names[idx])\n",
    "    \n",
    "    try:\n",
    "        combined_val_dataset_robust = CombinedValDatasetForRobustEval(\n",
    "            root_dir=DATA_DIR,\n",
    "            all_domain_names_list=ALL_TRAINABLE_DOMAIN_NAMES,\n",
    "            class_to_idx_map_overall=GLOBAL_CLASS_TO_IDX,\n",
    "            split_type='val'\n",
    "        )\n",
    "        print(f\"Total combined validation images for robust eval: {len(combined_val_dataset_robust)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error creating combined validation dataset for robust eval: {e}. Aborting.\")\n",
    "        combined_val_dataset_robust = None\n",
    "    \n",
    "    # --- 2. Run Inference ---\n",
    "    if combined_val_dataset_robust and len(combined_val_dataset_robust) > 0:\n",
    "        all_predictions_robust = []\n",
    "        all_true_labels_robust = []\n",
    "        all_original_domains_robust = []\n",
    "        \n",
    "        for i in tqdm(range(len(combined_val_dataset_robust)), desc=\"Robust Pipeline Eval\"):\n",
    "            pil_image, true_class_label, original_domain = combined_val_dataset_robust[i]\n",
    "            \n",
    "            predicted_label_idx, confidence = robust_inference_pipeline(\n",
    "                image_pil=pil_image,\n",
    "                base_vit=base_vit_frozen_global, # Original frozen ViT\n",
    "                dc_head=domain_classifier_head,\n",
    "                task_experts_dict=all_task_experts,\n",
    "                domain_map_idx_to_name_dc=domain_idx_to_name_dc, # From DC training\n",
    "                device=DEVICE\n",
    "            )\n",
    "            all_predictions_robust.append(predicted_label_idx if predicted_label_idx is not None else -1)\n",
    "            all_true_labels_robust.append(true_class_label.item())\n",
    "            all_original_domains_robust.append(original_domain)\n",
    "\n",
    "        # --- 3. Calculate and Report Accuracies ---\n",
    "        print(\"\\n--- Robust Inference Pipeline Evaluation Results ---\")\n",
    "        overall_correct_robust = 0\n",
    "        overall_total_robust = 0\n",
    "        domain_wise_stats_robust = {name: {'correct': 0, 'total': 0} for name in ALL_TRAINABLE_DOMAIN_NAMES}\n",
    "\n",
    "        for i in range(len(all_predictions_robust)):\n",
    "            pred_idx, true_idx, domain_name = all_predictions_robust[i], all_true_labels_robust[i], all_original_domains_robust[i]\n",
    "            if pred_idx != -1:\n",
    "                overall_total_robust += 1\n",
    "                domain_wise_stats_robust[domain_name]['total'] += 1\n",
    "                if pred_idx == true_idx:\n",
    "                    overall_correct_robust += 1\n",
    "                    domain_wise_stats_robust[domain_name]['correct'] += 1\n",
    "        \n",
    "        overall_accuracy_robust = (overall_correct_robust / overall_total_robust * 100) if overall_total_robust > 0 else 0.0\n",
    "        print(f\"Overall Accuracy (Robust Pipeline): {overall_accuracy_robust:.2f}% ({overall_correct_robust}/{overall_total_robust})\")\n",
    "        print(\"\\nDomain-wise Accuracies (Robust Pipeline):\")\n",
    "        for domain_name, stats in domain_wise_stats_robust.items():\n",
    "            acc = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0.0\n",
    "            print(f\"  Domain '{domain_name}': {acc:.2f}% ({stats['correct']}/{stats['total']})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7321032,
     "sourceId": 11686886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
