{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "!pip install timm scikit-learn tqdm matplotlib Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"/kaggle/input/officehome/OfficeHome\"\n",
    "VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LORA_RANK = 16\n",
    "NUM_CLASSES = 65\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, running on CPU. This will be very slow for later phases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "class OfficeHomeDomainDataset(Dataset):\n",
    "    def __init__(self, root_dir, domain_name, transform=None, split_ratios=(0.8, 0.1, 0.1), split_type='train', random_seed=42):\n",
    "        self.domain_path = os.path.join(root_dir, domain_name)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        idx = 0\n",
    "        for class_name in sorted(os.listdir(self.domain_path)):\n",
    "            if class_name not in self.class_to_idx:\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                self.idx_to_class[idx] = class_name\n",
    "                idx += 1\n",
    "            \n",
    "            class_path = os.path.join(self.domain_path, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            \n",
    "            domain_class_images = []\n",
    "            for img_name in sorted(os.listdir(class_path)):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    domain_class_images.append(os.path.join(class_path, img_name))\n",
    "            \n",
    "            # Split data for the current class\n",
    "            np.random.seed(random_seed) # for reproducible splits\n",
    "            np.random.shuffle(domain_class_images)\n",
    "            \n",
    "            n_total = len(domain_class_images)\n",
    "            n_train = int(n_total * split_ratios[0])\n",
    "            n_val = int(n_total * split_ratios[1])\n",
    "            # n_test is the rest\n",
    "\n",
    "            if split_type == 'train':\n",
    "                selected_images = domain_class_images[:n_train]\n",
    "            elif split_type == 'val':\n",
    "                selected_images = domain_class_images[n_train : n_train + n_val]\n",
    "            elif split_type == 'test':\n",
    "                selected_images = domain_class_images[n_train + n_val:]\n",
    "            else:\n",
    "                raise ValueError(\"split_type must be 'train', 'val', or 'test'\")\n",
    "\n",
    "            self.images.extend(selected_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(selected_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "try:\n",
    "    art_train_dataset = OfficeHomeDomainDataset(DATA_DIR, 'Art', transform=train_transform, split_type='train')\n",
    "    art_val_dataset = OfficeHomeDomainDataset(DATA_DIR, 'Art', transform=val_test_transform, split_type='val')\n",
    "\n",
    "    if len(art_train_dataset) > 0 and len(art_val_dataset) > 0:\n",
    "        art_train_loader = DataLoader(art_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        art_val_loader = DataLoader(art_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        print(f\"Art train dataset size: {len(art_train_dataset)}\")\n",
    "        print(f\"Art val dataset size: {len(art_val_dataset)}\")\n",
    "        print(f\"Number of classes in Art domain: {len(art_train_dataset.class_to_idx)}\")\n",
    "\n",
    "        # Check a sample batch\n",
    "        sample_images, sample_labels = next(iter(art_train_loader))\n",
    "        print(f\"Sample batch - images shape: {sample_images.shape}, labels shape: {sample_labels.shape}\")\n",
    "        assert sample_images.shape == (min(BATCH_SIZE, len(art_train_dataset)), 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        assert sample_labels.shape == (min(BATCH_SIZE, len(art_train_dataset)),)\n",
    "        print(\"Dataset and DataLoader for 'Art' domain seem OK.\")\n",
    "    else:\n",
    "        print(\"Warning: 'Art' dataset is empty. Check DATA_DIR and domain name.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {DATA_DIR}. Please ensure OfficeHome is downloaded and extracted there.\")\n",
    "    print(\"You might need to create dummy folders if you want to proceed without data for now.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during dataset loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Load ViT-Base/16\n",
    "vit_backbone = timm.create_model(VIT_MODEL_NAME, pretrained=True, num_classes=0) # num_classes=0 removes the original head\n",
    "vit_backbone = vit_backbone.to(DEVICE)\n",
    "\n",
    "# Freeze all parameters of the base ViT model\n",
    "for param in vit_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Loaded ViT backbone: {VIT_MODEL_NAME}\")\n",
    "total_params_vit = sum(p.numel() for p in vit_backbone.parameters())\n",
    "trainable_params_vit = sum(p.numel() for p in vit_backbone.parameters() if p.requires_grad)\n",
    "print(f\"Total ViT params: {total_params_vit:,}\")\n",
    "print(f\"Trainable ViT params: {trainable_params_vit:,}\") # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "\n",
    "        # Original weight and bias (frozen)\n",
    "        self.weight = nn.Parameter(linear_layer.weight.detach().clone(), requires_grad=False)\n",
    "        if linear_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(linear_layer.bias.detach().clone(), requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # LoRA matrices A and B\n",
    "        self.lora_A = nn.Parameter(torch.zeros(self.rank, self.in_features)) # Shape: (rank, in_features)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.out_features, self.rank)) # Shape: (out_features, rank)\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5)) \n",
    "        nn.init.zeros_(self.lora_B) # B is initialized to zero\n",
    "        \n",
    "        self.scaling = 1.0 / self.rank # Alpha is implicitly 1 here, scaling is 1/rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_original = nn.functional.linear(x, self.weight, self.bias)\n",
    "        lora_adaptation = (x @ self.lora_A.t()) @ self.lora_B.t()\n",
    "        \n",
    "        return out_original + lora_adaptation * self.scaling\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, rank={self.rank}'\n",
    "\n",
    "# Helper function to inject LoRA\n",
    "def inject_lora_to_vit_attention(vit_model, rank):\n",
    "    injected_count = 0\n",
    "    # First, freeze all parameters of the incoming vit_model\n",
    "    for param in vit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    for block_idx, block in enumerate(vit_model.blocks):\n",
    "        qkv_layer = block.attn.qkv\n",
    "        if isinstance(qkv_layer, nn.Linear):\n",
    "\n",
    "            block.attn.qkv = LoRALinear(qkv_layer, rank)\n",
    "            injected_count += 1\n",
    "            \n",
    "    if injected_count == 0:\n",
    "        print(\"WARNING: No QKV layers found or replaced with LoRA. Check ViT model structure.\")\n",
    "    else:\n",
    "        print(f\"Injected LoRA (rank={rank}) into {injected_count} QKV layers in ViT attention blocks.\")\n",
    "    return vit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "class DomainSpecificHead(nn.Module):\n",
    "    def __init__(self, in_features=768, num_classes=NUM_CLASSES): # NUM_CLASSES should be defined\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "# --- Phase 1 Specific Configurations ---\n",
    "ART_EPOCHS = 10 # As per plan\n",
    "ART_LR = 1e-4\n",
    "ART_EMBED_DIM = 768 # ViT-Base feature dimension\n",
    "\n",
    "# For saving models\n",
    "MODEL_SAVE_DIR = \"/kaggle/working/\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize GradScaler for AMP\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "print(\"Phase 1: Configurations set.\")\n",
    "print(f\"Art LoRA+Head training epochs: {ART_EPOCHS}, LR: {ART_LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# 1. Make a trainable copy of the backbone and inject Art-LoRA\n",
    "# The main `vit_backbone` from Phase 0 is frozen. We need a copy to inject LoRA into for this domain.\n",
    "art_vit_lora = copy.deepcopy(vit_backbone) # Deepcopy to not affect the original frozen backbone\n",
    "art_vit_lora = inject_lora_to_vit_attention(art_vit_lora, rank=LORA_RANK) # inject_lora also freezes base model\n",
    "art_vit_lora = art_vit_lora.to(DEVICE)\n",
    "\n",
    "# 2. Instantiate Art-Head\n",
    "art_head = DomainSpecificHead(in_features=ART_EMBED_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# 3. Define Optimizer for Art-LoRA parameters and Art-Head parameters\n",
    "# Collect only trainable parameters (LoRA A/B from art_vit_lora, and all from art_head)\n",
    "params_to_train = []\n",
    "for param in art_vit_lora.parameters():\n",
    "    if param.requires_grad:\n",
    "        params_to_train.append(param)\n",
    "for param in art_head.parameters():\n",
    "    if param.requires_grad: # Should be all head params\n",
    "        params_to_train.append(param)\n",
    "\n",
    "optimizer_art = optim.AdamW(params_to_train, lr=ART_LR)\n",
    "criterion_art = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Number of trainable parameters for Art expert: {sum(p.numel() for p in params_to_train):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# --- Training Loop ---\n",
    "print(\"\\n--- Training Art Expert (Art-LoRA + Art-Head) ---\")\n",
    "for epoch in range(ART_EPOCHS):\n",
    "    art_vit_lora.train()\n",
    "    art_head.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(art_train_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [Art Train]\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer_art.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            features = art_vit_lora(images)\n",
    "            # Assuming features[:, 0] is the CLS token if features.ndim == 3\n",
    "            cls_features = features[:, 0] if features.ndim == 3 else features\n",
    "            logits = art_head(cls_features)\n",
    "            loss = criterion_art(logits, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer_art)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += images.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions.double().item()/total_samples if total_samples > 0 else 0.0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    art_vit_lora.eval()\n",
    "    art_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(art_val_loader, desc=f\"Epoch {epoch+1}/{ART_EPOCHS} [Art Val]\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "                features = art_vit_lora(images)\n",
    "                cls_features = features[:, 0] if features.ndim == 3 else features\n",
    "                logits = art_head(cls_features)\n",
    "                loss = criterion_art(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            val_correct_predictions += torch.sum(preds == labels.data)\n",
    "            val_total_samples += images.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_loss / val_total_samples\n",
    "    epoch_val_acc = val_correct_predictions.double() / val_total_samples\n",
    "    print(f\"Epoch {epoch+1} Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Save the trained Art-LoRA (only LoRA params) and Art-Head\n",
    "art_lora_state_dict = {name: param for name, param in art_vit_lora.named_parameters() if 'lora_' in name and param.requires_grad}\n",
    "torch.save(art_lora_state_dict, os.path.join(MODEL_SAVE_DIR, \"art_lora.pth\"))\n",
    "torch.save(art_head.state_dict(), os.path.join(MODEL_SAVE_DIR, \"art_head.pth\"))\n",
    "print(\"Art-LoRA and Art-Head models saved.\")\n",
    "\n",
    "# --- Sanity Check: Art Expert Training ---\n",
    "assert epoch_val_acc > 0.1, \"Validation accuracy for Art is too low. Training might have failed.\" # Basic check\n",
    "print(\"Art expert training sanity check passed (accuracy > 0.1).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "\n",
    "print(\"\\n--- Baseline Validation of Art Expert on All Domains ---\")\n",
    "\n",
    "# --- 1. Load the Trained Art Expert ---\n",
    "\n",
    "# Make sure models are in evaluation mode\n",
    "art_vit_lora.eval()\n",
    "art_head.eval()\n",
    "\n",
    "# --- 2. Define Domain Names and Prepare DataLoaders ---\n",
    "domain_names_all = ['Art', 'Clipart', 'Product', 'RealWorld']\n",
    "baseline_accuracies = {}\n",
    "\n",
    "# For amp autocasting, ensure autocast_ctx is defined, or define it:\n",
    "if 'autocast_ctx' not in globals():\n",
    "    autocast_ctx = lambda: torch.amp.autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "# Criterion for validation (should match what was used in training)\n",
    "if 'criterion_art' not in globals(): # If not available from previous cell\n",
    "    criterion_val = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_val = criterion_art\n",
    "\n",
    "\n",
    "# --- 3. Iterate Through Domains and Evaluate ---\n",
    "for domain_name in domain_names_all:\n",
    "    print(f\"\\nValidating Art Expert on {domain_name} domain...\")\n",
    "\n",
    "    # Load validation dataset for the current domain\n",
    "    # Ensure val_test_transform is defined (from Phase 0)\n",
    "    try:\n",
    "        val_dataset_current_domain = OfficeHomeDomainDataset(\n",
    "            DATA_DIR,\n",
    "            domain_name,\n",
    "            transform=val_test_transform, # Use the standard validation/test transform\n",
    "            split_type='val' # Use the 10% validation split\n",
    "        )\n",
    "        if len(val_dataset_current_domain) == 0:\n",
    "            print(f\"Warning: Validation dataset for {domain_name} is empty. Skipping.\")\n",
    "            baseline_accuracies[domain_name] = 0.0\n",
    "            continue\n",
    "\n",
    "        val_loader_current_domain = DataLoader(\n",
    "            val_dataset_current_domain,\n",
    "            batch_size=BATCH_SIZE, # Use the global BATCH_SIZE\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Dataset for domain '{domain_name}' not found at {DATA_DIR}. Skipping.\")\n",
    "        baseline_accuracies[domain_name] = -1.0 # Indicate error\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading dataset for {domain_name}: {e}. Skipping.\")\n",
    "        baseline_accuracies[domain_name] = -1.0 # Indicate error\n",
    "        continue\n",
    "\n",
    "\n",
    "    val_loss_domain = 0.0\n",
    "    val_correct_predictions_domain = 0\n",
    "    val_total_samples_domain = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_current_domain, desc=f\"Val on {domain_name}\", leave=False):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            with autocast_ctx():\n",
    "                # Pass images through the Art-LoRA ViT\n",
    "                features = art_vit_lora(images)\n",
    "                cls_features = features[:, 0] if features.ndim == 3 else features\n",
    "                # Then through the Art-Head\n",
    "                logits = art_head(cls_features)\n",
    "                loss = criterion_val(logits, labels)\n",
    "\n",
    "            val_loss_domain += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            val_correct_predictions_domain += torch.sum(preds == labels.data)\n",
    "            val_total_samples_domain += images.size(0)\n",
    "\n",
    "    if val_total_samples_domain > 0:\n",
    "        epoch_val_loss_domain = val_loss_domain / val_total_samples_domain\n",
    "        epoch_val_acc_domain = val_correct_predictions_domain.double() / val_total_samples_domain\n",
    "        baseline_accuracies[domain_name] = epoch_val_acc_domain.item()\n",
    "        print(f\"{domain_name} Val Loss: {epoch_val_loss_domain:.4f}, Val Acc: {epoch_val_acc_domain:.4f}\")\n",
    "    else:\n",
    "        print(f\"No samples processed for {domain_name} validation.\")\n",
    "        baseline_accuracies[domain_name] = 0.0\n",
    "\n",
    "\n",
    "# --- 4. Print Summary of Baseline Accuracies ---\n",
    "print(\"\\n--- Baseline Art Expert Performance Summary ---\")\n",
    "for domain, acc in baseline_accuracies.items():\n",
    "    if acc != -1.0: # Check for loading errors\n",
    "        print(f\"Accuracy on {domain}: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Accuracy on {domain}: ERROR (Dataset not found or loading issue)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt to new domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# --- Phase 2 Specific Configurations for 'CLIPART' ---\n",
    "TARGET_DOMAIN_NAME = 'Clipart'\n",
    "PREV_DOMAIN_NAMES = ['Art'] # Keep track of all previous domains\n",
    "\n",
    "ADAPT_EPOCHS = 10\n",
    "ADAPT_LR_LORA = 1e-4\n",
    "ADAPT_LR_HEAD = 1e-4\n",
    "ADAPT_LR_LN = 1e-5 # Learning rate for LayerNorm affine parameters\n",
    "\n",
    "EMA_DECAY = 0.999\n",
    "PSEUDO_LABEL_START_THRESHOLD = 0.6\n",
    "PSEUDO_LABEL_END_THRESHOLD = 0.8 # Reaches this at the last epoch\n",
    "\n",
    "# Make a subdirectory for CLIPART models within the main model save dir\n",
    "CLIPART_MODEL_SAVE_DIR = os.path.join(MODEL_SAVE_DIR, TARGET_DOMAIN_NAME)\n",
    "os.makedirs(CLIPART_MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# For AMP\n",
    "# scaler should be globally defined from Phase 0/1, if not:\n",
    "if 'scaler' not in globals():\n",
    "    # scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda')) # Old\n",
    "    scaler = torch.amp.GradScaler(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "# --- 1. Load Frozen ViT Backbone ---\n",
    "# vit_backbone should be available from Phase 0 (the completely frozen one)\n",
    "base_vit_frozen = copy.deepcopy(vit_backbone)\n",
    "base_vit_frozen.eval() # Ensure it's in eval mode\n",
    "\n",
    "# --- Helper function to make LayerNorm affine parameters trainable ---\n",
    "# (Keep the function definition as it was)\n",
    "def set_layernorm_affine_trainable(model):\n",
    "    for name, mod in model.named_modules():\n",
    "        if isinstance(mod, nn.LayerNorm):\n",
    "            if hasattr(mod, 'weight') and mod.weight is not None:\n",
    "                mod.weight.requires_grad = True\n",
    "            if hasattr(mod, 'bias') and mod.bias is not None:\n",
    "                mod.bias.requires_grad = True\n",
    "    print(\"Made LayerNorm affine parameters trainable.\") # Added print statement\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# --- 2. Load Art Expert (Frozen) ---\n",
    "# # THIS CELL IS NOW NEEDED TO GET INITIAL WEIGHTS FOR THE STUDENT/TEACHER\n",
    "\n",
    "art_lora_state_dict_path = os.path.join(MODEL_SAVE_DIR, \"art_lora.pth\")\n",
    "art_head_state_dict_path = os.path.join(MODEL_SAVE_DIR, \"art_head.pth\")\n",
    "\n",
    "# Load the Art LoRA parameters\n",
    "try:\n",
    "    # Use weights_only=True for safety if the source is trusted, otherwise handle potential risks\n",
    "    art_lora_trained_weights = torch.load(art_lora_state_dict_path, map_location=DEVICE, weights_only=True)\n",
    "    print(f\"Loaded Art LoRA weights dictionary with keys: {list(art_lora_trained_weights.keys())[:5]}...\") # Print first few keys\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Art LoRA weights not found at {art_lora_state_dict_path}. Cannot initialize.\")\n",
    "    art_lora_trained_weights = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Art LoRA weights: {e}\")\n",
    "    art_lora_trained_weights = None\n",
    "\n",
    "\n",
    "# Load the Art Head parameters\n",
    "try:\n",
    "    # Use weights_only=True for safety\n",
    "    art_head_trained_weights = torch.load(art_head_state_dict_path, map_location=DEVICE, weights_only=True)\n",
    "    print(f\"Loaded Art Head weights dictionary.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Art Head weights not found at {art_head_state_dict_path}. Cannot initialize.\")\n",
    "    art_head_trained_weights = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Art Head weights: {e}\")\n",
    "    art_head_trained_weights = None\n",
    "\n",
    "# We don't need to create separate expert models here, just hold the state dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "# --- 3. Initialize CLIPART Student Model (Trainable LoRA and LayerNorm affines) ---\n",
    "# This is the model whose parameters we will optimize.\n",
    "student_vit_CLIPART = copy.deepcopy(base_vit_frozen)\n",
    "student_vit_CLIPART = inject_lora_to_vit_attention(student_vit_CLIPART, rank=LORA_RANK) # Injects *new* CLIPART LoRA\n",
    "# Don't make LN trainable yet, happens after loading weights if needed.\n",
    "student_vit_CLIPART = student_vit_CLIPART.to(DEVICE)\n",
    "\n",
    "student_head_CLIPART = DomainSpecificHead(in_features=ART_EMBED_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "# student_head_CLIPART parameters are trainable by default\n",
    "\n",
    "print(f\"Initialized STUDENT model structure for {TARGET_DOMAIN_NAME}.\")\n",
    "\n",
    "# --- Load Art weights into STUDENT ---\n",
    "weights_loaded = False\n",
    "if art_lora_trained_weights is not None and art_head_trained_weights is not None:\n",
    "    try:\n",
    "        # Load LoRA weights into the student ViT. `strict=False` is important\n",
    "        # as the student ViT has base weights too, which are not in the LoRA dict.\n",
    "        missing_keys, unexpected_keys = student_vit_CLIPART.load_state_dict(art_lora_trained_weights, strict=False)\n",
    "        print(f\"Loaded Art LoRA into student ViT. Missing keys: {len(missing_keys)}, Unexpected keys: {len(unexpected_keys)}\")\n",
    "        if not any('lora' in k for k in student_vit_CLIPART.state_dict() if k not in missing_keys):\n",
    "             print(\"Warning: It seems no LoRA weights were actually loaded.\")\n",
    "\n",
    "        # Load head weights\n",
    "        student_head_CLIPART.load_state_dict(art_head_trained_weights, strict=True)\n",
    "        print(\"Loaded Art Head weights into student Head.\")\n",
    "        weights_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading Art weights into student: {e}\")\n",
    "else:\n",
    "    print(\"Skipping weight initialization from Art due to previous loading errors.\")\n",
    "\n",
    "# --- Ensure required parameters are trainable AFTER loading ---\n",
    "student_vit_CLIPART = set_layernorm_affine_trainable(student_vit_CLIPART) # Make LN affine trainable *now*\n",
    "# Double-check LoRA parameters are trainable (should be by default from inject_lora)\n",
    "lora_trainable_count = 0\n",
    "for name, param in student_vit_CLIPART.named_parameters():\n",
    "    if 'lora_' in name:\n",
    "        param.requires_grad = True\n",
    "        lora_trainable_count += 1\n",
    "print(f\"Ensured {lora_trainable_count} LoRA parameters in student ViT are trainable.\")\n",
    "\n",
    "if weights_loaded:\n",
    "    print(f\"Initialized STUDENT model weights for {TARGET_DOMAIN_NAME} from Art expert.\")\n",
    "else:\n",
    "    print(f\"Proceeding with STUDENT model for {TARGET_DOMAIN_NAME} without Art initialization.\")\n",
    "\n",
    "\n",
    "# --- Report Trainable Params ---\n",
    "trainable_params_student_vit = sum(p.numel() for p in student_vit_CLIPART.parameters() if p.requires_grad)\n",
    "trainable_params_student_head = sum(p.numel() for p in student_head_CLIPART.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable params in student_vit_{TARGET_DOMAIN_NAME}: {trainable_params_student_vit:,}\")\n",
    "print(f\"Trainable params in student_head_{TARGET_DOMAIN_NAME}: {trainable_params_student_head:,}\")\n",
    "print(f\"Total trainable STUDENT params: {trainable_params_student_vit + trainable_params_student_head:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "# --- 4. Initialize EMA Teacher Model for CLIPART ---\n",
    "# This model provides pseudo-labels and is updated via EMA.\n",
    "# It STARTS as a copy of the (potentially Art-initialized) student.\n",
    "teacher_vit_CLIPART = copy.deepcopy(student_vit_CLIPART)\n",
    "teacher_head_CLIPART = copy.deepcopy(student_head_CLIPART)\n",
    "\n",
    "# Freeze teacher parameters\n",
    "for param in teacher_vit_CLIPART.parameters(): param.requires_grad = False\n",
    "for param in teacher_head_CLIPART.parameters(): param.requires_grad = False\n",
    "\n",
    "teacher_vit_CLIPART = teacher_vit_CLIPART.to(DEVICE).eval() # Set to eval mode\n",
    "teacher_head_CLIPART = teacher_head_CLIPART.to(DEVICE).eval()\n",
    "print(f\"Initialized EMA TEACHER for {TARGET_DOMAIN_NAME} (from student's initial state).\")\n",
    "\n",
    "# --- EMA Update Function (Keep as is) ---\n",
    "def update_ema_teacher(student_vit, student_head, teacher_vit, teacher_head, decay):\n",
    "    # ... (function definition remains the same) ...\n",
    "    with torch.no_grad():\n",
    "        # Update ViT (LoRA and LayerNorm affine parameters)\n",
    "        student_params = {name: param for name, param in student_vit.named_parameters() if param.requires_grad}\n",
    "        teacher_params = dict(teacher_vit.named_parameters())\n",
    "\n",
    "        for name, stud_param in student_params.items():\n",
    "            if name in teacher_params:\n",
    "                teach_param = teacher_params[name]\n",
    "                # EMA update: teacher = decay * teacher + (1 - decay) * student\n",
    "                teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "            # else:\n",
    "                # print(f\"Warning: Param {name} from student_vit not found in teacher_vit for EMA update.\")\n",
    "\n",
    "        # Update Head (all parameters)\n",
    "        for stud_param, teach_param in zip(student_head.parameters(), teacher_head.parameters()):\n",
    "            teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "# Perform an initial update (makes teacher parameters exactly match student initially)\n",
    "update_ema_teacher(student_vit_CLIPART, student_head_CLIPART, teacher_vit_CLIPART, teacher_head_CLIPART, 0.0) # Decay=0 means teacher = student\n",
    "print(\"Initial EMA update complete (Teacher = Student).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "# --- 5. Initialize EMA Teacher Model for CLIPART ---\n",
    "teacher_vit_CLIPART = copy.deepcopy(student_vit_CLIPART) # Has CLIPART LoRA and LN structure\n",
    "teacher_head_CLIPART = copy.deepcopy(student_head_CLIPART)\n",
    "\n",
    "for param in teacher_vit_CLIPART.parameters(): param.requires_grad = False\n",
    "for param in teacher_head_CLIPART.parameters(): param.requires_grad = False\n",
    "teacher_vit_CLIPART.eval()\n",
    "teacher_head_CLIPART.eval()\n",
    "print(\"Initialized EMA teacher for CLIPART.\")\n",
    "\n",
    "# --- EMA Update Function (Corrected) ---\n",
    "def update_ema_teacher(student_vit, student_head, teacher_vit, teacher_head, decay):\n",
    "    with torch.no_grad():\n",
    "        # Update ViT (LoRA and LayerNorm affine parameters)\n",
    "        student_trainable_vit_params = {name: param for name, param in student_vit.named_parameters() if param.requires_grad}\n",
    "        teacher_vit_params_dict = dict(teacher_vit.named_parameters())\n",
    "\n",
    "        for name, stud_param in student_trainable_vit_params.items():\n",
    "            if name in teacher_vit_params_dict:\n",
    "                teach_param = teacher_vit_params_dict[name]\n",
    "                teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "            else:\n",
    "                print(f\"Warning: Param {name} from student_vit not found in teacher_vit for EMA update.\")\n",
    "\n",
    "        # Update Head\n",
    "        for stud_param, teach_param in zip(student_head.parameters(), teacher_head.parameters()):\n",
    "            teach_param.data.mul_(decay).add_(stud_param.data, alpha=1 - decay)\n",
    "\n",
    "# Initial EMA update (student and teacher are identical at this point, but good practice)\n",
    "update_ema_teacher(student_vit_CLIPART, student_head_CLIPART, teacher_vit_CLIPART, teacher_head_CLIPART, EMA_DECAY)\n",
    "print(\"Initial EMA update complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "# Load CLIPART dataset (target domain, unlabeled for pseudo-labeling)\n",
    "CLIPART_train_dataset_unlabeled = OfficeHomeDomainDataset(DATA_DIR, TARGET_DOMAIN_NAME, transform=train_transform, split_type='train')\n",
    "CLIPART_val_dataset = OfficeHomeDomainDataset(DATA_DIR, TARGET_DOMAIN_NAME, transform=val_test_transform, split_type='val')\n",
    "\n",
    "# Loader for generating pseudo-labels from the *entire* training set each epoch\n",
    "CLIPART_train_loader_unlabeled_images = DataLoader(CLIPART_train_dataset_unlabeled, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2, pin_memory=True) # Use larger batch for generation, no shuffle needed\n",
    "# Loader for validation\n",
    "CLIPART_val_loader = DataLoader(CLIPART_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"{TARGET_DOMAIN_NAME} (target) unlabeled train dataset size: {len(CLIPART_train_dataset_unlabeled)}\")\n",
    "print(f\"{TARGET_DOMAIN_NAME} (target) val dataset size: {len(CLIPART_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "# --- Optimizer for CLIPART Adaptation (Student Only) ---\n",
    "# Group parameters for different LRs (LoRA, LayerNorm, Head)\n",
    "params_to_train_student = []\n",
    "# Group 1: LoRA parameters\n",
    "params_to_train_student.append({\n",
    "    'params': [p for name, p in student_vit_CLIPART.named_parameters() if 'lora_' in name and p.requires_grad],\n",
    "    'lr': ADAPT_LR_LORA\n",
    "})\n",
    "# Group 2: LayerNorm affine parameters\n",
    "params_to_train_student.append({\n",
    "    'params': [p for name, p in student_vit_CLIPART.named_parameters() if ('norm' in name.lower() or 'layernorm' in name.lower()) and p.requires_grad],\n",
    "    'lr': ADAPT_LR_LN\n",
    "})\n",
    "# Group 3: Head parameters\n",
    "params_to_train_student.append({\n",
    "    'params': student_head_CLIPART.parameters(), # Already requires_grad=True\n",
    "    'lr': ADAPT_LR_HEAD\n",
    "})\n",
    "\n",
    "optimizer_CLIPART_student = optim.AdamW(params_to_train_student) # Only optimizes student\n",
    "\n",
    "# Criterion for pseudo-labeled target data (Using hard labels here)\n",
    "# Use KLDivLoss if you collect soft probabilities from teacher\n",
    "criterion_pseudo = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Criterion for validation (standard classification)\n",
    "criterion_val = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Optimizer groups for STUDENT ({TARGET_DOMAIN_NAME}):\")\n",
    "total_params_optimized = 0\n",
    "for i, group in enumerate(optimizer_CLIPART_student.param_groups):\n",
    "    group_params = sum(p.numel() for p in group['params'])\n",
    "    print(f\"  Group {i}: LR={group['lr']}, Params={group_params:,}\")\n",
    "    total_params_optimized += group_params\n",
    "print(f\"Total trainable parameters optimized for {TARGET_DOMAIN_NAME} STUDENT: {total_params_optimized:,}\")\n",
    "\n",
    "print(f\"\\n--- Adapting to {TARGET_DOMAIN_NAME} (Optimizing STUDENT: LoRA, LN, Head) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "# For amp autocasting\n",
    "autocast_ctx = lambda: torch.amp.autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "# For early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "EARLY_STOPPING_PATIENCE = 5 # Example patience value\n",
    "\n",
    "print(f\"\\n--- Training Adaptation Loop for {TARGET_DOMAIN_NAME} ---\")\n",
    "print(f\"Using Mean Teacher (EMA Decay: {EMA_DECAY}) for pseudo-labeling.\")\n",
    "\n",
    "for epoch in range(ADAPT_EPOCHS):\n",
    "    # Calculate current pseudo-label threshold (linear schedule)\n",
    "    if ADAPT_EPOCHS == 1:\n",
    "        current_pseudo_threshold = PSEUDO_LABEL_END_THRESHOLD\n",
    "    else:\n",
    "        current_pseudo_threshold = PSEUDO_LABEL_START_THRESHOLD + \\\n",
    "                                   (PSEUDO_LABEL_END_THRESHOLD - PSEUDO_LABEL_START_THRESHOLD) * (epoch / (ADAPT_EPOCHS - 1))\n",
    "    print(f\"\\nEpoch {epoch+1}/{ADAPT_EPOCHS}, Pseudo-label threshold: {current_pseudo_threshold:.3f}\")\n",
    "\n",
    "    # --- 1. Pseudo-Label Generation using EMA TEACHER ---\n",
    "    pseudo_labeled_images_collected = []\n",
    "    pseudo_labels_collected = [] # Store hard labels (indices)\n",
    "\n",
    "    teacher_vit_CLIPART.eval() # Ensure teacher is in eval mode\n",
    "    teacher_head_CLIPART.eval()\n",
    "\n",
    "    print(f\"Collecting pseudo-labels for {TARGET_DOMAIN_NAME} using EMA TEACHER...\")\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the *entire* unlabeled target training set\n",
    "        for images, _ in tqdm(CLIPART_train_loader_unlabeled_images, desc=f\"Pseudo-Labeling {TARGET_DOMAIN_NAME}\", leave=False):\n",
    "            images = images.to(DEVICE)\n",
    "            with autocast_ctx():\n",
    "                # Get features and logits from the TEACHER\n",
    "                features_teacher = teacher_vit_CLIPART(images)\n",
    "                cls_features_teacher = features_teacher[:, 0] if features_teacher.ndim == 3 and features_teacher.shape[1] > 0 else features_teacher\n",
    "                logits_teacher = teacher_head_CLIPART(cls_features_teacher)\n",
    "\n",
    "            probs_teacher = torch.softmax(logits_teacher, dim=1)\n",
    "            max_probs, pred_labels = torch.max(probs_teacher, dim=1)\n",
    "\n",
    "            # Filter based on threshold\n",
    "            mask = max_probs >= current_pseudo_threshold\n",
    "            if mask.any():\n",
    "                pseudo_labeled_images_collected.append(images[mask].cpu())\n",
    "                pseudo_labels_collected.append(pred_labels[mask].cpu()) # Store hard labels\n",
    "\n",
    "    if not pseudo_labeled_images_collected:\n",
    "        print(\"Warning: No pseudo-labels collected in this epoch with current threshold. Skipping training steps.\")\n",
    "        # Optionally: Reduce threshold slightly for next epoch, or just continue\n",
    "        continue # Go to validation/next epoch\n",
    "\n",
    "    pseudo_labeled_images_cat = torch.cat(pseudo_labeled_images_collected, dim=0)\n",
    "    pseudo_labels_cat = torch.cat(pseudo_labels_collected, dim=0)\n",
    "    print(f\"Collected {len(pseudo_labeled_images_cat)} pseudo-labeled samples for {TARGET_DOMAIN_NAME} using EMA Teacher.\")\n",
    "\n",
    "    # Create DataLoader for the pseudo-labeled subset\n",
    "    pseudo_batch_size_actual = BATCH_SIZE\n",
    "    pseudo_dataset = torch.utils.data.TensorDataset(pseudo_labeled_images_cat, pseudo_labels_cat)\n",
    "    # num_workers=0 is often recommended for TensorDataset\n",
    "    pseudo_loader = DataLoader(pseudo_dataset, batch_size=pseudo_batch_size_actual, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # --- 2. STUDENT Training on Pseudo-Labels ---\n",
    "    student_vit_CLIPART.train() # Set student model to train mode\n",
    "    student_head_CLIPART.train()\n",
    "\n",
    "    running_loss_pseudo_epoch = 0.0\n",
    "    total_pseudo_samples_processed = 0\n",
    "\n",
    "    progress_bar = tqdm(pseudo_loader, desc=f\"Epoch {epoch+1} [Adapt Train {TARGET_DOMAIN_NAME}]\", leave=False)\n",
    "    for pseudo_batch_images, pseudo_batch_labels in progress_bar:\n",
    "        pseudo_batch_images = pseudo_batch_images.to(DEVICE)\n",
    "        pseudo_batch_labels = pseudo_batch_labels.to(DEVICE) # Hard labels from teacher\n",
    "        current_pseudo_batch_size = pseudo_batch_images.size(0)\n",
    "\n",
    "        with autocast_ctx():\n",
    "            # Features from STUDENT ViT\n",
    "            features_student = student_vit_CLIPART(pseudo_batch_images)\n",
    "            cls_features_student = features_student[:, 0] if features_student.ndim == 3 and features_student.shape[1] > 0 else features_student\n",
    "            # Logits from STUDENT Head\n",
    "            logits_student = student_head_CLIPART(cls_features_student)\n",
    "\n",
    "            # --- Loss on Pseudo-Labeled Data (Using CrossEntropy with hard labels) ---\n",
    "            loss_pseudo = criterion_pseudo(logits_student, pseudo_batch_labels)\n",
    "\n",
    "            # --- (Optional: Add Entropy Minimization here if needed) ---\n",
    "            # loss_ent = ... calculated on student outputs for unlabeled data\n",
    "\n",
    "        # --- Backpropagation (Optimizing STUDENT) ---\n",
    "        total_loss_step = loss_pseudo # + ENT_WEIGHT * loss_ent\n",
    "        if torch.isnan(total_loss_step) or torch.isinf(total_loss_step):\n",
    "             print(f\"Warning: NaN or Inf loss detected (pseudo loss: {loss_pseudo.item()}). Skipping step.\")\n",
    "             continue\n",
    "\n",
    "        optimizer_CLIPART_student.zero_grad()\n",
    "        scaler.scale(total_loss_step).backward()\n",
    "        # Optional: Gradient clipping\n",
    "        # scaler.unscale_(optimizer_CLIPART_student)\n",
    "        # torch.nn.utils.clip_grad_norm_(params_to_train_student_flat, max_norm=1.0) # Need to flatten param groups first\n",
    "        scaler.step(optimizer_CLIPART_student)\n",
    "        scaler.update()\n",
    "\n",
    "        # --- EMA Update of TEACHER (based on updated STUDENT) ---\n",
    "        update_ema_teacher(student_vit_CLIPART, student_head_CLIPART, teacher_vit_CLIPART, teacher_head_CLIPART, EMA_DECAY)\n",
    "\n",
    "        running_loss_pseudo_epoch += loss_pseudo.item() * current_pseudo_batch_size\n",
    "        total_pseudo_samples_processed += current_pseudo_batch_size\n",
    "\n",
    "        progress_bar.set_postfix(L_pseudo=loss_pseudo.item())\n",
    "\n",
    "    if total_pseudo_samples_processed > 0:\n",
    "        avg_loss_pseudo = running_loss_pseudo_epoch / total_pseudo_samples_processed\n",
    "        print(f\"Epoch {epoch+1} Adapt Train Avg Pseudo Loss: {avg_loss_pseudo:.4f}\")\n",
    "    else:\n",
    "        # This case handled by the check after pseudo-label generation\n",
    "        pass\n",
    "\n",
    "    # --- 3. Validation on Target Domain (using STUDENT model) ---\n",
    "    student_vit_CLIPART.eval()\n",
    "    student_head_CLIPART.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(CLIPART_val_loader, desc=f\"Epoch {epoch+1} [{TARGET_DOMAIN_NAME} Val]\", leave=False):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with autocast_ctx():\n",
    "                # Use STUDENT model for validation\n",
    "                features = student_vit_CLIPART(images)\n",
    "                cls_features = features[:, 0] if features.ndim == 3 and features.shape[1] > 0 else features\n",
    "                logits = student_head_CLIPART(cls_features)\n",
    "                loss = criterion_val(logits, labels) # Use standard CE loss for validation\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            val_correct_predictions += torch.sum(preds == labels.data)\n",
    "            val_total_samples += images.size(0)\n",
    "\n",
    "    if val_total_samples > 0:\n",
    "        epoch_val_loss = val_loss / val_total_samples\n",
    "        epoch_val_acc = val_correct_predictions.double() / val_total_samples\n",
    "        print(f\"Epoch {epoch+1} {TARGET_DOMAIN_NAME} Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- Early Stopping & Saving Best STUDENT Model ---\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            patience_counter = 0\n",
    "            # Save the state dicts of the best student model\n",
    "            CLIPART_best_lora_ln_state_dict = {name: param for name, param in student_vit_CLIPART.named_parameters() if param.requires_grad}\n",
    "            torch.save(CLIPART_best_lora_ln_state_dict, os.path.join(CLIPART_MODEL_SAVE_DIR, f\"{TARGET_DOMAIN_NAME.lower()}_best_lora_ln_student.pth\"))\n",
    "            torch.save(student_head_CLIPART.state_dict(), os.path.join(CLIPART_MODEL_SAVE_DIR, f\"{TARGET_DOMAIN_NAME.lower()}_best_head_student.pth\"))\n",
    "            print(f\"    -> New best validation accuracy: {best_val_acc:.4f}. Saved best student model.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"    -> Validation accuracy did not improve. Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"--- Early stopping triggered after epoch {epoch+1} ---\")\n",
    "                break # Exit training loop\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} {TARGET_DOMAIN_NAME} Val: No validation samples processed.\")\n",
    "\n",
    "# --- End of Training ---\n",
    "print(f\"\\n--- Adaptation training for {TARGET_DOMAIN_NAME} finished ---\")\n",
    "# Load the best saved student model weights for final use/evaluation\n",
    "print(\"Loading best student model weights...\")\n",
    "best_student_lora_ln_weights = torch.load(os.path.join(CLIPART_MODEL_SAVE_DIR, f\"{TARGET_DOMAIN_NAME.lower()}_best_lora_ln_student.pth\"), map_location=DEVICE)\n",
    "best_student_head_weights = torch.load(os.path.join(CLIPART_MODEL_SAVE_DIR, f\"{TARGET_DOMAIN_NAME.lower()}_best_head_student.pth\"), map_location=DEVICE)\n",
    "\n",
    "# Need to re-initialize a student model and load weights into it\n",
    "final_student_vit = copy.deepcopy(base_vit_frozen)\n",
    "final_student_vit = inject_lora_to_vit_attention(final_student_vit, rank=LORA_RANK)\n",
    "final_student_vit = set_layernorm_affine_trainable(final_student_vit)\n",
    "# Load the trainable parameters only\n",
    "final_student_vit.load_state_dict(best_student_lora_ln_weights, strict=False)\n",
    "final_student_vit = final_student_vit.to(DEVICE).eval()\n",
    "\n",
    "final_student_head = DomainSpecificHead(in_features=ART_EMBED_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "final_student_head.load_state_dict(best_student_head_weights)\n",
    "final_student_head = final_student_head.eval()\n",
    "\n",
    "print(\"Best student model loaded.\")\n",
    "\n",
    "# Now `final_student_vit` and `final_student_head` hold the best adapted model for Clipart\n",
    "# You would use these for final evaluation or as the starting point for the *next* domain adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cell 23\n",
    "# --- Saving the BEST STUDENT Model (Loaded after early stopping/loop completion) ---\n",
    "# The final_student_vit and final_student_head hold the best weights now.\n",
    "\n",
    "# Note: The saving already happened inside the loop when the best val acc was found.\n",
    "# This cell now mainly serves as a confirmation and sanity check.\n",
    "\n",
    "print(f\"Best {TARGET_DOMAIN_NAME} STUDENT model weights were saved during training to {CLIPART_MODEL_SAVE_DIR}\")\n",
    "print(f\"  ViT LoRA/LN file: {TARGET_DOMAIN_NAME.lower()}_best_lora_ln_student.pth\")\n",
    "print(f\"  Head file: {TARGET_DOMAIN_NAME.lower()}_best_head_student.pth\")\n",
    "\n",
    "# --- Sanity Check: CLIPART Adaptation (using the final loaded best student) ---\n",
    "# Re-run validation on the loaded best model to confirm accuracy\n",
    "final_val_loss = 0.0\n",
    "final_val_correct = 0\n",
    "final_val_total = 0\n",
    "final_student_vit.eval()\n",
    "final_student_head.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(CLIPART_val_loader, desc=f\"Final Validation [{TARGET_DOMAIN_NAME}]\", leave=False):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        with autocast_ctx():\n",
    "            features = final_student_vit(images)\n",
    "            cls_features = features[:, 0] if features.ndim == 3 and features.shape[1] > 0 else features\n",
    "            logits = final_student_head(cls_features)\n",
    "            loss = criterion_val(logits, labels)\n",
    "\n",
    "        final_val_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        final_val_correct += torch.sum(preds == labels.data)\n",
    "        final_val_total += images.size(0)\n",
    "\n",
    "if final_val_total > 0:\n",
    "    final_epoch_val_loss = final_val_loss / final_val_total\n",
    "    final_epoch_val_acc = final_val_correct.double() / final_val_total\n",
    "    print(f\"Confirmed Best {TARGET_DOMAIN_NAME} Student Val Loss: {final_epoch_val_loss:.4f}, Val Acc: {final_epoch_val_acc:.4f}\")\n",
    "    # Sanity check based on the best accuracy achieved during training\n",
    "    assert final_epoch_val_acc > 0.02, f\"{TARGET_DOMAIN_NAME} validation accuracy ({final_epoch_val_acc:.4f}) is too low. Adaptation might have failed.\"\n",
    "    print(f\"{TARGET_DOMAIN_NAME} adaptation sanity check passed (best accuracy > 0.02).\")\n",
    "else:\n",
    "    print(f\"Warning: {TARGET_DOMAIN_NAME} validation set was empty or not processed, cannot perform final sanity check.\")\n",
    "\n",
    "# IMPORTANT: For the *next* adaptation step (e.g., Clipart -> Product),\n",
    "# you would initialize the Product student/teacher based on the *final_student_vit*\n",
    "# and *final_student_head* from this Clipart adaptation phase."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7321011,
     "sourceId": 11665178,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7321032,
     "sourceId": 11686886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
